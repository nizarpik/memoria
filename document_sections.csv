DOCUMENT_TITLE,SECTION_TITLE,SECTION_TEXT
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,1. CAPITULO I: INTRODUCCION,"Desde tiempos muy antiguos, a nivel mundial, la forma de producir cobre es a través de la pirometalurgia.
Este proceso permite obtener ánodos de cobre, sin embargo esta vía genera un subproducto que se acumula por años, las escorias; las cuales son generadas a razón de 2,2 toneladas por cada tonelada de cobre producido llegando a generarse a nivel nacional 4,5 millones de escoria por año.
Este subproducto, comúnmente se dispone en escoriales y permanece allí por largo tiempo como pasivo ambiental, el cual con el paso de los años y su gran acumulación comienza a despertar un malestar visual en las comunidades.
A la vez, este subproducto llamado escoria encapsula especies de interés que lo hace atractivo para iniciar una apuesta que permita recuperar estas especies y ponerlas en valor y junto con ello ahorrar recursos de metal.
En lo netamente económico, procesar estos pasivos industriales permitirá obtener beneficios de algo que ya se creía sin un mayor valor económico; generando así un aporte adicional a la economía del país.
Actualmente las escorias son utilizadas en áreas de construcción y pavimentación, netamente como materia prima para la producción de cementos fortificados, áridos y gravilla.
Esto ya que presentan un marcado carácter vítreo que las hace difíciles de procesar al presentar refractariedad a ciertos procesos de recuperación de metales en ellas contenidos.
Sin embargo, hoy en día son sometidas a ciertos procesos para recuperar el contenido de valor.
Los procesos más usados son chancado, molienda, separación magnética, flotación, lixiviación y tostación de escorias, lo que permite recuperar metales tales como Fe, Cr, Cu, Al, Pb, Zn, Co, Ni, Nb, Ta, Au y Ag.
Pero los procesos de conminución, como es sabido, son procesos costosos dado el alto consumo energético que conlleva el funcionamiento de chancadores y molinos.
A su vez, el proceso de flotación también incurre en altos consumos de agua.
Esto despierta la búsqueda de un método que permita saltar etapas y abaratar costos de tratamiento de las escorias para lograr recuperar las especies de valor encapsuladas en ellas.
Todo lo anterior, conlleva a proponer un método para modificar las características vítreas de las escorias al salir del proceso pirometalúrgico, ya que esta podría ser la vía para saltar las etapas antes mencionadas y llegar a un tratamiento de lixiviación posterior que permita recuperar los metales contenidos en las escorias; abaratando costos de procesamiento y reduciendo la huella ambiental marcada en la madre naturaleza."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,1.1. 1.1 Hipótesis,"Debido a que elementos como Ca, Na y K modifican la estructura de escorias vítreas, es posible la transformación de escorias tipo fayalíticas a escorias pseudo-olivínicas, mediante una fusión alcalina en base a la correcta dosificación de estos elemento en forma de carbonato, disminuyendo con ello su refractariedad a la disolución ácida."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,1.2. 1.2 Objetivo General,Analizar el cambio de propiedades físicas de una escoria de cobre tras una transformación de escoria fayalítica a pseudo-olivínica.
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,1.3. 1.3 Objetivos Específicos," Establecer dosificación de agente modificador de estructura para lograr sistema olivínico.
 Modelar el cambio de viscosidad de escoria fayalítica al adicionar un modificador alcalino de estructura de escoria.
 Modelar la temperatura ponderada de fusión de la escoria para lograr la transformación de escoria fayalítica a pseudo-olivínica.
 Proponer condiciones experimentales que permitirán la validación de la modelación termodinámica para trabajos experimentales futuros a partir de los resultados obtenidos."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2. CAPITULO II: ANTECEDENTES Y MARCO TEORICO,
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.1. 2.1 Aspectos generales de las Escorias de Cobre,
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.1.1. 2.1.1 Origen de las Escorias,"Las escorias de cobre son un residuo que se produce tras llevar a cabo un proceso pirometalúrgico para obtener cobre desde un concentrado de minerales sulfurados.
Dicho proceso consiste en fases o etapas consecutivas de secado, fusión, conversión y refinación (ver Figura 1) a través de los cuales se busca extraer el metal de interés (cobre) desde el concentrado, eliminar la ganga presente y purificar el metal de interés.
Las etapas pirometalúrgicas mencionadas se realizan en hornos de lecho fluidizado o secador rotatorio, hornos de fusión, pudiendo ser horno flash u horno convertidor teniente, hornos de conversión como los Pierce Smith y hornos de refino a fuego.
En la etapa de fusión se producen dos fases líquidas separables o inmiscibles: la mata rica en cobre y la escoria.
La mata, según su contenido de cobre, continúa luego en el proceso de refinación, mientras que la escoria fundida se descarga directamente o pasa a procesos de recuperación de cobre.
En conversión se produce cobre blíster el cual tiene un mayor contenido de cobre, este sigue en el proceso de refinación.
Las escorias producidas en estas etapas, debido al contenido de cobre que poseen, son enviadas a hornos de limpieza de escoria (HLE) en los que se lleva a cabo un proceso de recuperación de cobre; el cual es recuperado como eje o mata con un contenido de cobre de entre 50% y 70% y retornado a la etapa de fusión.
Además, desde esta fase de limpieza se obtiene una escoria que va a botadero y que tiene un contenido de cobre de entre 0,7% y 1%.
[1] y [2]
Figura 1: Proceso de producción pirometalúrgica de Cobre.
[3]
En este proceso las escorias que se producen lo hacen a partir de óxidos contenidos en la carga de los hornos de fusión y de los óxidos de hierro que se producen por la oxidación durante el procesamiento pirometalúrgico; cuya naturaleza y proporción en la escoria dependerá de la naturaleza de los minerales, los concentrados, los fundentes y de las condiciones de operación de los equipos en las distintas etapas.
Ahondando en la secuencia anterior, lo que ocurre en cuanto a los componentes del concentrado, es decir los óxidos y los sulfuros, es que estos se combinan covalentemente para formar una fase Cu/Fe/O/S en ausencia de sílice.
Pero al realizar la fusión del concentrado entra en proceso sílice como fundente; la cual se combina con los óxidos presentes formando aniones de silicato fuertemente unidos por puentes de oxígeno que dan lugar a la formación de la fase escoria.
En este proceso también se agrega cal y alúmina, componentes que permiten estabilizar la estructura de la escoria.
[4] Una vez que las escorias pasan por la etapa de limpieza son llevadas a botadero donde son sometidas a un enfriamiento natural en el lugar; formando un producto cristalino denso y duro.
Pero si la escoria es enfriada rápidamente al verterla en agua se obtiene una escoria granulada amorfa.
Comúnmente la escoria de cobre es una especie de matriz de vidrio de silicato de hierro impuro con pequeñas inclusiones de sulfuros y óxidos de cobre que quedaron atrapados en la escoria durante el proceso, ya sea por atrapamiento mecánico y/o químico.
También se pueden encontrar fayalita y otras especies como Cr-Spinel (ver Figura 2).
Estudios recientes sobre la mineralogía de la escoria de cobre indican que el cobre está presente en la fase vítrea y que un 0,81% en peso del cobre contenido en la escoria esta como sulfuro, mientras que un 0,15% en peso del cobre esta como oxido.
En esta fase vítrea no solo hay cobre, sino que también especies de interés como Fe, Al, K, Mg entre otros (ver Tabla 1).
Además esta fase vítrea constituye un 85,18% de la escoria, por lo que poder modificar esta fase donde se encuentran las especies de interés resultaría de gran importancia para poder recuperar las especies que ella contiene.
[5]
Figura 2: Imagen SEM que muestra componentes minerales detectados en escoria de horno flash Outokumpu en diferentes regiones.
[5]
Tabla 1: Mineralogía para escoria de cobre determinada por QEMSCAN BMA. [5]
Mineral
Fórmula aproximada
Abundancia aproximada (%)
Fase vítrea (Al–Ca–Cu–Fe–K–Mg–Na–O–S–Si) 85,18
Fayalita (Mg, Fe)2 SiO4 12,85
Cromita FeCr2O4 0.90
Sulfuro CuxSy 1,05
TOTAL 100,00
Cabe destacar que las escorias hoy se producen a razón de 2,2 toneladas por cada tonelada de cobre producido, alcanzado una acumulación anual en Chile de 4,5 millones de toneladas; se estima que el país posee un volumen histórico de escorias que bordea los 50 millones de toneladas.
A escala mundial anualmente se generan alrededor de 24,6 millones de toneladas de escoria, sin embargo la mayor parte de estas se vierte en botaderos sin reciclaje apropiado.
[1] y [4]"
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.1.2. 2.1.2 Características físico-químicas de las Escorias,"Las escorias de cobre poseen características que las distinguen de otros residuos sólidos, pero que sin embargo son semejantes o transversales a las escorias producidas por diferentes tecnologías (ver Tabla 2).
Dentro de las características que las distinguen están su color, textura y estructura cristalina/amorfa, siendo esta última la que las distingue mayormente de otros residuos.
Además de ello, existen características que vienen dadas por la forma y tiempo de enfriamiento, como por ejemplo el tamaño de partícula; o la materia prima y las tecnologías utilizadas en el proceso pirometalúrgico.
Tabla 2: Principales características físicas de escorias de cobre.
Apariencia
 | Forma de partículas | Irregulares con bordes agudos
 | Densidad [kg/m3] | 3160 – 3870
 | Absorción de agua (%) | 0,15 – 0,55
 | Dureza | 4
–
6 De color negro, textura lisa para escorias de botadero y porosa para escorias granalladas Variada, según su formación puede ir desde 4” hasta material fino menor a 0,08 [mm]

Granulometría
Además de las características antes mencionadas, existe una propiedad de las escorias que es de alta relevancia en los procesos de reducción y afino de metales pero, generalmente, difícil de poder determinar.
Esta propiedad, que en los procesos pirometalúrgicos actúa como una variable, es la viscosidad; propiedad que tiene una fuerte dependencia de la temperatura, pero que en general presenta un comportamiento que sigue la ley de Arrhenius de la viscosidad con relación a la temperatura.
A parte de la dependencia de la temperatura, distintos estudios han mostrado que la viscosidad también se ve fuertemente influida por la composición de las escorias.
Lo cual ha llevado a desarrollar modelos que permiten determinar la viscosidad de escorias con contenidos de componentes ácidos y básicos.
Uno de los componentes de las escorias que tiene una incidencia tanto en la viscosidad como en la estructura de las escorias es el SiO2, cuyo contenido en la escoria puede hacer que la viscosidad aumente o disminuya.
Es así como para rangos de SiO2 de 0-30% la viscosidad de la escoria aumenta y para contenidos de SiO2 entre 30-40% la viscosidad se ve disminuida, mientras que para contenidos de SiO2 por encima del 40% la viscosidad de la escoria vuelve a aumentar.
[6] Cuando se lleva a la escoria desde una temperatura de alrededor de 1150[°C] a una temperatura menor de forma brusca, es decir, se somete a un enfriamiento rápido, la estructura de la escoria queda colapsada y es mayoritariamente amorfa y sus propiedades serán las de un material altamente reactivo.
En cambio, si el enfriamiento es lento, tenemos menor cantidad de estructura amorfa, predominando estructuras cristalinas impidiendo la formación de nuevos enlaces.
[7] Respecto a la composición de las escorias, anteriormente se ha mencionado que estas se forman a partir de óxidos contenidos en la carga de los hornos de fusión y de los óxidos de hierro que se producen por la oxidación durante el procesamiento pirometalúrgico; óxidos que dependen de factores como la naturaleza de la materia prima, las condiciones de operación de las diferentes tecnologías, de los fundentes agregados al proceso, entre otros.
Esto a su vez incide en la cantidad o porcentaje presente de ellos en las escorias, encontrándose comúnmente una proporción como la siguiente:
Óxidos de hierro (FeO, Fe3O4) de 30 – 40 %
Óxidos de silicio (SiO2) de 35 – 40 %
Óxidos de aluminio (Al2O3) hasta 10 %
Óxido de calcio (CaO) hasta 10 % El alto porcentaje de óxidos de hierro y sílice provenientes de los procesos de fusiónconversión de los sulfuros de cobre y las condiciones de temperatura y presión parcial de O2 de estos procesos, sitúan a la región líquidus de operación cerca de la zona donde es estable el compuesto 2FeOSiO2 (Fayalita) (ver Figura 3); cuya presencia en la escoria hace que esta tenga alta viscosidad, baja conductividad eléctrica, atrape más cobre y que al solidificar forme redes cristalinas de silicatos.
Esto último, le confiere las características vítreas a la escoria y por consiguiente una mayor refractariedad a disolución acida.
Figura 3: Diagrama de fases ternario FeO-Fe2O3-SiO2.
En las escorias, además de Fayalita, también hay formación de especies como magnetita, óxidos de calcio-aluminio (CaAl2O4), silicatos de zinc ((Zn2SiO4) y óxidos de cobre-hierro (CuFe2O4).
[1]
Las redes de silicatos están conformadas por una unidad básica: tetraedros de SiO4 (-4) (ver Figura 4); la unión de estas estructuras básicas mediante puentes de oxígeno (Si – O), le confiere una gran estabilidad a la red, ya sea en estado sólido o fundido, gracias al enlace covalente entre ellos y la estructura polimerizada que se forma en presencia de óxidos ácidos como sílice (SiO2) y alúmina (Al2O3) (ver Figura 5), dificultando así el rompimiento de enlaces por agentes lixiviantes.
Figura 4: (a) Tetraedro SiO4 (-4) completo, (b) tetraedro con átomo superior de oxígeno retirado, (c) vista lateral.
Figura 5: (a) Estructura sílice sólida, (b) estructura sílice fundida.
Según la composición química de la escoria de cobre de la fundición Hernán Videla Lira, las escorias presentan un contenido significativo de especies de interés como hierro, cobre, aluminio, calcio, cromo, plomo, zinc, plata, oro, entre otros que se muestran en la Tabla 3; los cuales son de gran importancia a nivel industrial y que de poder ser recuperados de una forma económica e introducidos nuevamente al proceso productivo se podría producir un aporte adicional a la economía nacional.
Tabla 3: Composición química de la escoria de cobre de la fundición Hernán Videla Lira.
Cu Ag Au Cr2O3 Fe(total) Fe3O4 SiO2 Al2O3 CaO MgO S Cl Sb As Pb
 | (%) | (g/t) | (g/t) | (%) | (%) | (%) | (%) | (%) | (%) | (%) | (%) | (%) | (%) | (%) | (%)
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | 0,75 | 1,83 | 0,20 | 0,05 | 41,45 | 5,14 | 27,89 | 2,91 | 2,10 | 0,88 | 1,01 | 0,12 | 0,01 | 0,01 | 0,11

Si bien en la Tabla 3 se cuantifica la ley de cobre en la escoria, este metal se encuentra presente en el residuo industrial como óxido de cobre, sulfuro de cobre o cobre metálico, predominando la presencia de cobre como sulfuro dependiendo de la tecnología de fundición de concentrado utilizada y de los parámetros operacionales de estas.
Sin embargo, dependiendo también de la operación, pudiese ser que el cobre se sobre oxide y exista en la escoria una mayor cantidad de óxido de cobre.
En la Tabla 3 se puede ver también que la ley de cobre en esta escoria supera la ley de cobre promedio de los yacimientos del país; hecho que se vuelve contraproducente y levanta la necesidad de tratar las escorias para lograr recuperar el cobre que se pierde en ellas.
Sin embargo, hay tipos de escorias con características distintas a las antes mencionadas, las que por su constitución y orden cristalográfico obtenido mediante procesos como el granallado permitirían a las escorias ser lixiviables y poder recuperar las especies de interés contenidas en ellas.
Estas escorias son las olivínicas, escorias del tipo calcio ferríticas, las cuales tienen baja solubilidad para cobre y son generadas tras adicionar CaO al baño fundido, logrando que el equilibrio se traslade hacia la formación de escoria olivínica."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.1.3. 2.1.3 Clasificación de las Escorias,"Las escorias pueden ser clasificadas a partir del contenido o grado de silicatos y del tiempo de enfriamiento:
 Según contenido o grado de silicatos:
Las escorias pueden ser ácidas, neutras o básicas dependiendo del índice de silicatación y del grado de saturación de estas.
El primero depende netamente del contenido de oxígeno de la porción de sílice que forma la escoria, y el contenido en oxigeno de la porción de todos los otros óxidos que la forman.
Mientras que el segundo depende de la razón entre el número total de gr/mol de los componentes básicos (CaO-MgO-MnO-FeO) y el número de gr/mol de los componentes ácidos (incluso Al2O3).
[7] De acuerdo con lo anterior, si el índice de silicatación es igual a uno la escoria se clasifica como neutra, si es mayor que uno es ácida y si es menor a uno será básica; tal como lo indica la Figura 6.
Figura 6: Tipos de escoria según el grado de silicato.
Para el caso del grado de saturación de escorias, si es igual a uno la escoria es neutra, si es mayor a uno es básica y si es menor que uno la escoria es ácida.
Tener conocimiento del tipo de escoria que se está trabajando es importante a la hora de hacerlas reaccionar con algún compuesto químico, puesto que el resultado dependerá de si la escoria es ácida o básica.
Para el caso de estudio, de concretarse la transformación de escoria fayalítica a pseudo-olivínica, la escoria debería ser del tipo básica con estructuras moleculares de tetraedros de silicatos.
 Según tiempo de enfriamiento: Anteriormente se ha dicho que el tiempo de enfriamiento determina la estructura de las escorias, la cual puede ser mayormente amorfa si el tiempo de enfriamiento es corto o bien mayormente cristalina si el tiempo de enfriamiento es largo.
Esto a su vez incide en el poder de reactividad de la escoria, ya que se ha encontrado que la estructura amorfa es donde reside el componente hidráulicamente activo de la escoria.
[1] De acuerdo con esto, se puede encontrar los siguientes tipos de escoria:
o Escoria granallada: se obtiene de pasar la escoria desde una alta temperatura (1150[°C]) a una temperatura cercana a los 100[°C] en el menor tiempo posible.
Esto se lleva a cabo en un pozo de granulación o granallador donde se pone en contacto la escoria a alta temperatura con agua a temperatura ambiente.
En este punto no ocurren reacciones químicas, pero si ocurren cambios físicos y mecánicos que le confieren a la escoria la estructura amorfa; estructura que provee a la escoria de cobre una mayor porosidad, aumentando la absorción de agua y una menor masa por unidad de volumen.
o Escoria de botadero: se obtiene netamente de verter la escoria en una cancha de escoria o escorial y esperar que esta se enfríe lentamente al aire hasta alcanzar temperatura ambiente.
El largo tiempo que toma este enfriamiento permite que la escoria desarrolle una estructura cristalina, lo que reduce la porosidad de ella limitando consigo la reactividad de la escoria en un proceso posterior.
Además, estudios anteriores han mostrado que el porcentaje de absorción de agua es menor y la densidad de estas escorias es mayor respecto a las escorias granalladas.
o Escoria expandida: se obtiene mediante un rápido enfriamiento y su expansión se logra mediante la aplicación controlada de agua, aire y vapor.
Esto produce una escoria de naturaleza vesicular y más ligera."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.1.4. 2.1.4 Usos de las Escorias,"Las escorias de cobre tienen un contenido de especies valiosas que en ocasiones suelen ser recuperadas mediante procesos de flotación para luego reincorporar estas especies al proceso productivo del cobre, pero esto involucra etapas de alto consumo energético y de recursos hídricos.
Por ello, actualmente las escorias de cobre son acumuladas en vertederos donde permanecen ocupando grandes extensiones de superficie de terreno como pasivos ambientales.
Sin embargo, en los últimos años ha despertado un creciente interés en distintos sectores asociados a la fabricación de materiales de construcción, por el uso de las escorias como:
 sustituto parcial del cemento hidráulico
 gravilla para líneas de ferrocarriles
 árido en mezclas asfálticas de obras viales
 árido constituyente de los morteros y hormigones de cemento
 abrasivo en la limpieza por chorro de arena de estructuras metálicas
 albañilerías de bloques de escoria moldeados
 material de reemplazo en forma parcial al cemento Portland
Del listado anterior se aprecia que las escorias de cobre son mayormente usadas en la elaboración de cemento y/o morteros, puesto que, según estudios previos, las escorias de cobre poseen propiedades cementosas que las hace atractivas en este rubro al conferirle al hormigón una mayor resistencia a la compresión, llegando a alcanzar valores de resistencia 34% más altos que los alcanzados por el hormigón convencional.
[3]
Este aumento en la resistencia de los hormigones, que incorporan escorias de cobre, viene dado por la modificación de la estructura vítrea de las escorias durante la elaboración del cemento mediante la adición de compuestos que poseen iones capaces de interrumpir los enlaces de puente de oxígeno en la red de silicatos; mejorando con esto la actividad puzolánica de ellas.
Uno de los compuestos adicionados es el CaO, cuyo contenido del catión alcalino Ca2+ (que se comporta como modificador de la red de silicatos), es favorable para la despolimerización de la red de los materiales vítreos como las escoria, lo que da como resultado una mayor reactividad de este tipo de materiales.
[8] Si bien el cobre en la naturaleza se encuentra asociado a otros minerales en forma de óxidos o sulfuros, una vez procesado el mineral concentrado de este metal mediante pirometalurgia, el cobre que no logra ser recuperado queda en la escoria y se encuentra en ella como óxido de cobre embebido en una matriz de óxidos de Fe-SiO2 o bien como sulfuro atrapado en esta matriz producto de la no coalescencia o decantación de pequeñas gotas de mata hacia la fase rica en cobre.
Este cobre de la escoria, junto con otros metales contenidos en ella, ha sido recuperado comúnmente mediante proceso de flotación.
Este proceso incluye etapas de conminución para bajar la granulometría de la escoria y lograr liberar el metal para recuperarlo mediante flotación.
Sin embargo, esta etapa de conminución implica consumo de energía y recursos hídricos (agua) altos; el consumo de energía asociado a la dureza de la escoria y a la granulometría a la que se debe llegar para poder flotar el cobre y obtener un concentrado y el consumo de recurso hídrico propio de la molienda.
Además de ello, está la necesidad de utilizar aditivos en la flotación, dígase espumantes y colectores, que encarecen el proceso.
En esta etapa también, el consumo del recurso hídrico es alto y primordial para el proceso, aun cuando se logra recuperar agua en etapas de espesamiento."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.2. 2.2 Cobre en las escorias: modos de recuperación del cobre,"Otros métodos de recuperar el cobre contenido en la escoria están asociados a las vías pirometalúrgica, hidrometalúrgica o ambas como un proceso conjunto; siendo un método la reducción carbotérmica de escoria de cobre.
Este método o modo de recuperar cobre basado en una reducción carbotérmica permite recuperar cobre y metales de interés, así como también el hierro contenido en la escoria como una aleación rica en hierro.
Estudios anteriores muestran que llevar a cabo una reducción carbotérmica de 1 hora a 1400 [°C] adicionando un 4% de coque en polvo, permite obtener una aleación Fe/Co/Cu con contenidos de 1,72% de Co y 4,41% de Cu, logrando recuperaciones de Co y Cu de 97,7% y 86,7% respectivamente.
Estas recuperaciones se lograron realizando el proceso en hornos de arco de corriente continua del tipo open top, sin embargo estudios del mismo tipo llevados a cabo en hornos de tipo cerrado mostraron contenidos de cobalto y cobre más altos así como un aumento en las recuperaciones de ambos metales.
[4] También se han desarrollado una serie de métodos hidrometalúrgicos utilizando agentes lixiviantes como ácidos, bases y sales para la extracción de metales básicos desde las escorias de fundición.
En estudios anteriormente realizados, se obtuvieron recuperaciones de más de un 92% de Cu a partir de una escoria de cobre con un contenido de 2,62% de este metal por lixiviación sulfúrica y amoniacal a 95 °C.
Otros estudios han mostrado también que se puede extraer más de 80% de Cu y Co de diferentes escorias por lixiviación con H2SO4, FeSO4, (NH4)2SO3, FeS2, NaCl o FCl2 después de realizar un proceso de tostación a la escoria.
Además se ha mostrado que se logra extraer más del 90% de Cu, Co y Ni de una escoria de cobre que contenía 4,03% de Cu, 0,48% de Co y 1,98% de Ni mediante lixiviación a presión y a 130°C."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.3. 2.3 Modificación de la estructura de las Escorias de Cobre,"Como se ha mencionado en ítems anteriores, las escorias de cobre poseen estructuras amorfas y/o cristalinas, cuya proporción dependerá del tipo y tiempo de enfriamiento.
Es sabido también, que las escorias tienen un alto contenido de óxidos de silicio, el cual bordea generalmente entre 35% y 40% y óxidos de hierro que bordean entre 30% y 40%.
Los cuales al unirse forman redes cristalinas de silicatos confiriéndole el carácter vítreo a la escoria.
Cuando se habla de estructuras amorfas, se habla de redes de silicatos que no siguen un patrón que se repita a lo largo de la red como ocurre en las estructuras cristalinas; y se dice entonces que se está en presencia de un material en el que las partículas que lo conforman poseen una estructura desordenada.
En la fase cristalina de la escoria de cobre existe una red polimerizada de tetraedros de silicatos, en donde el óxido de silicio (SiO2) actúa como formador de red y el óxido de hierro actúa como modificador de red.
En esta estructura los tetraedros de SiO4 (-4) están unidos por puentes de oxígeno a través de fuertes enlaces covalentes en una configuración Si –
O
– Si.
Sin embargo, la incorporación de compuestos que sometidos a calor se descomponen produciendo óxidos metálicos, que a su vez se disocian dejando libres iones que se incorporan en la estructura de la red, interrumpe los enlaces Si –
O
– Si a través de la no formación de puentes de oxígeno (ver Figura 7); esta interrupción de enlaces permite que haya una cesión de cargas negativas a los cationes modificadores que formaran enlaces iónicos débiles, como por ejemplo Si –
O
– Fe o bien Si –
O
– Ca, dependiendo del catión que se incorpore.
Esta incorporación de cationes alcalinos a la red de silicatos contribuye a la despolimerización de la estructura de las escorias de cobre y con ello se puede lograr mejoras en la cinética de disolución de estas.
[8]
Figura 7: Rompimiento puentes de oxígeno.
De acuerdo con lo anterior, adicionando una alta proporción de óxidos metálicos se puede lograr un rompimiento sucesivo y aleatorio de la red cristalina de silicatos y formar grupos más pequeños de ellos hasta lograr producir unidades tetraédricas como se muestra en la Figura 8 y obtener por tanto una escoria con estructura despolimerizada.
Figura 8: Estructura de una solución fundida de óxido metálico en sílice, la proporción de óxido metálico es más alta en (b) que en (a), en (c) la disociación de la red de sílice es completa.
Una de las formas de evidenciar la despolimerización de las escorias es mediante la medición de la viscosidad de estas.
La cual, como se ha dicho en capítulos anteriores, es una propiedad de las escorias y una variable de importancia para procesos pirometalúrgicos.
Y que depende fuertemente de la composición de las escorias y de la temperatura a la que se encuentren.
También se ha dicho que en el caso de escorias fayalíticas la viscosidad se ve aumentada al existir SiO2 en un rango de composición de 0-30%.
Por lo que al adicionar una alta proporción de óxidos metálicos y elevar la temperatura, debería ocurrir que el rango de composición de SiO2 baje y el rompimiento de los enlaces que forma la sílice u oxido de silicio aumente; evidenciándose esto mediante una disminución de la viscosidad.
Se mencionó también que se han estudiado distintos métodos y modelos para poder medir la viscosidad con componentes ácidos y básicos, siendo uno de ellos el modelo desarrollado por Toguri, el que se basa en datos bibliográficos de la viscosidad y se ajusta a un índice denominado Modulo de viscosidad (Kv), que representa la relación porcentual “base/ácido”, conocida en la industria siderúrgica como índice de basicidad.
Este índice Kv, es mucho más sensible a cambios en la composición química que el índice de basicidad para las escorias básicas.
Sin embargo, ha sido aplicado a escorias fayalíticas para poder determinar la viscosidad mediante una ecuación que se ajusta a la de Arrhenius para la viscosidad.
Dicho incide viene dado por:
𝐾𝑣 = ∑ (%𝑃𝑒𝑠𝑜Ó𝑥𝑖𝑑𝑜𝑠 𝐵á𝑠𝑖𝑐𝑜𝑠)𝑛 𝑖=1
∑ (%𝑃𝑒𝑠𝑜Ó𝑥𝑖𝑑𝑜𝑠 Á𝑐𝑖𝑑𝑜𝑠)𝑛 𝑖=1 (2.1)
Y está relacionado con la viscosidad mediante la siguiente ecuación empírica obtenida a partir de la linealidad que siguen ciertas curvas obtenidas de una relación simple entre Kv y temperatura.
Esta ecuación es:
Esta ecuación es la que se ajusta a la ecuación de Arrhenius para la viscosidad.
[6] Anteriormente se ha tomado conocimiento de que las escorias de cobre pueden ver modificada su estructura a través de la incorporación de cationes alcalinos a las redes de silicatos que las forman, lo cual se puede llamar alcalinización.
Esta alcalinización de las escorias de cobre se lleva a cabo comúnmente en la elaboración de cemento y hormigones donde se incorporan las escorias; y las fuentes alcalinas empleadas puede ser hidróxidos alcalinos, silicatos, carbonatos, sulfatos, aluminatos u óxidos; esencialmente cualquier sustancia soluble que puede suministrar cationes de metales alcalinos."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,2.4. 2.4 Alcalinización de las Escorias de Cobre,"e han clasificado seis grupos de activadores alcalinos de acuerdo con su composición química, siendo estos:
 Hidróxidos alcalinos: MOH
 Sales de ácidos débiles: M2CO3, M2SO3, M3PO4, MF, etc.
 Silicatos: M2O·nH2O
 Aluminatos: M2O·nAl2O3
 Aluminosilicatos: M2O·Al2O3·(2-6)SiO2
 Sales de ácidos fuertes: M2SO4
De estas categorías, los productos químicos con mayor disponibilidad son: NaOH, Na2O·nSi2, Na2SO4 y CaCO3.
Como se dijo, una de las tareas de la fuente alcalina es elevar el pH de la mezcla de reacción, puesto que esto contribuye a la disolución de los aluminosilicatos presentes y acelera la reacción.
En este punto los carbonatos y sulfatos confieren condiciones moderadas de alcalinidad y generan óxidos libres para el proceso de activación.
[9] Cuya generación de óxidos libres aportan oxígeno para la despolimerización de óxidos ácidos, lo que su vez contribuye al rompimiento de redes de estos óxidos ácidos.
Al usar carbonato de calcio como reactivo para alcalinizar escorias de cobre las reacciones que ocurren en presencia de calor son las siguientes:
El óxido producido, se disuelve en la sílice fundida y se disocia dejando un catión y un anión libres de acuerdo con la reacción:
𝐶𝑎𝑂 → 𝐶𝑎+2 + 𝑂−2
(2.4)
En donde el anión en presencia de cationes Si+4 se asocia a este según la reacción siguiente:
𝑆𝑖+4 + 4𝑂−2 → 𝑆𝑖𝑂4 −4
(2.5)
De estudios anteriores es sabido que al existir un porcentaje de óxido de calcio disuelto en las escorias de cobre entre un 0 a 38%, sumado a proporciones de SiO2 y FeO entre 2641% y 28-75% respectivamente [10], se estaría trabajando en la zona de estabilidad de la olivina según el diagrama siguiente:
Figura 9: Diagrama de fases Ternario CaO – FeO – SiO2.
Es así como al adicionar carbonato de calcio, no solo se logra modificar la acidez de una escoria hacia un grado más básico, sino que también se puede modificar la estructura física de ella.
Esto mediante el rompimiento de los enlaces antes mencionados; pudiendo con este rompimiento modificar la viscosidad de la escoria.
Además de ello la adición de carbonato, aparte de contribuir al rompimiento de los enlaces y la transformación de escoria, el someter ésta a una temperatura cercana a la de fusión de cada uno de los componentes de la escoria.
Pero para encontrar la temperatura adecuada se debe utilizar un método que relacione la composición de la escoria con la temperatura de fusión de cada compuesto.
De estudios anteriores se sabe que existe un método ampliamente usado y es el desarrollado por Lathe en 1951, el cual incluye la incidencia de óxidos menores mediante el análisis de atracción ión-oxígeno que existe entre estos compuestos.
[11]"
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,3. CAPITULO III: METODOLOGIA EXPERIMENTAL,
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,3.1. 3.1 Modelaciones previas,"En una primera instancia, se llevaron a cabo modelaciones usando el simulador ThermoCalc, teniendo como composiciones base de escoria de cobre las que se detallan en el ítem 2.1.2. En esta instancia se contempló la realización de modelaciones a presión parcial de O2 constante de 10-5 atmosferas, diferentes temperaturas y usando composiciones por elementos emanadas de balances de masa realizados para este fin.
Las temperaturas a las que se llevaron a cabo las modelaciones son 1200, 1150, 1100 y 1050 grados Celsius.
Con esto se esperaba establecer las condiciones a las cuales se logra la transformación de escoria tipo fayalítica a pseudo-olivínica; sin embargo al realizar una revisión de los resultados obtenidos se llega a la conclusión de que el simulador no cuenta con la data termodinámica suficiente para poder modelar la transformación de escoria.
Esto se pudo concluir puesto que se observó que la mayor cantidad de moles de cobre presentes en la escoria se encontraban formando compuestos en estado gaseoso y una menor cantidad en una fase rica en hierro (ver resultados en ANEXO).
Situación que gatilla trasladar el estudio hacia un análisis de cambio de propiedades físicas de la escoria tras someterla a la transformación antes mencionada.
Para este propósito, se llevan a cabo los pasos y cálculos que a continuación se mencionan."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,3.2. 3.2 Cálculo de masa de carbonato de calcio a adicionar,"El cálculo de masa de carbonato de calcio (CaCO3) a adicionar al sistema se obtiene a partir de un balance de masa que viene dado por la reacción química que surge de la mezcla del carbonato y los compuestos de la escoria de cobre a estudiar, que en este caso es una escoria de la Fundición Hernán Videla Lira; cuya composición es la indicada en la Tabla
3.
La reacción química que se establece para este estudio involucra solo los principales agentes actores en la transformación, y no se incluyen metales nobles como oro, plata o impurezas como plomo, arsénico o antimonio.
Teniendo en cuenta esto la reacción que se plantea es la siguiente:
En esta reacción, el reactivo límite es el CaCO3 y de la cantidad que de él se adicione depende la formación de los compuestos que conforman la escoria olivina.
En este punto se calcula también el porcentaje en peso de los compuestos del diagrama ternario correspondiente a las escorias tipo olivínicas.
Pudiendo así corroborar que se obtienen composiciones dentro del rango de CaO, FeO y SiO2 que, según indica la literatura, es donde se está en la región olivínica."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,3.3. 3.3 Cálculo de módulo de viscosidad de escoria,
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,3.4. 3.4 Cálculo de viscosidad de escoria,"Para el cálculo del módulo de viscosidad se utiliza la fórmula (2.1) y se lleva a cabo inicialmente una normalización de los porcentajes y masas de los compuestos involucrados en la fórmula de manera que estos sumen un total de 100%.
Adicional a ello, se varía la masa de CaCO3 según los valores obtenidos del balance másico del paso anterior, para así poder estudiar el efecto que ello produce en el valor del módulo de viscosidad mediante el cambio de los porcentajes másicos de los compuestos ácidos y básicos incidentes."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,3.5. 3.5 Cálculo de temperatura ponderada de fusión de escoria,"Una vez calculado el módulo de viscosidad (Kv), es posible obtener la viscosidad utilizando la ecuación empírica (2.2), obtenida a partir de la linealidad que siguen ciertas curvas obtenidas de una relación simple entre Kv y temperatura.
Con ello es posible obtener varios valores de viscosidad, graficarlos y analizar el cambio en la viscosidad de la escoria.
En este punto, las temperaturas usadas para el cálculo de viscosidad, son las de la región de estabilidad de escoria olivina; las cuales van desde los 1100[°C] a los 1250[°C] aproximadamente (ver Figura 9).
3.5 Cálculo de temperatura ponderada de fusión de escoria
El cálculo de la temperatura ponderada de fusión se obtiene a partir del método desarrollado por Lathe en 1951. El cual consiste en seleccionar al menos tres diagramas ternarios con la mayor representatividad de composiciones químicas de la escoria.
Para esto las composiciones de los óxidos de cada ternario escogido deben sumar más de un 75%.
[11] En este caso y teniendo en cuenta esta condición se escogieron los ternarios (FeO-SiO2-Al2O3), (FeO-SiO2-CaO) y (FeO-SiO2-Fe2O3), los cuales suman 81%, 80% y 82%, respectivamente (ver ANEXO).
Además de ello se debe incorporar el efecto de los componentes no incluidos en los ternarios, en este caso MgO; para ello se hace un cálculo de fracción molar de acuerdo a la expresión mostrada en ANEXO y que es evaluada en base al efecto de la atracción ión-oxígeno del Mg+2.
Obteniendo esto, se debe normalizar las composiciones de la escoria y encontrar con ellas las temperaturas de fusión en los diagramas ternarios.
Hecho esto se procede a usar la siguiente relación:
𝑻𝑷𝑭̅̅ ̅̅ ̅̅ = ∑(∑ (%𝑴𝒙𝑶𝒚) 𝒊 ∗𝑻𝒇𝒖𝒔𝒊ó𝒏𝒊)𝟑 𝒊=𝟏 𝒋
∑(∑ (%𝑴𝒙𝑶𝒚)
𝒊)𝟑 𝒊=𝟏 𝒋 (3.1)
Donde %𝑀𝑥𝑂𝑦 es la composición de cada óxido de la escoria.
Dicho cálculo se realiza para cada porcentaje en peso normalizado obtenido para el rango masa de CaCO3 calculada a partir del balance másico.
Y las temperaturas de fusión de los compuestos incidentes en la transformación corresponden a la temperatura de los compuestos en estado sólido; ya que los reactivos (escoria y carbonato), en un eventual experimento, se adicionarían en estado sólido.
𝑻𝑷𝑭̅̅ ̅̅ ̅̅ = ∑(∑ (%𝑴𝒙𝑶𝒚) 𝒊 ∗𝑻𝒇𝒖𝒔𝒊ó𝒏𝒊)𝟑 𝒊=𝟏 𝒋
∑(∑ (%𝑴𝒙𝑶𝒚)
𝒊)𝟑 𝒊=𝟏 𝒋 (3.1)
Donde %𝑀𝑥𝑂𝑦 es la composición de cada óxido de la escoria.
Dicho cálculo se realiza para cada porcentaje en peso normalizado obtenido para el rango masa de CaCO3 calculada a partir del balance másico.
Y las temperaturas de fusión de los compuestos incidentes en la transformación corresponden a la temperatura de los compuestos en estado sólido; ya que los reactivos (escoria y carbonato), en un eventual experimento, se adicionarían en estado sólido."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,4. CAPITULO IV: RESULTADOS Y DISCUSION,
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,4.1. 4.1 Masa de carbonato de calcio a adicionar para lograr transformación de escoria,"Del balance másico y químico realizados para poder encontrar una masa o rango de masas de carbonato de calcio (CaCO3) a adicionar para poder lograr una transformación de escorias; se obtiene que para un numero de 6 pruebas, donde la diferencia de masa entre las masas obtenidas es de 0,7[g], el rango de masa de carbonato que se puede adicionar esta entre los 19,3 y 22,8[g].
Tabla 4: Masa de carbonato de calcio a adicionar para lograr transformación de escoria.
 | N° prueba | Masa CaCO3 [g]
 | --- | ---
 | 1 | 19,3
 | 2 | 20,0
 | 3 | 20,7
 | 4 | 21,4
 | 5 | 22,1
 | 6 | 22,8

Se toma este rango de masas ya que, tras el análisis de composiciones de los productos obtenidos según la reacción planteada en el punto (3.1) de la metodología experimental, son los que permiten estar en el rango de composiciones de FeO, CaO y SiO2 en que se mueve una escoria olivina; los cuales, según dicta la literatura están entre un 28 a 75% de FeO, 0 a 38% de CaO y 26 a 41% de SiO2.
En la Tabla 5, se observa que el resultado de las composiciones de la escoria que resultaría está en los rangos de composición mencionados, por lo que se puede validar el rango de masas de carbonato de calcio a adicionar en un eventual experimento de validación.
Teniendo en cuenta lo anterior y considerando que la base de cálculo de escoria para la cual se hacen los análisis es 100[g], la razón másica promedio entre escoria y carbonato de calcio estaría en torno a 0,21[-].
Tabla 5: Composición de escoria pseudo-olivínica en relación a los componentes principales.
 | N° prueba |  | Masa CaCO3 [g] | %peso FeO CaO | SiO2
 | --- | --- | --- | --- | ---
 | 1 | 19,3 | 27,7 | 10,8 | 34,8
 | 2 | 20,0 | 28,7 | 11,2 | 36,0
 | 3 | 20,7 | 29,7 | 11,6 | 37,3
 | 4 | 21,4 | 30,7 | 12,0 | 38,5
 | 5 | 22,1 | 31,7 | 12,4 | 39,8
 | 6 | 22,8 | 32,7 | 12,8 | 41,1
"
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,4.2. 4.2 Módulo de viscosidad y viscosidad de escoria al adicionar CaCO3,"Los resultados para módulo de viscosidad (Kv) (Tabla 10, ANEXO), muestran que conforme se aumenta la adición de carbonato de calcio, aumenta también el módulo para una temperatura de prueba constante.
Esto se repite para cada temperatura de la región de estabilidad de escoria olivina que se evalúa.
(ver Tablas y Gráficos en ANEXO)
Como se dijo anteriormente, el módulo de viscosidad (Kv) tiene una relación inversamente proporcional con la viscosidad y de acuerdo a los resultados obtenidos, esta relación también se cumple para este caso de estudio.
Además de ello, existe esta misma relación entre la masa de carbonato de calcio adicionada y la viscosidad.
4,500 4,000 3,500 3,000 2,500
1,500 2,000 1,000
0,000 0,500 1,800 1,850 1,900 1,950 2,000 2,050 2,100 Kv [-]
Gráfico 1: Viscosidad vs Módulo de viscosidad para 1100[°C].
En el Gráfico 1 se evidencia con mayor claridad la relación existente entre módulo de viscosidad y viscosidad.
En él, se puede ver también que el cambio de la viscosidad para este caso es prácticamente lineal.
4,000 4,500 3,500 3,000 1100 [°C] 1150 [°C] 1200 [°C] 1250 [°C] 2,500 2,000 1,500 1,000 0,500
0,0 5,0 10,0 15,0 20,0 25,0 Masa de CaCO3 [g] Gráfico 2: Cambio de viscosidad respecto al aumento de masa de CaCO3 y temperatura.
A partir de los resultados graficados en el Gráfico 2, se puede rescatar que la viscosidad de la escoria decrece al adicionar carbonato de calcio y al aumentar la temperatura.
Sin embargo, la mayor disminución de viscosidad respecto a la viscosidad de la escoria sin adicionar carbonato se da al trabajar a una temperatura de 1100[°C].
Y no tan solo se evidencia una mayor disminución, sino que tras comparar las pendientes de las curvas se puede evidenciar que en el caso de los 1100°[C] la velocidad de cambio es mayor que en los demás casos.
Ahora, si bien es evidente que en el caso de los 1100[°C] el cambio de viscosidad es mayor, para una temperatura de 1250[°C] es donde se obtienen las viscosidades más bajas.
Con estos resultados, se puede decir que al trabajar con masas de carbonato de calcio entre el rango de 19,3 a 22,8[g] y una temperatura de 1250[°C], se lograría disminuir la viscosidad de la escoria en un 37,3% al adicionar la mayor masa de carbonato de calcio; lo que en términos de propiedades físicas de la escoria evidencia un cambio que se traduce en un rompimiento de las redes tetraédricas de la sílice logrando por tanto, despolimerizarla.
Este cambio en las propiedades de la escoria supone, por tanto, que la 0,000 escoria al solidificar mediante un proceso acelerado, vale decir, mediante un granallado, presentaría una red cristalina debilitada; lo cual, para efectos de refractariedad, se traduce en una escoria menos refractaria al ser lixiviada, pudiendo con este cambio apostar por obtener mayores recuperaciones de cobre frente a un proceso de este tipo."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,4.3. 4.3 Temperatura ponderada de fusión,"Las temperaturas ponderadas de fusión obtenidas en este estudio se muestran en la Tabla 13 de ANEXO y evidencian que la temperatura ponderada de fusión de la escoria al no agregar carbonato de calcio es de 1161,2[°C], mientras que al adicionar la primera cantidad de carbonato del rango de masas obtenido, la temperatura ponderada de fusión disminuye a 1159,9[°C], es decir, disminuye 1,3[°C].
Presentando en todos los casos de masa de carbonato adicionado una variación entre temperaturas de a lo más 0,05[°C], por lo que se puede decir que, si bien existe una tendencia a la baja de la temperatura ponderada de fusión al adicionar más masa de carbonato de calcio, no existe una disminución significativa de la temperatura ponderada de fusión puesto que la temperatura baja desde los 1159,9[°C] a los 1159,7[°C].
Esta tendencia a la baja se evidencia al observar el Gráfico
3.
Como se explicó en el ítem 3.5 de la metodología experimental, la temperatura ponderada de fusión depende del porcentaje en peso de los compuestos que forman los diagramas ternarios con la mayor representatividad de composiciones químicas de la escoria.
Por lo que haciendo un análisis de la poca variación de la temperatura ponderada de fusión, se puede decir que al obtener un rango acotado de masa de carbonato de calcio a adicionar para poder lograr una transformación de escorias, se tendrá un rango acotado de masa de óxido de calcio (CaO) tras la descomposición del carbonato de calcio.
Lo cual hace que la composición de óxido del calcio original de la escoria se vea poco modificada y con ello se vea poco modificado también el rango de variación de la composición del CaO en los diagramas ternarios usados para obtener las temperaturas necesarias para hacer el cálculo de la temperatura ponderada de fusión.
Si bien no existe una disminución significativa de la temperatura ponderada de fusión y el rango de temperaturas ponderadas de fusión obtenido es acotado, este se encuentra dentro de las temperaturas de equilibrio de la región olivina; por lo que se obtiene otro punto para aseverar que se logra transformar la escoria al adicionar las cantidades de carbonato de calcio encontradas al llevar a cabo pruebas de validación.
1161,30
1160,80
1160,30
1159,80
1159,30 0,0 5,0 10,0 15,0 20,0 25,0
Gráfico 3: Temperatura ponderada de fusión para cada masa de carbonato adicionada.
Cruzando estos resultados de temperatura con las pruebas de cambio de viscosidad, se puede decir que al trabajar a estas temperaturas se logra disminuir la viscosidad de la escoria llegando a una viscosidad que estaría entorno a los 1,36[poise] para una temperatura ponderada de fusión de 1159,7[°C], tal como se puede apreciar en el Gráfico 1159,9 1159,9 4. De donde además se puede observar que conforme aumenta la cantidad de carbonato de calcio adicionado, disminuye tanto la temperatura ponderada de fusión como la viscosidad de la escoria.
Por lo que, se puede decir que al existir un cambio de viscosidad conforme se adiciona carbonato de calcio a la escoria, se logra el rompimiento de las redes de silicatos y por ende se logra despolimerizar la escoria.
Concluyendo a partir de todo ello, que se logra la transformación de escoria fayalítica a escoria pseudo-olivínica, una disminución del carácter vítreo de esta a causa del rompimiento de la red cristalina conformada por los silicatos y por tanto, una pérdida de refractariedad de la escoria a ser lixiviada.
1160,0
1159,8 1159,8 1159,7 1159,7 1,460 1,420 1,440 1,400 1,380 1,360 1,340 19 19,5 20 20,5 21 21,5 22 22,5 23
Gráfico 4: Cambio de viscosidad y temperatura ponderada de fusión conforme la adición de carbonato de calcio a la escoria."
ANÁLISIS DEL CAMBIO DE PROPIEDADES FÍSICAS DE ESCORIA DE COBRE TRAS UNA TRANSFORMACION DE ESCORIA FAYALÍTICA A PSEUDO-OLIVÍNICA,5. CAPITULO V: CONCLUSIONES Y RECOMENDACIONES,"De todo el trabajo anterior, se concluye que adicionando carbonato de calcio a una razón másica promedio de 0,21[-] entre la masa de escoria de cobre tipo fayalítica y carbonato de calcio, se puede lograr una transformación de escoria fayalítica a escoria pseudoolivínica.
Esto dado que al analizar la composición del producto formado a partir de la mezcla de escoria y carbonato, se obtiene que la composición del CaO estaría entre un 10,8 y 12,8%, del FeO entre un 27,7 y 32,7% y del SiO2 entre un 34,8 y 41,1%.
Lo cual sitúa la composición en el rango de composición de las escorias olivinas.
En cuanto al análisis del cambio de viscosidad de la escoria al adicionar carbonato de calcio, se obtiene que para las temperaturas de prueba estudiadas, la viscosidad disminuye conforme se aumenta la masa de carbonato adicionada.
Por lo que existiría una relación inversamente proporcional entre ellas.
Encontrándose las viscosidades más bajas al probar a una temperatura de 1250[°C].
Con estos resultados, se puede decir que al trabajar con masas de escoria y carbonato de calcio a razón másica de 0,21[-] y a una temperatura de 1250[°C], se lograría disminuir considerablemente la viscosidad de la escoria; lo que en términos de cambio físico de la escoria, significa que hay un rompimiento de las redes tetraédricas de la sílice logrando por tanto, despolimerizarla, disminuir el carácter vítreo de esta y con ello una pérdida de refractariedad de la escoria a ser lixiviada.
Respecto al análisis de las temperaturas ponderadas de fusión encontradas se concluye que, el rango de temperaturas obtenido se encuentra dentro de las temperaturas de equilibrio de la región olivina, que hay muy poca variación entre las temperaturas obtenidas para cada masa de carbonato de calcio adicionado, por lo que al obtener un promedio de estas temperaturas se puede decir que la transformación de escoria se lograría a los 1160[°C] aproximadamente.
Además de ello, a esta temperatura se estaría logrando un cambio en la viscosidad de la escoria y por ende un grado de despolimerización de esta.
Dado todo lo anterior, las condiciones para lograr la transformación de escoria fayalítica a pseudo-olivínica serían adicionar escoria y carbonato de calcio hasta alcanzar una razón de 0,21[-], llevar a cabo la fusión alcalina a 1160[°C] aproximadamente y bajo una presión parcial de oxigeno de alrededor de 10-5[atm].
Estas condiciones permitirían poder hacer una validación del estudio realizado y aterrizar el análisis físico-químico aportado por este trabajo."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,1. Introducción,"Los organismos vivos se encuentran deﬁnidos por material genético, principalmente caracterizado por el ADN, el cual es una pieza fundamental ya que contiene las instrucciones para el desarrollo del ser vivo.
Además existe otra molécula llamada ARN, que es obtenida luego de copiar el ADN mediante el proceso de transcripción, y que una vez codiﬁcada servirá para convertirse en proteı́nas, las que cumplirán una gran cantidad de funciones en el organismo.
Por eso es que se han desarrollado diversas técnicas a lo largo de los años para obtener el ARN de un ser vivo y entender el funcionamiento del organismo en ese mismo instante de tiempo.
Algo ası́ como tomar una fotografı́a del material genético para luego poder observar cuáles son los genes que están participando en el desarrollo.
En el tiempo se ha trabajado para seguir mejorando aquellos métodos, llegando a la secuenciación del ARN (RNA-Seq) que en los últimos años se ha convertido en la solución más preferida por los investigadores.
Esta metodologı́a entrega gran poder en el estudio de los seres vivos, y es posible aplicarla en la caracterización de enfermedades u otras condiciones biológicas.
Una aplicación de este tipo de trabajos puede ser la identiﬁcación de etapas fenológicas en plantas, distinguiendo ası́ cuáles son los genes protagonistas en cada etapa de crecimiento.
Paralelamente, la producción de uva y la industria del vino en los últimos años se han convertido en una de las actividades más importantes para Chile, ubicando al paı́s en octavo lugar a nivel mundial gracias a su nivel de producción de aquella bebida.
Esto hace que se observe con atención la obtención de la uva en todo su proceso de crecimiento, siendo de gran importancia la etapa fenológica en la que se encuentra la vid.
La disminución en costos de secuenciación de ARN y la gran importancia económica que tienen las plantaciones de uva en nuestro paı́s motivan este trabajo, que busca evaluar el estado del arte del análisis RNA-Seq en plantas, en base a eso generar un ﬂujo de trabajo RNA-Seq rápido, y por último desarrollar un prototipo de plataforma web que presente un resumen del análisis al agricultor.
En el capı́tulo 1 de este documento se estudia el contexto de genómica y bioinformática sobre el cual se trabaja, además de presentar los objetivos generales y especı́ﬁcos del proyecto.
En el capı́tulo 2 se entrega el estado del arte alrededor de las herramientas bioinformáticas disponibles para el análisis RNA-Seq.
El capı́tulo 3 presenta la solución propuesta para abordar los objetivos planteados.
El capı́tulo 4 habla de la ejecución y se comentan los resultados obtenidos, para ﬁnalmente dar paso a las conclusiones de esta memoria."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2. Definición del Problema,"La vid (Vitis vinifera) es una de las plantas más cultivadas en el mundo, teniendo 7,5 millones de hectáreas de terreno vitı́cola y alcanzando 76,8 millones de toneladas de uva a nivel mundial en el año 2016, de acuerdo al último balance de la OIV [4].
La producción de uva tiene como ﬁn su comercialización como fruta, jugo o pasas, aunque en su mayorı́a es utilizada para su fermentación en vino.
En las últimas décadas la industria del vino ha sido una de las más importantes para Chile, donde la superﬁcie vitivinı́cola estimada en 2016 fue de 214 mil hectáreas y aquella destinada exclusivamente a la producción de vino fue de 141,9 mil hectáreas [5].
Además, según la OIV Chile se encuentra en octavo lugar a nivel mundial en cuanto a producción de vino, con 10,1 millones de hectolitros en el último año [6].
Es por esto que, con el ﬁn de entregar resultados de calidad, la producción de uva es una actividad que requiere mucha rigurosidad y precisión en todo su desarrollo, de principio a ﬁn.
La cosecha de la uva (llamado vendimia en el caso de las uvas destinadas a la producción de vino) se realiza una vez alcanzada la maduración de la planta.
Es un proceso que requiere mucha precisión, puesto que la calidad de la uva obtenida depende en gran parte del momento en el cual se elige cosechar, afectando directamente al producto ﬁnal.
Con el objetivo de obtener una producción de uvas de calidad, se hace importante supervisar el crecimiento de la planta y cómo el clima y el entorno afectan el desarrollo de ésta.
El estudio de aquello es conocido como fenologı́a.
Para controlar el crecimiento de la vid, durante el último siglo se han desarrollado distintos sistemas de referencia, los cuales entregan información de los distintos estados fenológicos de la planta.
El año 1952, Baggiolini plantea la primera escala para describir las etapas de crecimiento de la vid [7], con el ﬁn de permitir una buena sincronización en cuanto a medidas de protección de la planta.
Esta escala se caracteriza por tener poca precisión y por describir solamente hasta la etapa de cuaje.
Eichhorn y Lorenz en el año 1977 publican una nueva forma de describir el crecimiento de la planta, la cual mejora las deﬁciencias presentes en el método anterior, con un sistema más detallado que cubre 22 etapas desde el comienzo hasta la caı́da de las hojas [8].
En 1993, Baillod y Baggiolini [9] modiﬁcaron la escala creada por Baggiolini en 1952, añadiéndole 7 nuevas etapas que van desde cuaje hasta la caı́da de hojas.
Lorenz et al.
[10] publican en 1994 el sistema BBCH modiﬁcado, el cual nace como un modelo para la Unión Europea y adapta el método BBCH estándar para la vid.
Finalmente, en el año 1995, Coombe [11] presenta una modiﬁcación a la escala de Eichhorn y Lorenz, la cual además de deﬁnir claramente las etapas más importantes, entrega un mayor detalle sobre las etapas intermedias.
En la actualidad se utilizan estas metodologı́as como sistema de referencia para determinar de manera visual el estado fenológico, poniendo especial atención a las caracterı́sticas fı́sicas de la planta.
Lo anterior se puede traducir en que esta tarea depende completamente de la percepción humana en ese momento, lo que lo convierte en un método inmediato pero poco preciso.
En este trabajo se explora la posibilidad de ir más allá de la mera observación y propone un camino para determinar el estado fenológico a partir de las propiedades genéticas de la planta en distintas etapas de su crecimiento."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.1. Genómica y bioinformática,"Para comprender esta memoria se hace necesario introducir las bases acerca de genes, genética y genómica."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.1.1. Genes,"El término “gen” hoy posee una deﬁnición no muy precisa.
Inicialmente, esta palabra era utilizada para indicar la unidad de herencia de un fenotipo, y luego este signiﬁcado continuó siendo usado en un contexto no cientı́ﬁco para referirse a la expresión de cierta caracterı́stica en un individuo; por ejemplo,”gen del cabello rubio”, o”gen de los ojos azules”.
Con el avance en investigaciones fue posible establecer que las proteı́nas están compuestas por muchos aminoácidos, y a su vez, aquellos aminoácidos se encontraban codiﬁcados por regiones cromosómicas que podı́an ser identiﬁcadas genéticamente [12] (ver Figura 1.1).
Figura 1.1: Pirámide de complejidad de la vida, según Oltvai y Barabási [1].
Finalmente, junto con la aparición de la secuenciación de ADN fue posible considerar al gen como una región molecular compuesta de nucleótidos, pudiendo ası́ ser identiﬁcado en la región cromosómica que codiﬁca al polipéptido.
En organismos eucariontes el gen es una sección compuesta de exones e intrones, en donde solamente los exones sirven para codiﬁcar la proteı́na y los intrones son la parte no codiﬁcante de esta."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.1.2. Genética,"Como su nombre lo indica, genética es el nombre que recibe el estudio de los genes y el rol que ellos juegan en herencia.
Durante la historia, la herencia siempre ha sido un tema de interés para las personas.
Incluso desde antes que la biologı́a existiera como una rama de la ciencia, la gente buscaba formas para mejorar su cultivo de plantas o la cruza de animales.
No fue hasta mediados del siglo XIX, y reconocido de manera póstuma a comienzos del siglo XX, que la genética fue planteada formalmente con el trabajo de Gregor Mendel.
Estos estudios deﬁnen un conjunto de principios y procedimientos analı́ticos que marcan el inicio de la genética como el estudio de herencia, genes y variación genética [13]."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.1.3. Genómica,"El conjunto completo de ADN dentro de un organismo es llamado genoma.
En cuanto a la especie humana, cada célula dentro del cuerpo contiene una copia completa de aproximadamente tres mil millones de pares de bases, conformando entre todas el genoma humano [14].
Con el desarrollo de técnicas de ADN recombinante, en 1977 Fred Sanger et al.
[15] marcan la aparición de la genómica mediante el desarrollo de un nuevo método de secuenciación de ADN, pudiendo ası́ encontrar el genoma de 5400 nucleótidos del virus φX174 [16, capı́tulo 21].
Con el tiempo fueron secuenciados distintos otros genomas, pero como el proceso era lento y laborioso, el estudio se limitó a organismos con genomas pequeños.
A mediados de la década de 1980, investigadores interesados en el genoma humano utilizaron técnicas de ADN recombinante para mapear secuencias de ADN en ciertos cromosomas.
Ası́ se estiman, aunque incorrectamente, aproximadamente unos 100.000 genes en el genoma humano, y es posible notar que con los métodos existentes hasta esa fecha, encontrar el genoma humano serı́a una tarea larga y difı́cil.
En las tres décadas siguientes, con el aumento del poder de cómputo y el mejoramiento en las tecnologı́as de secuenciación de ADN, fue posible considerar el estudio de genomas eucariontes más largos y complejos, incluyendo ﬁnalmente los tres mil millones de genes que comprende el genoma humano [16, capı́tulo 21]."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.1.4. Bioinformática,"La bioinformática es el campo de la ciencia que usa herramientas computacionales (hardware y software) y aplicaciones matemáticas para el ordenamiento y análisis de datos biológicos como estructura de genes, secuencias de genes, expresión génica y estructura y función de proteı́nas [16].
Según Ramsden [17], bioinformática puede deﬁnirse simplemente como”la aplicación de ciencias de la información en la biologı́a”.
Entre algunas aplicaciones, las tecnologı́as bioinformáticas son usadas para identiﬁcar correlaciones entre secuencias de genes y enfermedades, para predecir estructuras de proteı́nas a partir de secuencias de aminoácidos, para aportar en el diseño de nuevos fármacos, o para adaptar tratamientos a pacientes a partir de su secuencia de ADN."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.2. Proceso de transcripción,"Las células procariotas son aquellos organismos unicelulares que carecen de un núcleo deﬁnido, teniendo todos sus componentes (incluyendo el material genético) repartidos en el citoplasma.
El resto de los organismos vivos se conocen como eucariontes, y están compuestos por una o más células del mismo nombre.
Tanto en aquellas como en las procariotas, el ADN codiﬁca toda la información necesaria sobre las propiedades y funciones de cada célula; cierta secuencia del material genético se copia en el ARN y luego se traducirá en proteı́na.
A diferencia de los organismos procariotas, las células eucariontes contienen gran parte del material genético dentro de su núcleo y es aquı́ donde se realiza el proceso de transcripción.
En palabras simples, la transcripción es el proceso en el que la secuencia de ADN es copiada para formar una molécula de ARN.
Una enzima llamada ARN polimerasa tiene como tarea copiar el ADN, para lo cual separa sus dos hebras y se acopla a una de ellas, la cadena molde, con el ﬁn de codiﬁcar el ARN.
El resultado de esta etapa es una molécula (o transcrito) de ARN llamada pre-ARNm, o pre ARN mensajero, la cual está compuesta de nucleótidos en donde cada uno de ellos es el complemento de la base nucleotı́dica en la hebra de ADN (ver Figura 1.2).
Es decir, si en la hebra de ADN se encuentra presente la secuencia TACTAG1, la hebra de ARN codiﬁcará el complemento de aquellas bases2 como CUAGUA3 (AUGAUC en la dirección opuesta).
Figura 1.2: ARN polimerasa copiando la cadena de ARNm a partir de la cadena molde [2].
El siguiente paso es transformar el transcrito primario (pre-ARNm) en una molécula que más adelante pueda convertirse en proteı́na.
Actualmente el pre-ARNm está compuesto de dos secciones importantes llamados exones e intrones, y que se encuentran presentes de forma intercalada a lo largo de la hebra.
Los exones son aquellas partes codiﬁcantes y que serán útiles para que la proteı́na pueda cumplir la función que se espera de ella.
Por el contrario, las secciones que reciben el nombre de intrones poseen material que no sirve y es inútil para el proceso de traducción (de ARNm a proteı́na).
Es por esta razón que los intrones son ﬁltrados, eliminados, y los exones se unen entre ellos en la etapa llamada empalme, o splicing.
Finalmente de lo anterior resulta una cadena llamada ARN mensajero (abreviado ARNm) o 1A: adenina, T: timina, C: citosina, G: guanina, U: uracilo.
2Las parejas de nucleótidos que sirven de complemento para el otro en el caso del ADN son: A-T y C-G.
Para el ARN son: A-U y C-G.
3El complemento de adenina en el ARN es uracilo en lugar de timina.
ARNm maduro, el que luego saldrá del núcleo celular y se dirigirá al ribosoma donde comienza la etapa de traducción, que tiene como ﬁnalidad convertir aquel ARN mensajero en péptido o proteı́na."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.3. Análisis de expresión diferencial,"A partir de muestras biológicas de células o tejido de un organismo es posible realizar un análisis de su transcriptoma, lo que es útil para lograr el entendimiento de ciertas variaciones fenotı́picas en biologı́a, como lo son las enfermedades [18].
A partir de dos muestras de material genético en condiciones biológicas diferentes, por ejemplo una extraı́da de un organismo sano y la otra obtenida de uno enfermo, es posible aplicar distintas técnicas como microarrays o RNA-Seq para encontrar cuáles son los genes expresados para cierta condición biológica; esto se encuentra evaluando cuáles genes son los que presentan una abundancia considerable comparado con la otra muestra.
Este estudio es conocido como análisis de expresión diferencial."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.3.1. Microarrays,"Durante las décadas pasadas, los microarrays (chips de ADN en español) tuvieron un papel importante en el análisis de expresión diferencial, ya que hasta hace poco tiempo era una de las técnicas más usadas para encontrar genes diferencialmente expresados.
Se extraen muestras de ARNm de un organismo en condiciones normales y de otro enfermo, tiñendo cada una de un color distinto.
Por ejemplo, la muestra en condiciones normales será de color azul y la muestra enferma de color amarillo.
Luego se combinan y se aplican al chip microarray, donde mediante análisis de imagen se pueden observar los niveles de hibridación de los genes en estudio mediante una señal de ﬂuorescencia.
Si el color observado para cierto gen es amarillo, entonces ese gen se encuentra expresado con mayor intensidad en la muestra enferma.
De forma análoga, si para un gen se observa un color azul entonces ese gen estará expresado mayormente en el organismo de condiciones normales.
Si el color que se obtiene para un determinado gen es el verde o alguna mezcla entre azul y amarillo, entonces aquel gen se expresa de manera similar en ambas muestras."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.3.2. Secuenciación de ARN (RNA-Seq),"En [19] se describe a RNA-Seq como un conjunto de métodos experimentales y computacionales utilizados para determinar la identidad y abundancia de secuencias de ARN en muestras biológicas.
La metodologı́a involucra el aislamiento del ARN de la célula o tejido, la preparación de la librerı́a para el ARN en la muestra, la secuenciación quı́mica de la librerı́a, y el análisis bioinformático de los datos obtenidos.
La secuenciación de ARN (RNA-Seq) tiene como objetivo identiﬁcar la secuencia, estructura y abundancia de las moléculas de ARN en una muestra biológica [19, capı́tulo 1].
La palabra secuencia se reﬁere al orden especı́ﬁco que tendrán las bases A, C, G y U. Estructura del ARN hace referencia a la organización que tiene la molécula en cuanto a elementos como el promotor, uniones intrón-exón, regiones sin traducir y la cola poli(A).
Por último, con abundancia en el ARN se reﬁere a la cantidad numérica que tendrá cada secuencia (gen) en la muestra.
A diferencia de la secuenciación de ADN, la cual es útil para obtener un perﬁl genético de un organismo, la secuenciación de ARN entrega solo aquellas secuencias que se encuentran activamente expresadas en el organismo en el instante de tomar la muestra.
Para ponerlo de una forma más cotidiana, obtener la secuencia de ARN de un organismo es como hacer una foto de su material genético en ese momento.
RNA-Seq destaca por sobre los métodos anteriores como microarrays gracias al alto rendimiento que ofrecen las plataformas de secuenciación actuales, la sensibilidad que entregan las nuevas tecnologı́as de secuenciación, y la habilidad de descubrir transcritos nuevos, modelos de genes y especies de ARN pequeños no codiﬁcantes [19, capı́tulo 1].
En cuanto al análisis de expresión diferencial, el método de RNA-Seq diﬁere del realizado con microarrays en que los datos observados usando el primero son conteos discretos, mientras que la medición con microarrays entrega un resultado continuo que proviene de la señal ﬂuorescente obtenida [19, capı́tulo 8].
1.4 Motivación y objetivos
En las últimas décadas los costos de secuenciación del ADN han disminuido considerablemente, costando $150 mil dólares americanos en el año 2001, y llegando a costar cerca de $1.000 dólares el año pasado [3] (ver Figura 1.3).
Por otra parte, el mejoramiento en los algoritmos bioinformáticos ha acortado los tiempos de ejecución del análisis RNA-Seq considerablemente.
Todos estos avances tecnológicos permiten realizar mejores análisis en un menor tiempo y con un costo notablemente más bajo.
Figura 1.3: Costos de secuenciación de un genoma de tamaño humano [3].
La disminución en el costo de secuenciación de ARN, junto con el desarrollo de mejores algoritmos y el mejoramiento en las tecnologı́as de hardware computacional podrı́an permitir un análisis rápido del ARN de los organismos vivos, en especı́ﬁco de la Vitis vinifera.
Es por estos motivos que se desarrolló este estudio como trabajo en conjunto al proyecto de clasiﬁcación de fenologı́a de la uva que se trabaja en Telefónica I+D, proyecto liderado por Francisco Altimiras, y del cual este trabajo forma parte.
1.4.1 Objetivo general
El objetivo principal de este trabajo es entregar el primer prototipo de un sistema de monitoreo aplicable a la industria agrı́cola mediante el análisis rápido de datos RNA-Seq, especı́ﬁcamente con datos provenientes de la Vitis vinifera.
1.4.2 Objetivos especı́ﬁcos
• Realizar un estudio del estado del arte en cuanto a las herramientas bioinformáticas que sirven al análisis RNA-Seq.
• Generar un ﬂujo de trabajo para la analı́tica de datos RNA-Seq, especı́ﬁcamente para el análisis rápido de secuenciaciones de genes de la planta Vitis vinifera.
• Desarrollar un prototipo de plataforma de visualización basado en la web que muestre un resumen del análisis RNA-Seq de la Vitis vinifera."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.4. Motivación y objetivos,"En [19] se describe a RNA-Seq como un conjunto de métodos experimentales y computacionales utilizados para determinar la identidad y abundancia de secuencias de ARN en muestras biológicas.
La metodologı́a involucra el aislamiento del ARN de la célula o tejido, la preparación de la librerı́a para el ARN en la muestra, la secuenciación quı́mica de la librerı́a, y el análisis bioinformático de los datos obtenidos.
La secuenciación de ARN (RNA-Seq) tiene como objetivo identiﬁcar la secuencia, estructura y abundancia de las moléculas de ARN en una muestra biológica [19, capı́tulo 1].
La palabra secuencia se reﬁere al orden especı́ﬁco que tendrán las bases A, C, G y U. Estructura del ARN hace referencia a la organización que tiene la molécula en cuanto a elementos como el promotor, uniones intrón-exón, regiones sin traducir y la cola poli(A).
Por último, con abundancia en el ARN se reﬁere a la cantidad numérica que tendrá cada secuencia (gen) en la muestra.
A diferencia de la secuenciación de ADN, la cual es útil para obtener un perﬁl genético de un organismo, la secuenciación de ARN entrega solo aquellas secuencias que se encuentran activamente expresadas en el organismo en el instante de tomar la muestra.
Para ponerlo de una forma más cotidiana, obtener la secuencia de ARN de un organismo es como hacer una foto de su material genético en ese momento.
RNA-Seq destaca por sobre los métodos anteriores como microarrays gracias al alto rendimiento que ofrecen las plataformas de secuenciación actuales, la sensibilidad que entregan las nuevas tecnologı́as de secuenciación, y la habilidad de descubrir transcritos nuevos, modelos de genes y especies de ARN pequeños no codiﬁcantes [19, capı́tulo 1].
En cuanto al análisis de expresión diferencial, el método de RNA-Seq diﬁere del realizado con microarrays en que los datos observados usando el primero son conteos discretos, mientras que la medición con microarrays entrega un resultado continuo que proviene de la señal ﬂuorescente obtenida [19, capı́tulo 8].
1.4 Motivación y objetivos
En las últimas décadas los costos de secuenciación del ADN han disminuido considerablemente, costando $150 mil dólares americanos en el año 2001, y llegando a costar cerca de $1.000 dólares el año pasado [3] (ver Figura 1.3).
Por otra parte, el mejoramiento en los algoritmos bioinformáticos ha acortado los tiempos de ejecución del análisis RNA-Seq considerablemente.
Todos estos avances tecnológicos permiten realizar mejores análisis en un menor tiempo y con un costo notablemente más bajo.
Figura 1.3: Costos de secuenciación de un genoma de tamaño humano [3].
La disminución en el costo de secuenciación de ARN, junto con el desarrollo de mejores algoritmos y el mejoramiento en las tecnologı́as de hardware computacional podrı́an permitir un análisis rápido del ARN de los organismos vivos, en especı́ﬁco de la Vitis vinifera.
Es por estos motivos que se desarrolló este estudio como trabajo en conjunto al proyecto de clasiﬁcación de fenologı́a de la uva que se trabaja en Telefónica I+D, proyecto liderado por Francisco Altimiras, y del cual este trabajo forma parte."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.4.1. Objetivo general,"El objetivo principal de este trabajo es entregar el primer prototipo de un sistema de monitoreo aplicable a la industria agrı́cola mediante el análisis rápido de datos RNA-Seq, especı́ﬁcamente con datos provenientes de la Vitis vinifera."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,2.4.2. Objetivos especı́ﬁcos,"• Realizar un estudio del estado del arte en cuanto a las herramientas bioinformáticas que sirven al análisis RNA-Seq.
• Generar un ﬂujo de trabajo para la analı́tica de datos RNA-Seq, especı́ﬁcamente para el análisis rápido de secuenciaciones de genes de la planta Vitis vinifera.
• Desarrollar un prototipo de plataforma de visualización basado en la web que muestre un resumen del análisis RNA-Seq de la Vitis vinifera."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3. Estado del Arte,
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1. Análisis de datos RNA-Seq,"Desde el año 2008 [20] se utilizan técnicas de secuenciación de alto rendimiento o de nueva generación (NGS) para la secuenciación de ARN (RNA-Seq), con el objetivo de mostrar la presencia y cantidad de secuencias de ARN en una muestra biológica en un determinado instante de tiempo.
En los últimos años una gran cantidad de procedimientos de análisis RNA-Seq han sido publicados, por lo que no existe una única forma óptima de realizar esta tarea [21].
A continuación se presentan las etapas más importantes en un procedimiento de análisis RNA-Seq, y las herramientas bioinformáticas utilizadas actualmente en cada una de ellas."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1.1. Obtención de datos,"Una vez que la muestra de ARN ha sido aislada de la célula o tejido, el siguiente paso es secuenciar.
Existen múltiples plataformas con las que se puede realizar la secuenciación del ARN, donde entre las más importantes se encuentran SOLID, Roche 454 e Illumina, siendo esta última la compañı́a lı́der en este mercado gracias a su precisión y a la secuenciación de lecturas largas (long-reads).
En la última década han aparecido otros competidores como Ion Torrent y Paciﬁc Biosciences, pero Illumina sigue siendo el método más utilizado para la secuenciación de lecturas largas de ARN [21].
Para esta memoria los datos que serán utilizados serán de Illumina y provienen de la base de datos ArrayExpress [22], la cual es proveı́da por el Instituto Europeo de Bioinformática (EMBL-EBI) y que almacena datos de experimentos de genómica funcional con el ﬁn de ponerlos a disposición de futuras investigaciones."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1.2. Control de calidad,"Luego de la obtención de datos, estas secuenciaciones deben pasar por una etapa de control de calidad, o quality check (QC).
Las raw reads corresponden a aquellas lecturas obtenidas directamente desde Illumina, las cuales son almacenadas como un archivo de texto de formato FASTQ.
Cada secuencia dentro del texto consta de cuatro lı́neas: identiﬁcador, secuencia de nucleótidos, sı́mbolo ‘+’ seguido de una descripción (actualmente sin uso), y en último lugar valores de calidad para los nucleótidos de la lı́nea 2. El control de calidad en las raw reads involucra revisar la calidad de secuencias, contenido GC, presencia de adaptadores, lecturas duplicadas o k-mers sobre representados.
Los niveles aceptables de lecturas duplicadas, k-mers o contenido GC son especı́ﬁcos del organismo que está siendo estudiado, pero ellos deben ser homogéneos para las distintas muestras dentro del experimento.
Una herramienta que ha sido muy popular en la literatura y que sigue siendo usada en la actualidad [23] [24] es FastQC [25], la cual entrega reportes html con distintas métricas que dan cuenta de la calidad de los datos obtenidos.
El reporte generado se divide en distintas secciones que informan de la calidad mediante gráﬁcos, usando el valor Phred [26] para cuantiﬁcar la calidad.
Los parámetros más comunes se presentan a continuación.
• Calidad de las bases: esta sección muestra una visión general de los rangos de las calidades de todas las bases presentes en el archivo FASTQ.
Generalmente las calidades de las bases varı́an entre 0 y 40 [19, capı́tulo 3].
• Calidad de las secuencias: este reporte permite ver si un subgrupo de secuencias tiene una baja calidad entre la totalidad de ellas.
• Contenido de bases en secuencias: muestra una curva para cada base, mostrando la proporción de ella en cada secuencia.
En general las curvas deben ser paralelas y no desbalanceadas comparando entre las bases.
• Contenido GC en las secuencias: mide el contenido GC a lo largo de cada secuencia y compara esta curva con otra que muestra la distribución teórica de aquel valor.
Si existe mucha diferencia entre ambas distribuciones puede existir contaminación en los datos.
• Contenido de K-mer: esta sección toma en consideración que cualquier fragmento pequeño de una secuencia (k-mer) no deberı́a tender a aparecer en una posición determinada.
Con esto en cuenta, FastQC muestra una lista con aquellos k-mers que tienen algún sesgo en su posición, y además muestra un gráﬁco con los seis k-mers más sesgados.
De forma que tenga una rápida ejecución, se analiza el 2 % de los datos y luego los resultados son extrapolados al resto.
• Secuencias duplicadas: calcula el grado de duplicación para cada secuencia, y crea un gráﬁco mostrando la cantidad relativa de secuencias con distinto grado de duplicación.
FastQC es una buena herramienta para realizar QC en datos obtenidos desde Illumina.
En caso de que las secuencias hayan sido obtenidas a partir otra plataforma, una herramienta que se recomienda es NGSQC [27]."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1.3. Trimming,"A continuación viene la etapa llamada trimming.
No existe consenso para determinar si efectivamente este proceso será útil para el análisis RNA-Seq.
En un estudio del año 2016, Williams et al.
[28] plantean que el uso de herramientas de trimming queda a disposición de los investigadores puesto que no existe una diferencia notoria entre el análisis con trimming o sin este.
También se sugiere que el uso de esta tarea sea exclusivamente en secuenciaciones largas (como 100 o 150 bases de largo), puesto que de esta manera, luego de hacer el trimming, las lecturas seguirán siendo largas.
Algunas herramientas disponibles en la literatura son SolexaQA [29], cutadapt [30] y Trimmomatic [31], que tiene el artı́culo más citado.
Según Del Fabbro et al.
[32] no existe una herramienta que sea la mejor para realizar trimming, y recomiendan que la elección del programa se realice evaluando el tradeoﬀ entre la pérdida de lecturas y la mejora en calidad."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1.4. Alineamiento (mapping),"La siguiente etapa del proceso de análisis RNA-Seq consiste en alinear las lecturas obtenidas a un genoma o transcriptoma de referencia con el objetivo de estimar de dónde vienen.
La diﬁcultad está en identiﬁcar correctamente las uniones de empalme (splice junctions), sobre todo porque pueden existir diferencias con el genoma de referencia o errores en las secuenciaciones obtenidas.
El alineador más popular es TopHat [33], o TopHat2 [34] en su versión más reciente.
El alineamiento de esta herramienta consiste en hacer mapping a aquellas lecturas que no abarcan una unión, identiﬁcando ası́ los exones.
Luego, las lecturas que quedaron sin alinear son divididas y alineadas a diferentes exones.
La desventaja de esta herramienta es el tiempo de ejecución, la cual es muy alta comparada con otras herramientas [35], y puede llegar hasta cinco horas [36].
La herramienta HISAT2 [37] tiene mejores tiempos de ejecución gracias a un algoritmo de búsqueda rápida, pero tiene una sensibilidad (recall) pobre, comparable con TopHat2 [35].
Distintos otros alineadores aparecen en la literatura.
Las herramientas GSNAP [38], PALMapper [39] y MapSplice [40] son útiles para identiﬁcar SNP (polimorﬁsmo de un solo nucleótido) e indels (inserción - deleción), MapSplice [40] y STAR [41] [42] servirán para detectar uniones de empalme no canónicos, las herramientas GEM [43] y STAR [41] [42] son útiles para realizar un mapping rápido.
En general, STAR [41] [42] se encuentra muy bien evaluada en la literatura ya que incluso utilizando la conﬁguración de parámetros por defecto tiene un buen rendimiento en precisión y rapidez [44], lo que la convierte en una herramienta conﬁable para la etapa de alineamiento.
La única desventaja es la cantidad de memoria RAM que requiere para ser ejecutada, que ronda los 32 GB."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1.5. Cuantificación,"Después de haber alineado las secuencias al genoma de referencia, el siguiente paso será realizar el conteo de la cantidad de secuencias alineadas para cada gen. Para realizar esto existen herramientas como HTSeq-count [45] o featureCounts [46].
HTSeq [45] es una librerı́a de Python para realizar análisis RNA-Seq, la cual incluye el script HTSeq-count, y que realiza la cuantiﬁcación a partir de un archivo GTF (Gene Transfer Format) que incluye las coordenadas de los exones del genoma.
En caso de realizar el análisis RNA-Seq en R, la librerı́a Rsubread tiene entre sus componentes el programa featureCounts [46], el cual funciona de igual forma que HTSeq-count.
Otra forma para encontrar conteos es cuantiﬁcando los niveles de expresión de los transcritos, alineando o pseudo-alineando parte de las lecturas al transcriptoma [47].
La idea es cuantiﬁcar los transcritos alternativos dentro de cada gen, para luego combinarlos y encontrar el total de conteos para cada gen. Las herramientas más citadas que utilizan esta forma de cuantiﬁcar son Cuﬄinks [48], eXpress [49], Flux Capacitor [50], RSEM [51], kallisto [52] y Salmon [53].
Teng [47] encuentra que todas las herramientas mencionadas, a excepción de Flux Capacitor, tienen un rendimiento muy similar entre ellas.
Pseudo-alineamiento + cuantiﬁcación
Salmon y kallisto son las herramientas más recientes en el alineamiento a un transcriptoma.
En el caso de kallisto el proceso no es un alineamiento propiamente tal, sino que realiza la cuantiﬁcación mediante un proceso llamado pseudo-alineamiento en el cual forma un grafo de Bruijn a partir del transcriptoma de referencia, y luego alinea los k-mers de la lectura en los nodos del grafo [52].
Por otra parte, Salmon realiza un proceso de tres partes: un mapping ligero al transcriptoma de referencia; una fase online que estima los niveles de expresión iniciales; y una fase oﬄine que aﬁna aquellas estimaciones.
Juntas, las fases online y oﬄine optimizan los conteos de abundancia estimados, cuyos resultados son más precisos que kallisto [53].
Finalmente, hoy kallisto y Salmon son consideradas las mejores herramientas a utilizar para una cuantiﬁcación veloz, puesto que no necesita de un proceso de alineamiento previo y la ejecución de ellos sobre una muestra de secuenciación RNA es muy rápida comparada con las otras herramientas [52] [53]."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,3.1.6. Análisis de expresión diferencial,"Una vez realizada la cuantiﬁcación (o pseudo-alineamiento) se obtiene una tabla de conteos que tiene las abundancias para cada gen de cada muestra.
Esta tabla es utilizada como input para la etapa de análisis de expresión diferencial, donde existen varias herramientas para realizar aquella tarea.
El método más popular es edgeR [54], una herramienta ampliamente utilizada a pesar de ser la más antigua de todas, y que además del análisis de expresión diferencial logra normalizar considerando posibles sesgos que puedan existir en los datos.
Otra herramienta es DESeq (o la más actual DESeq2) [55] [56], la cual utiliza su propia forma de normalización y que al igual que edgeR parte de la base de que los datos se distribuyen de una forma binomial negativa.
Ambas herramientas, edgeR y DESeq2, son útiles para experimentos en donde las réplicas biológicas utilizadas no superan las cinco muestras por grupo.
Otras herramientas como NOISeq [57] o SAMseq [58] utilizan métodos no paramétricos; es decir, no parten de una suposición sobre el comportamiento de los datos y la distribución estadı́stica de ellos se inﬁere ahı́ mismo."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4. Propuesta de Solución,"Gracias a la gran abundancia de herramientas bioinformáticas hoy existen múltiples maneras de realizar un análisis RNA-Seq.
Para este trabajo en especı́ﬁco es necesario un ﬂujo de trabajo que considere un estudio rápido de los datos biológicos, para luego realizar un análisis de expresión diferencial y obtener un output que pueda servir en la visualización de su resultado.
Con lo recogido en el párrafo anterior, también se busca desarrollar una plataforma de visualización que preste información relevante al sector agrı́cola y que ayude en la toma de decisiones sobre el sujeto en estudio.
Para este trabajo, aquel sujeto será la Vitis vinifera, con un foco en las etapas de desarrollo de la planta.
A partir de la información recabada en el estado del arte, en este capı́tulo se presentará el ﬂujo de trabajo RNA-Seq adecuado para el cumplimiento de los objetivos, además de una propuesta de prototipo web para la visualización de datos provenientes del análisis de expresión diferencial de genes."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4.1. Flujo de Trabajo Bioinformático RNA-Seq,
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4.1.1. Datos a utilizar,"Como fue mencionado en la sección 2.1.1, los datos que se utilizarán para este trabajo serán secuenciaciones de la planta Vitis vinifera en distintos estados fenológicos, y las cuales serán obtenidas a partir de la base de datos ArrayExpress [22], del Instituto Europeo de Bioinformática.
En detalle, los datasets a utilizar son los indicados en la tabla 3.1.
 | ID | Código ArrayExpress | Número de muestras | Etapas de crecimiento | Single-End o Paired-End
 | --- | --- | --- | --- | ---
 | 1 | E-GEOD-56844 | 23 | 4 | Single-end
 | 2 | E-GEOD-71146 | 24 | 4 | Single-end
 | 3 | E-GEOD-58061 | 6 | 3 | Paired-end
 | 4 | E-MTAB-4220 | 9 | 3 | Paired-end
 | 5 | E-GEOD-63512 | 6 | 3 | Single-end

Tabla 3.1: Datasets de ArrayExpress a analizar."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4.1.2. Trimming y control de calidad,"Se utilizará la herramienta FastQC [25] para realizar la etapa de control de calidad de las muestras.
Su elección se debe a la facilidad de uso, su compatibilidad con secuenciaciones de Illumina y la simplicidad que otorga al momento de interpretar los ı́ndices de calidad.
Con el ﬁn de eliminar las bases de mala calidad, aquellas lecturas que tienen largo 100 o más y cuyas bases tienen un puntaje de Phred 20 o menor [28] pasarán por el proceso de trimming (poda en español), utilizando la herramienta Trimmomatic [31]."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4.1.3. Alineamiento y cuantificación,"Para realizar la cuantiﬁcación de los genes es necesario que, en un primer lugar, las secuenciaciones obtenidas de los dataset se alineen al transcriptoma de referencia.
El método clásico de alinear las secuenciaciones al transcriptoma es mediante herramientas como TopHat2 o STAR.
Éstas son herramientas que realizan el alineamiento de forma exhaustiva, pero requieren mucho tiempo de ejecución o muchos recursos.
La ventaja de TopHat2 es la precisión por sobre las otras herramientas disponibles [34], además de requerir pocos recursos.
STAR, por otra parte, tiene la ventaja de ser un alineador muy rápido, estando listo en menos de una hora comparado con los otros programas.
Ambas herramientas tienen desventajas que las convierten en opciones descartables para este trabajo: en cuanto a TopHat2, si bien los recursos computacionales que requiere se encuentran bajo los 8 GB de memoria RAM, es la ejecución del programa la que toma entre 8 y 18 horas en alinear; por otra parte, STAR es muy rápido para realizar el alineamiento, pero no es tan preciso como TopHat2 u otras herramientas, y en cuanto al poder computacional se requiere un mı́nimo de 28 GB de RAM.
Dejando atrás el método clásico de alineamiento, como fue mencionado en el capı́tulo 2 en los últimos años han aparecido herramientas de cuantiﬁcación donde no se realizan alineamientos exhaustivos al transcriptoma, y que aún ası́ logran buenos resultados en cuanto al conteo de genes.
Las herramientas más relevantes en esta técnica de cuantiﬁcación son kallisto y Salmon.
Salmon [53] realiza un proceso de alineamiento ligero, seguido de un algoritmo online donde estima los niveles de expresión, y termina con una fase oﬄine que ajusta las estimaciones anteriores.
La herramienta es más precisa y mucho más rápida que otros alineadores y cuantiﬁcadores, con excepción de kallisto que realiza una tarea similar a una rapidez comparable, alineando los k-mers de las secuencias al grafo de Bruijn del transcriptoma [52].
Aún no hay consenso en cuál es el método óptimo para realizar el alineamiento; sin embargo, al ser uno de los objetivos de este trabajo realizar el proceso de una forma rápida es que se opta por utilizar las herramientas kallisto y Salmon como métodos de cuantiﬁcación.
Con el ﬁn de cumplir con el objetivo de generar un ﬂujo de trabajo rápido, en esta memoria se comparará la ejecución del trabajo utilizando ambas herramientas y ası́ encontrar aquella que es la más veloz."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4.1.4. Análisis de expresión diferencial,"Previo a esta etapa, con la herramienta tximport se uniﬁcan los resultados de la cuantiﬁcación para generar una matriz de conteo, la cual servirá de input para el análisis de expresión diferencial.
A continuación se realiza el análisis de expresión diferencial, para el cual existen varias herramientas (sección 2.1.6) de donde se ha escogido edgeR.
Este programa es parte de Bioconductor1 y, como el nombre lo sugiere, está basado en R.
Para muestras biológicas provenientes de distintas fuentes o individuos, la variabilidad entre ellas usualmente se encuentra modelada como una distribución binomial negativa [19].
Lo anterior reduce la búsqueda de herramientas a dos: edgeR y DESeq2.
Se preﬁere el uso de la primera puesto que tiene una mayor antigüedad y posee una buena documentación, además de apoyo en linea por parte de la comunidad.
A partir del análisis de expresión diferencial se obtendrán dos grupos de genes: upregulated (regulación positiva) y downregulated (regulación negativa).
Los genes upregulated serán aquellos que, debido al estı́mulo externo (enfermedad, etapa fenológica, etc), incrementan en cantidad.
En cambio, los genes que disminuyen en cantidad de acuerdo a aquel estı́mulo son los llamados downregulated.
Estas listas de genes pasarán a la siguiente etapa de anotación funcional, la cual hace referencia a la anotación y análisis estadı́stico de aquellas listas mediante métodos estadı́sticos para identiﬁcar a qué procesos biológicos, funciones moleculares y componentes celulares aquellos genes se encuentran relacionados.
En este trabajo aquella tarea es realizada con PANTHER DB, herramienta en linea que recibe como input la lista de genes, y como output entrega la clasiﬁcación correspondiente para cada uno.
En la ﬁgura 3.1 se presenta el ﬂujo de trabajo a utilizar para el análisis RNA-Seq de los distintos datos biológicos de la Vitis vinifera.
1Proyecto bioinformático de código abierto que provee herramientas para el análisis y comprensión de datos genómicos.
Sitio web: http://bioconductor.org/
Figura 3.1: Flujo de información para el procesamiento de datos RNA-Seq."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,4.2. Visualización de datos,"Para hacer sentido de los resultados provenientes del análisis previo, se hace importante mostrarlos de una forma resumida y fácil de entender de forma que puedan ser comprendidos por el agricultor o el usuario ﬁnal.
Es por esta razón que se propone una plataforma web de visualización de los datos biológicos provenientes de diferentes plantaciones, donde el usuario pueda observar el desarrollo de estas a través del año, y además ver cuáles son los genes diferencialmente expresados y a qué procesos están asociados.
Para ello, en este trabajo se desarrolla un primer prototipo de esta plataforma, la cual estará construida utilizando Shiny, una librerı́a de RStudio para construir sitios web interactivos basados en R. Además, se utilizará la librerı́a de código abierto Leaﬂet para mostrar mediante mapas la variación de las etapas de desarrollo en las plantaciones durante un año.
En la ﬁgura 3.2 se muestra un esquema del contenido general del sitio.
En la sección 1 se muestran las zonas de cultivo en un mapa, donde cada zona debe estar bien delimitada y coloreada según la etapa de desarrollo correspondiente.
La sección 2 servirá para presentar mediante una tabla los genes diferencialmente expresados y sus respectivas funcionalidades.
La sección estará separada en dos pestañas que distingan entre genes upregulated y downregulated.
Finalmente, en la sección 3 se encuentran los ﬁltros que permitirán actualizar la información que muestran las secciones 1 y 2. En primer lugar deberá existir un ﬁltro de tiempo que actualiza el desarrollo que han tenido las zonas de cultivo durante un año, y además deben haber ﬁltros para actualizar los genes que se muestran en la sección 2 según etapa de desarrollo.
Figura 3.2: Esquema del contenido de la plataforma Shiny.
Sección 1: Consiste en un mapa que muestra las zonas de cultivo de un agricultor, donde además deben estar coloreadas de acuerdo a la etapa de desarrollo en la que se encuentra.
Sección 2: Presenta en una tabla los genes diferencialmente expresados en cierta etapa de desarrollo.
Sección 3: Filtros que actualizan la información mostrada en el mapa y en las tablas."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5. Resultados,
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.1. Análisis RNA-Seq,"En una primera instancia se realizó un análisis RNA-Seq siguiendo las etapas mostradas en la ﬁgura 3.1.
Para gran parte del análisis RNA-Seq se utilizó una máquina virtual alojada en los servidores de la universidad, la cual corre sistema operativo Ubuntu 18.04, con 31 GB de memoria RAM, 500 GB de disco duro y un procesador Intel Xeon E3-12xx de cuatro núcleos.
Además fue necesario instalar R para poder trabajar con el framework de bioinformática Bioconductor (http://bioconductor.org).
En primer lugar se debe realizar la obtención de los datos biológicos con los que se realizará el trabajo, según fue mencionado en la sección 3.1.1.
Aquello se realiza desde el sitio web de ArrayExpress (www.ebi.ac.uk/arrayexpress/), en donde se realizó una búsqueda de las secuenciaciones disponibles de Vitis vinifera, y luego se hizo un ﬁltro para mantener solo aquellos datasets que describen la etapa fenológica de esos datos.
De lo anterior se obtuvieron 89, 13 GB de datos repartidos en cinco datasets, con 68 archivos de muestras biológicas (archivos de extensión.fastq) en total; en la tabla 4.1 se muestran las etapas fenológicas presentes junto con la cantidad de muestras que se obtuvieron para cada una, además de una breve descripción de la etapa.
La siguiente tarea será entonces el control de calidad de las secuencias.
 | Etapa fenológica | Cantidad de muestras | Descripción
 | --- | --- | ---
 | EL-3 | 6 | Brote lanudo
 | EL-5 | 6 | Roseta con puntas de hojas visible
 | EL-15 | 8 | 8 hojas separadas,
 | brote se alarga rápidamente
 | EL-17 | 5 | 12 hojas separadas
 | EL-27 | 2 | Cuajado de fruta
 | EL-31 | 6 | Bayas del tamaño de una arveja
 | EL-35 | 13 | Bayas toman color y se agrandan
 | EL-36 | 4 | Bayas con valores Brix intermedios
 | EL-38 | 15 | Bayas maduras listas para cosechar
 | EL-41 | 3 | Bayas post-cosecha

Tabla 4.1: Etapas fenológicas a utilizar para el análisis RNA-Seq, junto con la cantidad de muestras disponibles y una descripción de la etapa.
Las etapas se encuentran nombradas por su numeración en escala Eichhorn-Lorenz."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.1.1. Control de calidad,"Se ejecutó el programa FastQC para realizar el control de calidad, donde se analizaron los 68 archivos.fastq, obteniendo reportes HTML para cada uno de ellos.
Cada reporte comienza mostrando los metadatos del archivo, con información como cantidad total de secuencias, largo de una secuencia y cantidad de secuencias marcadas con mala calidad.
Además, tal como se explicó en la sección 2.1.2, en el resto del reporte se presentan distintas gráﬁcas donde cada una entrega un signiﬁcado diferente en relación a la calidad de las secuencias.
La calidad de una base indica la conﬁanza con la que se determinó aquella al momento de la secuenciación, y se expresa en escala Phred.
Se determina calculando log10 P donde P es la probabilidad de que la base sea errónea, y luego ese valor se multiplica por −10.
Por ejemplo, si existe una probabilidad de 1 en 1000 de que la base sea errónea, entonces el valor de calidad de Phred será q = −10∗ log10(0,001) = 30.
Valores entre 40 y 28 son considerados buena calidad, entre 28 y 20 son de calidad media y cuando cae entre 20 y 0 será de mala calidad.
El dataset 1, según la numeración de la tabla 3.1, se evalúa de forma positiva luego del control de calidad, puesto que todas sus bases entregan un puntaje de Phred sobre 30.
Además, del cuarto gráﬁco se puede decir con seguridad que la calidad promedio varı́a entre 30 y 39 para la gran mayorı́a de las bases.
Es por estas caracterı́sticas que el dataset 1 no tendrá la necesidad de someterse al proceso de trimming.
Lo mismo ocurre con el dataset 5, donde todos los indicadores de calidad son resultados positivos, por lo que tampoco caliﬁca para trimming.
El dataset 4 varı́a un poco con respecto a los dos anteriores ya que según se puede ver en los reportes, el último cuartil de los box plots baja más allá de Phred 20 en las últimas bases, siendo de peor calidad que los dataset anteriores.
Además se puede notar que por ser lecturas paired-end, las lecturas de dirección reversa tienen peor calidad que las lecturas de dirección directa.
Los tres dataset anteriores tienen en común que el largo de las secuenciaciones no supera las 51 bases.
Lo anterior no los convierte en un buen sujeto para el proceso de trimming, ya que al ser lecturas tan cortas se corre el riesgo de podar partes que son de buena calidad.
De los datasets restantes, el 3 presenta mala calidad en las últimas cuatro bases de las lecturas, pero siempre el promedio se mantiene en buena calidad sobre el Phred 30.
A pesar de ello, como era de esperar las lecturas de dirección reversa empeoran en calidad en las últimas bases.
Finalmente el dataset 2 será el peor evaluado de todos.
La totalidad de las bases tienen al menos su último cuartil dentro de la zona con peor calidad (bajo Phred 20), y también la mayorı́a de los box plot estarán bajo calidad 20, incluso llegando al peor puntaje posible.
Además, como se puede ver en la ﬁgura 4.1a, una gran parte de las secuenciaciones tiene un promedio de calidad menor a Phred 20, lo que deﬁnitivamente es muy bajo como para seguir trabajando con estos datos.
Por las razones anteriores, y porque ambos datasets cuentan con un largo de 100 bp, se decide realizar trimming a estos dos grupos de datos.
(a) Antes del trimming cerca de 750.000 secuencias tienen puntaje Phred 0.
(b) Luego de hacer trimming los reads de mala calidad disminuyeron cerca de un 75 %.
Figura 4.1: Distribución de la calidad promedio de las lecturas del dataset 2.
Trimming
Utilizando la herramienta Trimmomatic mediante lı́nea de comandos, los datasets 2
y
3 son sometidos al proceso de trimming con parámetros LEADING:20 y MINLEN:50, donde la ejecución fue cercano a las cuatro horas para cada dataset.
Considerando que un dataset tiene 24 muestras y el otro seis, la similitud en los tiempos de ejecución puede parecer extraña, pero la diferencia está en que la máquina toma cerca de cuarenta minutos en podar las lecturas paired-end (dataset 3), y para el dataset 2, que tiene cuatro veces más muestras, demora entre 5 y 20 minutos por cada una de ellas.
Esto tiene más sentido si se observa la relación que tiene, para el dataset 2, el tamaño del archivo a podar con el tiempo que demora en ejecutarse Trimmomatic (ver Figura 4.2), donde claramente existe una relación directamente proporcional con el tamaño en GB de la muestra.
Figura 4.2: Relación entre el tamaño de la muestra biológica y el tiempo que toma Trimmomatic en podar aquel archivo.
Finalmente, como se puede ver en la ﬁgura 4.3b, se logró una mejora considerable en la calidad de las lecturas para el dataset 2 (E-GEOD-71146) en comparación con lo presentado en la ﬁgura 4.3a, eliminando una buena cantidad de secuencias con mala calidad (ver ﬁgura 4.1).
En relación al dataset 3, su reporte de calidad luego de hacer trimming no muestra cambios sustanciales en la calidad de éste."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.1.2. Cuantificación libre de alineamiento,"Para la cuantiﬁcación se consideró el uso de dos herramientas de última generación: Salmon y kallisto.
Antes de realizar la tarea principal, ambos programas necesitan un archivo de ‘ı́ndice’, el que debe ser creado a partir del transcriptoma de referencia, encontrado en el sitio web http://plants.ensembl.org/.
Salmon es presentado como un programa muy rápido y a la vez altamente preciso para producir cuantiﬁcaciones de datos RNA-seq, esto gracias a la técnica de quasi-mapping que utiliza en el proceso.
El programa recibirá como input el ı́ndice del transcriptoma y las secuenciaciones producto del trimming.
Como salida entrega una carpeta por cada muestra
(a) Antes del trimming al menos un 30 % tiene la peor calidad posible.
(b) Después del trimming se logró eliminar una gran cantidad de lecturas de mala calidad.
Figura 4.3: Distribución de los puntajes Phred para el dataset 2.
analizada, donde cada una de ellas contiene un archivo en formato TSV listando un gen por ﬁla, indicando su largo (Length), largo efectivo (EﬀectiveLength), su abundancia relativa en transcritos por millón (TPM) y el número estimado de lecturas originados a partir del transcriptoma (NumReads).
Además la opción -p (o --threads) permite elegir la cantidad de hilos con los que ejecutar la cuantiﬁcación.
Esta herramienta no se diferencia mucho de kallisto en cuanto a su ejecución.
Mediante el proceso de pseudo-alineamiento, kallisto permite cuantiﬁcar secuenciaciones directamente sin la necesidad de realizar el proceso de alineamiento, razón que lo convierte en una alternativa comparable a Salmon.
Al igual que esta última, kallisto recibe como input un archivo ‘ı́ndice’ del transcriptoma y las secuenciaciones a cuantiﬁcar, y entrega como output información sobre conteos estimados, TPM y largo efectivo.
Se ejecutaron ambas herramientas mediante la lı́nea de comandos de Ubuntu, indicando que utilizara cuatro hilos (threads).
Al comparar los programas, si bien existen algunas diferencias, ambos cuantiﬁcadores se ejecutan con buen rendimiento (ver tabla 4.2).
Para los datasets 1, 4
y
5 el tiempo de ejecución nunca supera los 20 minutos, y si se compara con las herramientas de alineamiento completo [44] el rendimiento es muy bueno.
En cuanto a los datasets 2 y 3, la cuantiﬁcación toma bastante más tiempo, llegando incluso a demorar 1 hora 24 minutos (dataset 3 con Salmon).
 | # Dataset cuantiﬁcado | Programa Duración | Diferencia
 | --- | --- | ---
 | 1 | E-GEOD-56844 | Salmon | 0:19:44 | 0:02:18
 | --- | --- | --- | --- | ---
 |  |  | kallisto | 0:17:26 | 
 | 2 | E-GEOD-71146 | Salmon | 0:46:46 | 0:15:00
 |  |  | kallisto | 0:31:46 | 
 | 3 | E-GEOD-58061 | Salmon | 1:23:29 | 0:37:03
 |  |  | kallisto | 0:46:26 | 
 | 4 | E-MTAB-4220 | Salmon | 0:13:47 | 0:01:25
 |  |  | kallisto | 0:12:22 | 
 | 5 | E-GEOD-63512 | Salmon | 0:05:23 | 0:01:31
 |  |  | kallisto | 0:03:52 | 

Tabla 4.2: Tiempo de ejecución utilizado en la cuantiﬁcación, por dataset y programa utilizado.
Una de las razones por las que los programas demoran más para los datasets 2
y
3 puede explicarse en el largo de sus secuencias; si bien se realizó trimming sobre estos datos, el largo que inicialmente era de 100 bp1 no disminuyó de manera considerable, por lo que ambos datasets tienen lecturas que llegan a ser el doble de los otros."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.1.3. Unificación de resultados,"El análisis de expresión diferencial requiere como input una matriz de los conteos producidos en el paso anterior (count matrix), y es por esta razón que se hace necesario uniﬁcar los resultados de la cuantiﬁcación en matrices para su posterior uso en edgeR.
Para realizar esto se utiliza la librerı́a de R tximport, que es parte de Bioconductor.
Esta librerı́a contiene utilidades para importar y compendiar las abundancias provenientes de herramientas como Salmon o kallisto, para ası́ entregar un resultado que pueda ser utilizado por herramientas de análisis de Bioconductor, como DESeq2 o edgeR.
Para llevar a cabo esta tarea se debió realizar un script en R donde se indica el programa 1bp: base pair.
Cuando la muestra es single-end se reﬁere a los nucleótidos con la abreviación nt.
de cuantiﬁcación utilizado (si Salmon o kallisto), la ruta de los conteos de cada muestra, y la anotación de genes y transcritos, el que se puede obtener del sitio Ensembl.
Una vez que se ejecuta el código se obtiene una matriz compuesta por todos los datasets cuantiﬁcados, donde cada columna corresponde a una muestra.
Además, se escribió otro script para agrupar aquellas columnas por sus etapas de crecimiento.
El resultado de esta tarea es una matriz de conteos, con las columnas ordenadas por etapa fenológica, para que las muestras puedan ser fácilmente agrupadas en la siguiente etapa."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.1.4. Análisis de expresión génica diferencial,"Una de las partes más importantes de este pipeline RNA-Seq se realiza en el análisis de expresión génica, donde el objetivo es encontrar los genes que se encuentran más o menos expresados entre dos condiciones biológicas distintas, observando ası́ los niveles de ARN como la cantidad de lecturas que se superponen en la región del gen en el transcriptoma.
En este trabajo aquellas condiciones biológicas serán dos etapas fenológicas consecutivas, lo que signiﬁca que se realizarán nueve análisis distintos.
Para realizar esto se escribe un script en R que hace uso de la librerı́a edgeR de Bioconductor.
En primer lugar, el script debe obtener los datos a comparar, los cuales provienen de la matriz de conteos generada en el paso anterior.
Además es necesario proveer la etapa fenológica correspondiente a cada columna de la matriz2, de esta forma edgeR puede agrupar las muestras en base a su etapa de crecimiento respectiva.
Luego en el script se eliminan aquellas ﬁlas donde la gran mayorı́a de los conteos son cero, quitando ası́ más de seis mil genes y dejando una matriz de 23.657 ﬁlas.
En los datos RNA-Seq pueden existir muestras en donde el tamaño de librerı́a (library size) sea mayor al resto, lo que lleva a tener conteos muy mayores en comparación a otras muestras, y que luego se puede traducir a identiﬁcar genes erróneamente como downregulated o viceversa.
Para prevenir esto se realiza la tarea de normalización mediante la función calcNormFactors de edgeR.
Una vez hecho esto ya es posible obtener los genes diferencialmente expresados por cada etapa fenológica; para esto se realiza un proceso iterativo 2Cada columna de la matriz es una muestra biológica, y cada ﬁla es un gen de la planta donde se compara una etapa con la consecutiva, luego esta última con la siguiente, y ası́ hasta llegar a la etapa EL-41.
Se realizó la selección de genes diferencialmente expresados con la función decideTestsDEG y utilizando un logFC 3 de 1,5 como ﬁltro.
En la tabla 4.3 se presenta la cantidad de genes obtenidos por cada comparación.
Las listas de genes son muy grandes y pueden ser observadas en el sitio web presentado en la sección 4.2 de este trabajo.
 |  | Etapas fenológicas | Upregulated Downregulated
 | --- | --- | ---
 | 1 | EL-3 y EL-5 | 10 | 7
 | --- | --- | --- | ---
 | 2 | EL-5 y EL-15 | 454 | 261
 | 3 | EL-15 y EL-17 | 19 | 15
 | 4 | EL-17 y EL-27 | 942 | 997
 | 5 | EL-27 y EL-31 | 1522 | 1810
 | 6 | EL-31 y EL-35 | 282 | 1661
 | 7 | EL-35 y EL-36 | 308 | 1834
 | 8 | EL-36 y EL-38 | 263 | 193
 | 9 | EL-38 y EL-41 | 1757 | 3417

Tabla 4.3: Cantidad de genes upregulated y downregulated obtenidos a partir del análisis de expresión génica.
Se encuentran clasiﬁcados por comparación entre etapa de desarrollo fenológico."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.1.5. Anotación funcional de genes,"Producto del análisis de expresión génica se obtienen los genes diferencialmente expresados para cada transición entre etapas.
El objetivo en esta etapa es encontrar las funcionalidades a las que están asociados aquellos genes, con el ﬁn de proveer esta información en la plataforma web que verá el agricultor.
Para ello se utiliza la herramienta web PANTHER, que es parte del proyecto Gene Ontology [59], y que recibe como entrada 18 archivos de texto con las listas de genes upregulated y downregulated, y entrega como salida una tabla por cada lista, que describe cada gen de la lista con su funcionalidad en caso de tenerla registrada.
3logFC: log2 fold-change.
Los datos presentados en la tabla son los siguientes:
• Species: Especie a la cual pertenece el gen.
• Gene ID: ID del gen.
• Complete Gene ID: ID del gen en su forma completa.
• Gene Name and Gene Symbol: Nombre del gen.
• PANTHER family / subname: Familia y nombre que recibe el gen dentro de la clasiﬁcación PANTHER.
• PANTHER protein class: Clase de proteı́na proveniente del gen.
• GO-slim Molecular Function: Subset de GO4 de funciones moleculares del producto génico.
• GO-slim Biological Process: Subset de GO de procesos biológicos en los cuales participa el gen.
• GO-slim Cellular Component: Subset de GO de componentes celulares de los cuales el gen forma parte o es un subcomponente.
• GO database Molecular Function (complete): Funciones moleculares del producto génico.
• GO database Biological Process (complete): Procesos biológicos en los cuales participa el gen.
• GO database Cellular Component (complete): Componentes celulares de los cuales el gen forma parte o es un subcomponente.
4Subset de GO, o GO-slim, son formas de representar la ontologı́a de una forma amplia y abreviada."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,5.2. Prototipo de plataforma de visualización,"La última etapa de este trabajo es la creación de un prototipo para una plataforma de visualización del estado de los cultivos.
Más especı́ﬁcamente, a través de un sitio web se busca presentar el desarrollo fenológico de los cultivos, con información proveniente del análisis RNA-Seq de sus plantas.
En una instancia previa, mediante la ﬁgura 3.2 se mostró una organización general del sitio web; ahora, para tener una idea más precisa de las funcionalidades requeridas, se esbozó un wireframe (ver Figura 4.4) que materializa los elementos que deberá poseer el prototipo hecho en Shiny.
Figura 4.4: Wireframe del prototipo desarrollado.
Shiny es un paquete de R que provee un framework para el desarrollo de aplicaciones web, pudiendo generar plataformas interactivas para el análisis de datos sin tener que recurrir directamente a lenguajes como HTML, JavaScript o CSS.
Una aplicación en Shiny está compuesta de dos scripts R para que pueda funcionar, los cuales se comunican entre ellos.
Estos componentes son: un script de interfaz de usuario, el que tendrá información sobre el aspecto del sitio y los elementos que lo conforman; y un script de servidor, el que tendrá instrucciones para los inputs de usuario, outputs y procesamiento de datos mediante código en lenguaje R.
A continuación se describe la composición del prototipo realizado, siguiendo la organización del layout que se muestra en la ﬁgura 3.2.
Sección 1:
Toda la sección 1 fue desarrollada gracias a la librerı́a Leaﬂet para R. Aquı́ se presenta un mapa que muestra las zonas de cultivo analizadas, las cuales se encuentran debidamente delimitadas, y cada una de ellas lleva un color de relleno según la etapa fenológica que corresponda.
Aquella etapa fenológica irá cambiando a medida que el ﬁltro “mes del año” sea deslizado.
Además, cada zona lleva una etiqueta asociada a ella, la que aparece solo cuando el cursor está moviéndose por sobre alguna de las zonas.
La etiqueta muestra datos relacionados con la zona correspondiente, informando sobre: especie, etapa fenológica actual, cantidad de genes diferencialmente expresados, y cantidad de genes upregulated y downregulated.
Sección 2:
Esta sección se encontrará compuesta por dos pestañas llamadas “Upregulated” y “Downregulated”.
Cada pestaña mostrará una tabla con los genes diferencialmente expresados (upregulated o downregulated) en una determinada etapa fenológica.
Esta etapa debe ser proveı́da por el usuario en la sección de ﬁltros (sección 3).
Además, las columnas de la tabla pueden mostrarse u ocultarse mediante el ﬁltro de checkbox que pueden encontrarse en la sección 3.
Sección 3:
La sección 3 corresponde a la barra lateral izquierda de la interfaz, y que entrega distintos ﬁltros interactivos que, al modiﬁcarlos, cambian la información presentada al usuario.
En primer lugar destaca la presencia de una barra slider que entrega doce opciones (una por mes del año), donde por cada ı́tem se actualiza la información del mapa, cambiando también el color de las zonas según corresponda.
Otro ﬁltro disponible es un menú desplegable (drop down menu), del cual cada opción es una etapa fenológica del organismo estudiado, y al seleccionar una opción se actualizan ambas tablas de la sección 2. El último ﬁltro será de checkbox, los cuales habilitan/deshabilitan las columnas mostradas en las tablas de la sección 2.
La ﬁgura 4.5 muestra el resultado ﬁnal del prototipo hecho en Shiny; en la imagen el slider ha sido ﬁjado en “Abril 2018”, y la tabla muestra los genes diferencialmente expresados upregulated correspondientes a la etapa EL-17.
También el prototipo puede ser visitado en https://vitis-deg.shinyapps.io/shiny-app/.
Figura 4.5: Captura de pantalla del prototipo."
SISTEMA DE PROCESAMIENTO Y VISUALIZACIÓN DE DATOS GENÓMICOS PARA LA PRODUCCIÓN DE UVA,6. Conclusiones,"En este documento se planteó un marco de trabajo RNA-Seq para el análisis de datos biológicos provenientes de plantas; para ello se utilizaron secuenciaciones de ARN de Vitis vinifera.
En primer lugar se hace uso de la herramienta de control de calidad FastQC, y luego algunos de los datasets analizados pasan por una fase de trimming para eliminar lecturas de mala calidad, gracias al programa Trimmomatic.
La ejecución de FastQC se realiza sin mayores diﬁcultades, obteniendo los reportes esperados y que entregan la información necesaria para realizar trimming.
La etapa de poda se realizó solo a dos datasets, donde para cada uno la ejecución tomó poco menos de cuatro horas.
Llama la atención la demora que tiene esta etapa del análisis ya que la duración es bastante más larga que otras de las tareas, lo que convierte al trimming en el cuello de botella del análisis RNA-Seq.
La poda de los datasets 2
y
3 demoran un tiempo similar, pero la gran diferencia es que el último, si bien tiene menos muestras que el otro dataset, sus datos son paired-end con cada archivo pesando hasta cinco veces más que uno proveniente del dataset 2.
En la ﬁgura 4.2 se observa una relación directamente proporcional entre el peso de la muestra y el tiempo que demora el programa, lo que explica la demora con la que se ejecuta Trimmomatic en el dataset 3, siendo de más de 38 minutos en promedio para cada muestra.
Con esta evidencia y considerando lo que dice la literatura, se pone en duda la etapa de trimming puesto que, a menos que las lecturas sean todas de un largo mayor a 100 bp, la ejecución de Trimmomatic se convertirá en un obstáculo para el análisis gracias a su gran tiempo de ejecución, además de no asegurar resultados de mejor calidad.
En la etapa de cuantiﬁcación se utilizaron dos herramientas, kallisto y Salmon, y como muestra la tabla 4.2 el rendimiento de ambos programas es muy similar, a pesar de las diferencias para los datasets 2 y 3.
Además del tiempo de ejecución, la literatura indica que Salmon es más preciso al detectar expresión génica en comparación con kallisto y otras herramientas [53].
Si bien kallisto tiene mejores tiempos de ejecución que Salmon por unos minutos, esa diferencia no hace mucha distinción si se compara con los otros métodos que consideran un alineamiento completo.
Posterior a la fase de cuantiﬁcación, con la herramienta tximport se uniﬁcaron los resultados de los conteos en una matriz.
Se obtuvieron matrices con las cuantiﬁcaciones de kallisto y Salmon, y a partir de ellas se llevó a cabo el análisis de expresión génica con edgeR.
Una vez obtenidas las listas de genes diferencialmente expresados, se realizó una validación de tal forma de veriﬁcar la cantidad de genes en común entre ambos métodos de cuantiﬁcación.
De esto se encontró que la cantidad de genes en común para kallisto y Salmon es siempre mayor que el número de genes presentes solo en uno de ellos.
Por conclusión, la herramienta de cuantiﬁcación preferida será Salmon gracias a su rapidez y a la precisión que otorga en comparación con otras herramientas existentes.
No obstante, se recomienda considerar kallisto como alternativa puesto que, junto con Salmon, ambos son programas en constante mejora y desarrollo.
Finalmente se desarrolló el prototipo de una plataforma web que toma los datos provenientes del análisis RNA-Seq y los presenta en una interfaz fabricada en Shiny, donde además el bioinformático o el agricultor puede observar un mapa que tiene sus zonas de plantación delimitadas y coloreadas según el estado fenológico, gracias a la librerı́a Leaﬂet para R. El prototipo puede ser visitado en https://vitis-deg.shinyapps.io/shiny-app/ y el código fuente se encuentra disponible en el repositorio de GitHub https://github.com/ claudiogalaz/vitis-deg/.
Por último, como trabajo a futuro se propone considerar la omisión de la etapa de trimming para próximos análisis RNA-Seq, además del desarrollo de una plataforma web que, junto con los datos del análisis RNA-Seq, entregue información resumida sobre los procesos biológicos para cada etapa fenológica.
Además, serı́a útil considerar un análisis para una lista predeterminada de genes, de forma de evaluar cómo cambia la expresión diferencial de éstos en las distintas etapas fenológicas.
También se propone la secuenciación y utilización de datos de Vitis vinifera provenientes de los campos de agricultores chilenos, además de la creación de un modelo clasiﬁcador que, a partir de los datos de expresión génica de una planta, indique en qué etapa de crecimiento se encuentra."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,1. INTRODUCCIÓN,"El presente proyecto de título se realizará respecto a la línea de vacío que consta de las máquinas Effisa Omega FF 301 y Effisa Omega FF 302 de la empresa ITI Chile, ubicada en el Parque Industrial de Coronel.
La empresa en comento se dedica a la remanufactura de madera, pintado de madera y reclasificado de la misma.
ITI Chile, de propiedad familiar, es parte del Grupo ITI de empresas ubicadas en Australia, Nueva Zelanda e Indonesia.
Con más de 30 años de experiencia en la industria de productos de madera, ITI ha desarrollado y distribuido productos de madera de alta calidad y aptos para su propósito.
En el año 2016, ITI Chile invirtió fuertemente en la planta, reemplazando de esta forma muchos procesos de producción manuales obsoletos por equipos automatizados de última generación, como tecnología de escaneo y optimización, mayor capacidad del horno, prensas laminadoras, línea de tratamiento de boro y muchas más actualizaciones.
Esta inversión multimillonaria no sólo ha ampliado la capacidad a 90 contenedores por mes, sino que también ha permitido que los productos cumplan con mejores estándares de calidad para los mercados de Australia y América del Norte.
El objetivo de este trabajo de título es generar un plan de mantenimiento a la línea de pintado de pintura a perfiles y molduras de madera.
Estos equipos permiten el proceso de pintado con distintos materiales, tales como: barnices, lacas, tintes, entre otros materiales a base de agua, como lo es en el caso de la empresa con la que pude trabajar: ITI Chile.
El principio de funcionamiento consta en bañar la pieza o perfil con la pintura al agua con el sistema de aspiración por vacío el cual a su vez permite retirar la pintura sobrante, generando el gramaje de pintura necesaria.
Se describirá rigurosamente el funcionamiento de la línea de pintado con detallados valores reales de funcionamiento y se analizarán mejoras contundentes en los sistemas mecánicos, que incluye la línea de vacío para mejorar la confiabilidad y disponibilidad del sistema.
OBJETIVOS.
Objetivo general.
1. El objetivo general es generar y elaborar un plan de mantenimiento a la línea de vacío, el cual consta de la máquina de pintado de perfiles de madera por vacío, del área de producción de la empresa ITI Chile ubicada en el Parque Industrial de Coronel, Chile; con el fin de mejorar la confiabilidad, disponibilidad y por consecuencia, la producción de la empresa.
Objetivos específicos.
• Recopilar información técnica del contexto operacional y los antecedentes históricos de la línea de vacío de la planta de pintado ITI CHILE, priorizando un equipo crítico mediante diagrama de dispersión logarítmica.
• Jerarquizar los modos de fallo del equipo seleccionado mediante el análisis de modos de fallas, efectos y criticidad para la selección de tareas periódicas de mantenimiento.
• Planificar las tareas de mantenimiento evaluando la prefactibilidad técnicoeconómica, estimando los principales beneficios económicos."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,"2. ANTECEDENTES GENERALES, PROCESO OPERATIVO Y PROBLEMÁTICA","En el presente capítulo se presentará una pequeña, sin embargo, minuciosa reseña de lo que es ITI (Innovative Timber Ideas); tanto de la empresa original de Australia como la de Chile.
Se abarcará en el proceso de pintado de madera de la línea de vacío de la planta para la obtención de perfiles y la problemática inicial de la empresa."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.1. ANTECEDENTES GENERALES,"Desde su establecimiento en Five Dock en Nueva Gales del Sur en 1987, el Importador Independiente de Madera (ITI) se ha comprometido a redefinir el panorama de distribución y venta al por mayor de la industria de la madera.
ITI trabaja en estrecha colaboración con proveedores, clientes y asociaciones de la industria para promover la resistencia, la estética y los beneficios ambientales del uso de madera.
Al mismo tiempo, ITI redefine el papel de los mayoristas con su modelo de distribución y servicio.
Con la inversión masiva en centros de distribución, desarrollo de productos, personal y estándares de servicio a principios de 2002, ITI abandonó el nombre de un importador de madera independiente y lo reemplazó por ITI, e introdujo un nuevo logotipo con un 'concepto innovador de madera', descripción de esta actitud de la empresa.
Actualmente, ITI consta de 9 puntos de distribución en Australia y 130.000 metros cuadrados de almacén subterráneo, la fábrica de reacondicionamiento, tratamiento e imprimación de Chile, una oficina en Indonesia, 2 puntos de distribución en Nueva Zelanda y ventas en América del Norte.
De igual manera, ITI Chile se estableció en 2006 como una planta de imprimación y tratamiento para satisfacer la creciente demanda de ITI Australia de productos de madera laminada y adherida de alta calidad con el objeto de introducir sus productos de madera procesada en el mercado norteamericano.
Debido a la alta disponibilidad de pino cultivado y cosechado de manera sostenible, Chile se presenta idóneamente como el lugar preciso para construir las respectivas líneas de procesamiento y producción de imprimaciones.
Cuando la planta se completó en 2006, era la planta de procesamiento de LOSP (“es una efectiva fórmula preservante diseñada para brindar protección a largo plazo de los productos de madera utilizados sobre la tierra al aire libre”) tecnológicamente más avanzada de Chile, y continúa siendo la misma en la actualidad.
En 2010, ITI Chile se expandió de una planta de tratamiento y de imprimación a una planta de remanufactura totalmente integrada mediante la compra de una planta de remanufactura contigua, por lo que tiene la capacidad de producir 30 contenedores por mes.
En 2016, ITI Chile realizó una gran inversión en la planta, reemplazando así muchos procesos de producción manual obsoletos con equipos de automatización, dentro de los cuales se incluye moderna tecnología de escaneo, uniones de dedos de alta velocidad, aumento de la capacidad del horno, máquinas laminadoras, líneas de procesamiento y más actualizaciones.
Ilustración 1-1 Planta de remanufactura Fuente: https://itiamericas.com/about-us/"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.2. PROCESO OPERATIVO,"Actualmente, la empresa cuenta con tres plantas: remanufactura, pintado y reclasificado.
En remanufactura se recibe la madera que no está procesada, de la cual se obtienen los perfiles de madera que son de diferentes medidas, las cuales varían respecto a las especificaciones de los clientes.
La medida más común dentro de la planta es de 16 pies de largo con 23 pulgadas de grosor, la cual cuenta con variedad de anchuras; 11”, 9”, 3”, etc.
Sin embargo, el proceso continúa en la planta de pintado, la cual se encarga de pintar, clasificar y en el caso de que existan imperfecciones en la madera o pintura, reparar el producto con la finalidad de volver a pintarlo.
Por último, el procedimiento se sigue desenvolviendo en la respectiva planta de reclasificado, encargada de recibir y procesar aquella madera que no cumpla con los requisitos de calidad necesarios.
El proceso consiste en disminuir las dimensiones de los perfiles a aquellas medidas que la empresa en el momento esté necesitando.
Ahora bien, la planta de pintado consta de 3 procesos principales: pintado, clasificado y embalaje.
Los encargados de pintar la madera son: Airless 1
y
2 y Línea de Vacío.
A diferencia de Vacío, cabe destacar que Airless posee sólo una cámara de pintado y no cuenta con hornos.
El método de secado se realiza con galpones que cuentan con ventilación continua."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.3. LAYOUT DE LA LÍNEA DE VACÍO,"La línea de Vacío cuenta con un sistema de pintado completo, de marca Effisa.
Esta área cuenta con los siguientes componentes esquematizados de manera ordenada en el siguiente layout:
Fuente: ITI CHILE"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.4. SISTEMA Y PROCESO PRODUCTIVO,"La línea de vacío está constituida por los siguientes elementos:
 Dos cámaras de pintado.
Ilustración 1-2 Primera cámara de pintado.
 Dos hornos.
Ilustración 1-3 Segunda cámara de pintado.
Fuente: elaboración propia.
Ilustración 1-4 Horno principal transversal.
Ilustración 1-5 Horno secundario lineal.
Fuente: elaboración propia.
 Una línea de 4 pulidores, segmentados de la siguiente manera: 2 abajo y 2 arriba.
Ilustración 1-6 Mesa de pulidores ajustables.
5 turbinas.
 Cada cinta o cadena transportadora es propulsada por motores de 2,2 KW cada uno.
Ilustración 1-7 Turbinas.
Ilustración 1-8 Motor propulsor de cinta transportadora.
Con la finalidad de que tanto el operador como sus ayudantes trabajen correctamente, resulta menester reajustar la máquina cada vez que se cambie la medida de los perfiles.
Este proceso consta de las siguientes etapas:
 Ajustar los cargadores y guías por donde son transportados los perfiles.
Ilustración 1-9 Cargador y guía de perfiles.
 Cambiar o modificar los pulidores a la medida exacta de la superficie del perfil.
Estos cuentan con un sistema de ajuste mecánico por tornillo sin fin, que permite calibrar la altura y la profundidad del pulidor con respecto al perfil.
Ilustración 1-10 Pulidor propulsado por motor.
 Ajustar las entradas y salidas de cada cámara de pintado como se aprecia en la ilustración 1-11 Entrada a la cámara de pintado.
Ilustración 1-11 Entrada a la primera cámara de pintado.
 Realimentar las cámaras de pintado con pintura WIL-PRO VC2000 OFF WHITE TOTE o VC2700.
Ilustración 1-12 Pintura utilizada.
En esta línea, el desarrollo para la obtención de un perfil de madera de calidad va en la siguiente secuencia:
1. La máquina es ajustada a la medida y especificaciones de la madera con la que se trabajará.
Normalmente, este proceso demora entre 1 hora y 2 horas.
2. El alimentador de la máquina generalmente llena la mesa transportadora transversal de alimentado (ilustración 1-13) dependiendo del tamaño del producto, ya que al llenar la mesa con piezas demasiado grandes.
ésta tiende a atocharse debido a que el peso del producto frena a las piezas que vienen detrás a la espera de ser introducidas a la máquina, generando paradas esporádicas en el sistema.
La madera es introducida a la máquina gracias a una rueda giratoria que actúa por un cilindro neumático (ilustración 1-14), cargando la madera contra la cinta transportadora a una velocidad media de 50 m/min.
Ilustración 1-13 Mesa transversal de alimentación.
Ilustración 1-14 Cilindro accionador de rueda alimentadora de perfiles.
3. Existen dos pulidores antes de la primera cámara, uno arriba y otro por debajo del perfil, que giran en sentido contrario de la dirección a la que va el perfil, con el fin de retirar cualquier astillado o impureza que aparezca en el perfil, las cuales trabajan a una velocidad de 1450 rpm, para que así el perfil de madera entre lo más descontaminado posible sin el peligro de que se genere una obstrucción dentro de la cámara de pintado.
4. La cámara de pintado 1, baña por completo la superficie del perfil con la pintura especificada por el cliente por el principio de vacío, el cual trabaja a una presión de 1240 mmCA.
5. Al salir de la cámara de pintado, el perfil entra al horno principal el cual es transversal al sentido de trabajo de la línea de transporte.
Esto se logra mediante 5 brazos que se encargan de retirar cada perfil e introducirlo al horno.
Este horno trabaja a una temperatura promedio de 80 ° Celsius y una velocidad de 4 m/min, permitiendo un secado completo de la madera.
6. Al salir del horno transversal, el perfil es recibido por múltiples rodillos giratorios que trabajan a una velocidad de 110 m/min en sentido lineal con dirección hacia dos pulidores.
Éstos se ubican de la siguiente manera: uno arriba y otro abajo, que giran a una velocidad de 1450 rpm en sentido contrario a la dirección del perfil, permitiendo retirar las impurezas que se ubican en la superficie del perfil.
7. Luego, el perfil entra a la segunda cámara de pintado, la cual trabaja a una presión de 1240 mmCA, obteniendo un acabado en la pintura más pulcro, de acuerdo con lo que el cliente desea.
8. Por consiguiente, el perfil entra al horno secundario, el cual trabaja a una temperatura media de 550° y 650° Celsius.
La alta temperatura no genera riesgo de incendio, ya que ésta se encuentra únicamente adentro del horno alrededor de 5 segundos.
9. Al salir del horno, la madera entra al proceso de pulido y satinado que se consigue gracias a los 4 pulidores ubicados de la siguiente manera: dos arriba y dos abajo.
Dichos pulidores trabajan a una velocidad de 1450 rpm y giran en sentido contrario a la pieza.
Los pulidores son ajustables en altura y profundidad, lo que permite un mayor o menor pulido y satinado.
10.
Finalmente, la madera se encuentra con un tope, y un sensor acciona un cilindro neumático que baja los rodillos, permitiendo que la pieza entre a una mesa de 5 cintas transversales.
Esta mesa de salida trabaja a una velocidad de 8 m/min.
En esta etapa el área de calidad se encarga de monitorear el correcto estado del perfil y su pintura.
A continuación, se mostrará un esquema básico estructural de la línea de vacío: Esquema 1-2 Layout Línea de vacío.
Fuente: ITI CHILE."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.5. PROBLEMÁTICA,"Actualmente, la empresa cuenta con un déficit categórico que es comparado con años anteriores.
En profundidad, la causa de este déficit se debe al incumplimiento de los mantenimientos preventivos, centrándose solamente en mantenimiento correctivo.
Igualmente, se ha podido constatar que la línea de vacío sufre de múltiples fallas que son constantemente ignoradas por la gerencia, siendo generalmente reparadas por el mismo personal a cargo de la producción en la máquina, como en este caso el operador y sus ayudantes de producción.
Por otra parte, la máquina no cuenta con un manual de mantenimiento preventivo.
A raíz de esto, el único mantenimiento que se realiza a la máquina es una limpieza generalizada y estratégica, la cual es específica a los conductos y cañerías por donde transcurre la pintura.
El aumento de las fallas en los últimos 3 meses se ha incrementado debido al estado deplorable en que se encuentra la máquina: cintas transportadoras cortadas, cargadores y guías en mal estado, cadenas corroídas o flectadas.
Todos estos componentes de la máquina que resultan indispensables para el correcto funcionamiento no son tomados en cuenta sino hasta que fallan y generan una detención en la producción.
Se ha podido analizar y constatar que dichas fallas anteriormente mencionadas son de carácter constante y generan, al menos, dos detenciones por cada turno."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.6. MANTENIMIENTO,"A continuación, se presentarán las distintas definiciones que ayudan a comprender el capítulo de mantenimiento de una manera más vertiginosa.
 Mantenimiento.
Se entiende por mantenimiento todas las acciones y gestiones administrativas que nos permiten resguardar, preservar o incluso restaurar todo artefacto, artículo o máquina con el fin de que éste pueda llevar a cabo su respectiva función requerida.
 Mantenimiento preventivo.
Este mantenimiento se realiza de una manera calendarizada para anticipar el surgimiento de fallas que generen un problema en el funcionamiento correcto del equipo o en su defecto la detención de éste.
Ventajas:
o Disminución de mantenimientos correctivos.
o Disminución de costos.
o Disminuciones de tiempos de detenciones por falla.
o Administración del stock de repuestos.
Desventajas:
o Aumento de costos por constantes capacitaciones del personal del área de mantención.
 Mantenimiento correctivo.
Es aquel mantenimiento cuyo fin consiste únicamente en localizar y reparar las fallas que se presentan en los dispositivos o maquinarias.
Ventajas:
o Permite extender la vida útil de los equipos.
Desventajas:
o Ante la imposibilidad de predecir el fallo, genera una detención obligada de la producción.
o A largo plazo, los costos evaluados por mantenimiento correctivo son excesivos.
 Mantenimiento predictivo.
El mantenimiento predictivo o condicional evalúa el estado de la máquina y recomienda la intervención o no intervención según su estado, lo que se traduce en importantes ahorros.
Ventajas:
o Menos pérdida de material debido a paradas y reinicios no planificados.
o Reducir la tasa de intervención/diferencia año.
o Aumentar la disponibilidad de la máquina.
o Reduce el gasto en piezas de repuesto, ya que el número de intervenciones se puede disminuir durante la vida útil del activo hasta en 1/5 (por ejemplo, para rodamientos).
o Los costos de los seguros industriales son más bajos en razón de que la planta logra mejores indicadores clave de desempeño, lo que reduce los riesgos para la compañía de seguros.
Desventajas:
o Requiere una gran inversión inicial: el equipo utilizado en el mantenimiento predictivo está hecho de materiales especiales para mediciones precisas.
Por lo tanto, el uso de tales instrumentos de medición implica altos costos iniciales.
o Requiere personal calificado: La implementación de esta estrategia requiere personal calificado para recopilar y analizar datos.
Asimismo, es necesario invertir en la capacitación periódica de los empleados para que en el futuro estén capacitados para realizar los procedimientos de mantenimiento.
o No se aplica a todas las empresas: cada negocio tiene un objetivo específico, por lo tanto, en muchos casos no se puede aplicar el mantenimiento predictivo.
Para la aplicación de esta estrategia de mantenimiento se debe consensuar razonablemente el cronograma, alcance y cooperación entre las distintas áreas del negocio.
Esto incluye la gestión.
Como resultado, algunas empresas optan por contratar analistas externos para realizar procedimientos de mantenimiento predictivo para máquinas o procesos específico."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3. JERARQUIZACIÓN DE LOS MODOS DE FALLO,En el siguiente capítulo se realizarán estudios basados en el historial de fallas proporcionados por la empresa.
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.1. DIAGRAMA DE DISPERSIÓN LOGARÍTMICA: JACK KNIFE,"Este diagrama de dispersión logarítmica se realizó basándose en los datos extraídos de la administración de la empresa por una bitácora de fallas del equipo.
Esta bitácora de fallas transcurre desde inicio de agosto 2021 hasta octubre de 2021.
Esta tabla se realizó en base de la clasificación de las fallas, la cantidad de éstas y el tiempo fuera de servicio.
El MTTR viene dado por la siguiente fórmula:
𝑀𝑇𝑇𝑅 = 𝑇𝑖𝑒𝑚𝑝𝑜 𝑡𝑜𝑡𝑎𝑙 𝑑𝑒 𝑚𝑎𝑛𝑡𝑒𝑛𝑖𝑚𝑖𝑒𝑛𝑡𝑜 𝑁𝑢𝑚𝑒𝑟𝑜 𝑑𝑒 𝑟𝑒𝑝𝑎𝑟𝑎𝑐𝑖𝑜𝑛𝑒𝑠
Tabla 2-1 Muestra para uso de Jack Knife.
 | Minutos trabajad os | 66960
 | --- | ---
 | # | Subsiste ma | Cantida d fallas | de | Tiempo fuera de servicio (min) | MTT R | Tasa de fallas (1/MTBF) | Indisponibilid ad
 | --- | --- | --- | --- | --- | --- | --- | ---
 | 1 | Cámara 1 | 92 |  | 2760 | 30 | 0,0014 | 4%
 | 2 | Cámara 2 | 100 |  | 1800 | 18 | 0,0015 | 3%
 |  | Horno transvers al |  |  |  |  |  | 14%
 |  | Horno lineal |  |  |  |  |  | 10%
 |  | Mesa de pulidores |  |  |  |  |  | 1%

8%
Fuente: Elaboración propia basándose en historial de fallas de agosto 2021 - octubre 2021.
Con los datos presentados en la anterior tabla se pudo llegar a la siguiente gráfica:

 |  |  |  | Series1 ISO-tasa de fallas ISO-MTTR ISO-Indisponibilidad Lineal (ISO-tasa de fallas)
 | --- | --- | --- | ---
 | Proveedor | Entradas | Proceso | Salidas | Cliente
 | --- | --- | --- | --- | ---
 | 0,0001 | 0,1000 | 0,1000 | 
 | Grafica 2-1 Diagrama de Jack Knife.
 | Diagrama de Jack Knife
 | 1000
 | 100
 | 240
 | 90
 |  | 81,6 | 81,6 | 
 | 30 18
 | 10
 | 1
 | 1,0000
 | 0,1 0,0010 0,0100 0,1000
 | 0,01
 | Fuente: Elaboración propia de acuerdo con el historial de fallas del sistema Gracias a esta estrategia se pueden configurar los tiempos de indisponibilidad de los modos de falla de los equipos o sistemas. Se determinó un tiempo crítico de 120 (min), la cual, constituye o representa sin duda, una pérdida económica significativa para ITI Chile."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.2. SELECCIÓN DE EQUIPO CRÍTICO,"El resultado entregado gracias al diagrama de Jack Knife habilita para concluir que el subconjunto crítico para el sistema, es el Horno Transversal, el cual es el encargado de secar la pintura de manera uniforme toda la sección de cada perfil de madera.
 | Este horno cuenta con un sistema de ventilación múltiple, el cual permite obtener una mayor temperatura en menor tiempo. De igual forma, este horno cuenta con 5 cadenas orientadas transversalmente, las cuales desplazan convenientemente la madera de manera ordenada.
 | La técnica de Jack Knife indica que, el horno transversal cuenta con un modo de falla repetitivo, generando una detención importante en la producción. De este subconjunto del sistema, se generan diversas fallas que llegan a resultar críticas para la productividad de la empresa.
 | Ilustración 2-1 Horno transversal. Fuente: https://effisa.com.br/es/wp-content/uploads/2017/12/20170627100914.jpg"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.3. FUNCIÓN DEL HORNO TRANSVERSAL,"En este capítulo, teniendo el equipo crítico ya seleccionado, se analizará este componente. El equipo cuenta con una capacidad para perfiles de hasta 5800 (mm) de largo. Las dimensiones del equipo son:
 | o Longitud: 10200 (mm).
 | o Ancho: 8900 (mm).
 | o Alto: 1600 (mm).
 | Su principio de funcionamiento consiste en secar múltiples materiales, tales como: pintura, barniz, selladores, yeso, entre otros. Sin embargo, en este proceso el material utilizado es la pintura.
 | Este equipo proporciona un secado uniforme, permitiendo que los perfiles procesados puedan dar continuidad a nuevos procesos como el lijado o pulido, siguiendo el flujo natural en la línea de producción de acabado de perfiles.
 | Este horno utiliza el gas como medio de calefacción, sin embargo, en otras industrias se puede utilizar vapor, agua caliente o resistencia eléctrica, dependiendo de la comodidad y necesidad de la empresa.
 | La madera ingresa al horno gracias a 5 brazos que giran en sentido antihorario y que permiten a la madera ser retirada de la línea sin mayor complejidad. Para que la pintura logre secarse por completo de manera uniforme, el horno debe estar a una temperatura de 80 ° a 90 ° Celsius. Las cadenas que transportan los perfiles dentro del horno se mueven a una velocidad de 4 (m/min).
 | Ilustración 2-2 Salida del horno transversal por rodillos. Fuente: https://effisa.com.br/projeto-detalhes/tunel-de-secagem-transversal-omega-ff-"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.4. ANÁLISIS SIPOC: HORNO TRANSVERSAL,"El siguiente análisis habilita para identificar la función principal y secundaria del equipo e igualmente poder comprender y analizar de mejor manera los parámetros de funcionamiento que tiene el sistema analizado, a través de un proveedor, entradas, proceso, salidas y cliente.
 | Tabla 2-2 Análisis SIPOC al Horno Transversal.
 | SISTEMA: HORNO TRANSVERSAL.
 | La madera llega bañada en pintura de la primera cámara de pintado, transportada por cadenas transportadoras, | Gracias a los brazos, los perfiles son retirados del flujo de la línea, las cuales giran a una | Dentro del horno, la temperatura es de 90 ° Celsius, con 6 cadenas transportadoras que tienen un lento | A la salida del horno, los perfiles son arrastrados perpendicularmente uno por uno por 8 rodillos que giran a una velocidad de 110 (m/min). | Los perfiles son recibidos por pulidores los cuales son los encargados de retirar las

completando así la primera capa de pintura.
velocidad de 2300 (RPM).
De esta manera, los perfiles son ingresados de manera prolija a la entrada del horno.
movimiento de 4 (m/min), lo cual permite un secado uniforme por toda la sección del perfil.
impurezas que existan en la superficie del perfil, para luego ingresar a la segunda cámara de pintado.
Fuente: elaboración propia, según las funciones primarias y secundarias del horno transversal.
Del análisis, se puede deducir que, primeramente, la madera llega impregnada de pintura por todo el exterior al área del horno transversal y es retirada debido a la función de unos brazos, ser posteriormente ingresada a la entrada del horno de manera transversal.
Continuando el respectivo proceso, la madera es transportada por cadenas dentro del horno con la finalidad de secar cada perfil uniformemente.
Al finalizar el proceso, la madera es devuelta a la línea gracias a unos rodillos, retirando cada perfil de manera exacta con el uso de un sensor que indica si el perfil ha salido del horno o no. Por último, la madera ingresa a otro proceso de pintado.
Con los objetivos principales y secundarios del horno transversal y mediante el análisis SIPOC, se obtiene que la función principal de este equipo es:
o Secar la madera humedecida con pintura por el proceso y transportarla a una velocidad de producción de 4 (m/min) desde el área de pintado 1 al área de pintado 2 de manera transversal."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5. GENERACIÓN DEL FMECA,"Gracias a los estudios, investigación y análisis que anteriormente se realizaron, en referencia al equipo crítico, se definirán algunos conceptos."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.1. FALLA FUNCIONAL,"Cuando un activo físico no puede satisfacer la función deseada por el usuario, fallará.
Todo lo que un activo necesita hacer se define como una función; cada función tiene múltiples funciones y, por lo general, muchas funciones diferentes.
Dado que cada una de estas funciones puede fallar, cualquier activo puede experimentar una variedad de estados de falla diferentes.
Esto explica por qué es más preciso definir la falla en función de la pérdida de una función específica, en lugar de la falla del activo en su conjunto.
El proceso RCM utiliza el término 'falla funcional' para describir el estado de falla, no solo 'falla', por lo que, para completar la definición de falla, se deben examinar los problemas del estándar de desempeño con más cuidado.
En el caso del horno transversal, éste debe transportar diferentes cantidades de madera en kilogramos, por lo cual, con el transcurso del tiempo, esto irá deteriorando la máquina significativamente, reduciendo muchas veces la producción deseada."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.2. MODO DE FALLO,"Un modo de falla se puede describir como una situación que puede causar la falla de un activo, sistema o proceso."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.3. EFECTO DE FALLO,Son los síntomas previos que nos indican que el modo de fallo está ocurriendo o lo estará.
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.4. NÚMERO DE PRIORIDAD DE RIESGO,"A partir de los modos de falla, los impactos y el desglose de la criticidad, se identificó un número de prioridad de riesgo, NPR, para cada modo de falla, priorizando así entre cada uno.
Este NPR vendrá dado por la probabilidad de ocurrencia (Po), la severidad del resultado (So) y la probabilidad de detección (Pd) de dicho modo de falla.
𝑁𝑃𝑅 = 𝑃𝑜 × 𝑆𝑜 × 𝑃𝑑
Las variables mostradas en la ecuación anterior vienen dadas por los estándares según SAE J1739.
Este estándar define números del 1 al 10, que indican debilidad severa y aguda, respectivamente."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.5. CRITERIO: PROBABILIDAD DE DETECCIÓN DE UN MODO DE FALLA (PD),"Las siguientes tablas se utilizan con el fin de mostrar los distintos criterios de asignación numérica para el cálculo del número de prioridad de riesgo.
La Probabilidad de detección de un modo de fallo nos muestra, según la norma SAE J1739, la manera en que los operadores se pueden anticipar a un modo de fallo antes de que éste se convierta en un fallo funcional.
Tabla 2-3 Criterios para diagnóstico de un fallo.
Fuente: Norma SAE J1739"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.6. CRITERIO: PROBABILIDAD DE OCURRENCIA (PO),"Este criterio hace referencia a la frecuencia con la que ocurre un modo de fallo, teniendo una asignación numérica en el ranking.
Tabla 2-4 Criterios para el diagnóstico de la ocurrencia.
Fuente: Norma SAE J1739"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.5.7. CRITERIO: SEVERIDAD DE LA CONSECUENCIA (SO),"Este criterio se debe seleccionar de acuerdo con las consecuencias operacionales que tiene la producción respecto al equipo analizado.
Tabla 2-5 Criterios para la severidad del efecto.
Fuente: Norma SAE J1739
NPR permite priorizar, en el momento del mantenimiento, dónde algunos modos de falla presentan mayores riesgos que otros, ya sea para la seguridad del trabajador o la producción o una de las variables que pueden verse afectadas por fallas por ese modo.
Los modos de falla con NPR superior a 60 se considerarán para los propósitos de este estudio, con las tareas de mantenimiento relevantes realizadas para cada modo en la tabla de decisiones de RCM."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.6. FMECA,"En la siguiente tabla 2-6 se observa la relación existente entre la función de cada elemento del equipo crítico, su respectiva falla funcional, el modo de falla y el efecto de la falla.
Tabla 2-6 Relación entre función, falla funcional, modo de falla y efecto de falla.
 | Equipo: Horno transversal . | # | Función. | # | Falla funcional. | # | Modo de falla. | # | Efectos de la falla.
 | --- | --- | --- | --- | --- | --- | --- | --- | ---
 |  | 1 Retirar el perfil de la línea para ser ingresada al horno transversal. |  | 1. 1 | Atascamiento de perfiles en la entrada del horno. | 1.1. 1 | Desajuste del tope de salida de la línea. | 1 Ruido, rotura de piezas y detención de la máquina. | 
 | 1.1. 2 Velocidad de giro de los brazos excedido. 1 Ruido, rotura de piezas y detención de la máquina.
 |  | 2 Secar y transportar los perfiles dentro del horno. |  | 2. 1 | Perfiles húmedos. | 2.1. 1 | Conductos de ventilación apagados. 1 La |  | madera sale completamente húmeda.

2.1.
2 Bajo nivel de temperatura.
1 La madera sale
completamente húmeda.
2. 2 Atascamiento de perfiles dentro del horno.
2.2.
1 Desacople de una o dos cadenas al giro del motor rotativo.
1 Ruido por atascamiento.
2.2.
2 Peso excesivo de los perfiles transportados.
1 Ruido por atascamiento.
Retirar el perfil del horno transversal para ser ingresada a la línea.
3.
1 Atascamiento de los perfiles a la salida del horno.
3.1.
1 Sensor de salida desajustado.
1 Atascamiento y rotura de perfiles.
3.1.
2 Cinta transportadora desacoplada.
1 Atascamiento y rotura de perfiles.
3.1.
3 Entrada a la línea demasiado pequeña para el perfil.
1 Atascamiento y rotura de perfiles.
Fuente: elaboración propia, de acuerdo con función, modo de falla y efectos de la falla.
Finalmente, en la tabla 2-7, se encuentra la asignación de prioridad identificada por un número, según la norma SAE J1739.
Tabla 2-7 Asignación de la prioridad de riesgo según norma SAE J1739.
# Modo de falla Análisis de criticidad Po So Pd NPR 1.1.1 Desajuste del tope de salida de la línea.
3
6
5 90 1.1.2 Velocidad de giro de los brazos excedido.
5
6
4 120 2.1.1 Conductos de ventilación apagados.
4
8
9 288 2.1.2 Bajo nivel de temperatura.
4
8
8 256 2.2.1 Desacople de una o dos cadenas al giro del motor rotativo.
3 7 10 210 2.2.2 Peso excesivo de los perfiles transportados.
5 6 10 300 3.1.1 Sensor de salida desajustado.
7
6
5 210 3.1.2 Cinta transportadora desacoplada.
6
5
7 210 3.1.3 Entrada a la línea demasiado pequeña para el perfil.
8
7
5 280
Fuente: Elaboración propia, de acuerdo con los criterios de la norma SAE J1739"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.7. DIAGRAMA DE DECISIÓN RCM II,"El diagrama de decisión RCM II permite analizar las consecuencias de las fallas.
Dichas consecuencias pueden ser en relación con la seguridad y medio ambiente, operacionales y no operacionales o fallas ocultas.
El objetivo de este diagrama es generar tareas de mantenimiento para cada modo de fallo analizado en el FMECA.
A continuación, la tabla 8-2 está representado de manera sencilla la forma que debe tener la hoja de decisión RCM.
Tabla 2-8 Representación de la hoja de decisión RCM II.
H1 Tareas de mantenimiento Frecuencia inicial
S1
E1
O1
Sistema: Horno transversal Subsistema: Entrada al horno, horno, salida del horno.
Referencia de la información Tareas a falta de
F FF MF H S E O H4 H5 S4
Evaluación de las consecuencias Fuente: Elaboración propia, de acuerdo con el texto RCM II, John Moubray, 2004.
La información en la hoja de decisión permite organizar los datos del FMECA a través del diagrama de decisión RCM II, asignando tareas de mantenimiento para cada modo de fallo (MF) y eliminar las fallas funcionales (FF).
H2
S2
E2
O2
H3
S3
E3
N3
Realizarse por: NPR
Ahora bien, para comprender de mejor manera esta tabla, se definirá a grandes rasgos cada aspecto de esta.
Cuando se encuentra una falla oculta, se designa como H, las consecuencias en la seguridad son S, la E corresponde a las consecuencias en el medio ambiente y la O son las consecuencias operacionales y no operacionales.
Luego, las columnas que continúan van de acuerdo con cada una de las letras anteriormente mencionadas, asignándoles un número del 1 al 3.
Éstas representan el tipo de tarea proactiva que se realizará mediante la respuesta Si o No a las preguntas en el diagrama de decisión.
Las columnas H4, H5 y S4 son aquellas a las que no se le asignan tareas proactivas y que están registradas como “tareas a falta de”.
También, existe la columna de “tareas de mantenimiento” las cuales se registran para cada modo de fallo analizado, en el que exista una tarea propuesta que se requiere para mitigar la falla funcional.
El encargado de llevar a cabo la labor de mantenimiento asignado se registra en la columna de “realizarse por”.
Finalmente, el NPR, el cual se ha calculado anteriormente, indica el número de prioridad de riesgo para cada modo de falla.
Ilustración 2-3 Diagrama de decisión RCM II.
Fuente: Mantenimiento centrado en la confiabilidad RCM II, John Moubray.
Se debe responder cada modo de fallo con la pregunta H, la cual debe ir continuando la rama de decisiones de acuerdo con la respuesta de SI o NO, hasta llegar a un tipo de tarea."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.7.1. TAREAS PROACTIVAS,"Es un mantenimiento muy común denominado preventivo, aunque RCM utiliza términos como “tareas de reacondicionamiento” o “sustitución cíclicos”.
“Estas tareas se llevan a cabo antes que o curra una falla, con el objetivo de prevenir que el componente llegue a un estado de falla” (Moubray, 2004).
La persona que ejecuta el RCM escoge las tareas anteriormente mencionadas, mediante su propio criterio, el cual toma en cuenta los costos y efectividad que tendrá sobre los componentes.
A continuación, se definirán las tareas de reacondicionamiento, de sustitución y a condición:
 Tareas de reacondicionamiento cíclico: “El reacondicionamiento cíclico consiste en reacondicionar la capacidad de un elemento o componente antes o en el límite de edad definido, independientemente de su condición en ese momento”.
Evita que el componente alcance su zona de desgaste, produciendo un fallo potencial y dando origen a una falla funcional del equipo.
 Tareas de sustitución cíclica: “Las tareas de sustitución cíclica consisten en descartar un elemento o componente antes, o en el límite de edad definida, independientemente de su condición en ese momento”.
Ésta se diferencia de la anterior, ya que, el componente que falla se sustituye por uno nuevo."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.7.2. TAREAS A FALTA DE,"“Éstas tratan con el estado de falla, y son elegidas cuando no es posible identificar una tarea proactiva efectiva.
Las acciones a falta de incluyen búsqueda de falla, rediseño y mantenimiento a rotura o correctivo” (Moubray, 2004).
 Tarea de búsqueda de fallas: Se aplican a activos para el reconocimiento de nuevas fallas que puedan ocurrir.
Mitiga las condiciones que ayudan a que las fallas funcionales sean más concurrentes.
 Tareas de rediseño: Cuando un activo está produciendo altas pérdidas a la empresa, se recurre a este tipo de tarea.
Esto implica que la empresa debe tener en cuenta los altos costos por el diseño de un nuevo componente o la adquisición de éste.
 Tarea a rotura o correctiva: Este tipo de tarea es aplicable cuando los activos poseen una tasa de fallo constante a lo largo de su vida útil.
Se utiliza el máximo de tiempo de un componente, sin intervenir, hasta que algún componente se dañe, reemplazándolo por uno nuevo."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.7.3. TAREAS PREDICTIVAS,"Las tareas predictivas o a condición, son aquellas que consisten en verificar si existe alguna falla potencial, para que ésta se pueda intervenir, previniendo una falla funcional del componente y evitando las consecuencias de ésta.
Se trata de predecir si el elemento va a fallar, por lo que toma este nombre de mantenimiento predictivo, o también mantenimiento basado en la condición.
Estas tareas deben ser realizadas a intervalos menores al intervalo PF, que es el período de tiempo entre el momento en que ocurre una falla potencial y su decaimiento hasta convertirse en una falla funcional.
• Tareas de mantenimiento a condición: Este tipo de tareas recae en el monitoreo del equipo, permitiendo anticiparse a la falla funcional que está por ocurrir en algún componente del equipo.
Se debe considerar que todos los equipos trabajan dentro de parámetros que son normales para su funcionamiento, y existen, por tanto, síntomas en los equipos que permiten anticiparse a un fallo potencial que esté por ocurrir, con el objeto de evitar un posterior paro por avería del equipo.
Dicho intervalo PF se puede ver a continuación, en el cual P, representa un fallo potencial.
A su vez, F nos muestra el punto en el cual el equipo queda inoperante por producirse un fallo funcional.
Fuente: RCM II, Mantenimiento Centrado en la Confiabilidad Figura 24.
Intervalo P-– John Moubray F en las tareas a condición.
Ilustración 2-4 Intervalo P-F en las tareas a condición.
Fuente: RCM II, Mantenimiento centrado en la confiabilidad - John Moubray."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.8. HOJA DE DECISIÓN RCM II,"Sistema: Horno transversal. Subsistema: Entrada al horno, horno, salida del horno.
Referencia de la información.
Evaluación de las consecuencias.
Tabla 2-9 Hoja de decisiones RCM II.
H1
S1
E1
O1
H2
S2
E2
O2
H3
S3
E3
N3
Tareas a falta de
 |  | Tareas de mantenimiento. | Frecuencia inicial. | Realizarse por: | NPR
 | --- | --- | --- | --- | ---
 | F | FF | MF | H | S | E | O | H4 | H5 | S4
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | 1 | 1.1 | 1.1.1 | S | N | N | S | S | Tareas a condición. Chequeo de longitud de los perfiles. | Semanal. | Mecánico 1. | 90
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 |  |  | 1.1.2 | S | N | N | S | S | Tareas a condición. Recalibración de velocidad de giro de los brazos. | Semanal. | Mecánico 2. | 120
 |  |  | 2.1.1 | N |  |  |  | N N N S | Tareas de búsqueda de fallas. Diagnostico al sistema eléctrico del horno transversal. | Mensual. | Ingeniero Electromecánico. | 288
 |  |  | 2.1.2 | S | N | N | S | S | Tarea a condición. Chequeo y diagnóstico del panel de ajustes. | Semanal. | Ingeniero Eléctrico | 256
 |  |  | 2.2.1 | S | N | N | S | N S | Tarea de reacondicionamiento cíclico. Realizar recalibración a las paletas y guías de la cadena. | Mensual. | Mecánico 1. | 210
 |  |  | 2.2.2 | N |  |  |  | N N N S | Realizar un estudio de búsqueda de fallas. | Mensual. | Ingeniero en Mantención. | 300
 |  |  | 3.1.1 | S | N | N | S | S | Tarea a condición. Chequear el ajuste correcto de los sensores. | Semanal. | Electromecánico. | 210
 |  |  | 3.1.2 | S | N | N | S | N S | Tarea de reacondicionamiento cíclico. | Mensual. | Mecánico 2. | 210
 |  |  | 3.1.3 | S | N | N | S | S | Tarea a condición. Chequear el ajuste de cada guía o cargador. | Semanal. | Mecánico 1. | 280

2 2.1
2.2
3 3.1
Fuente: Elaboración propia, basándose en texto RCM II, John Moubray, 2004.
CAPÍTULO 3: PLANIFICACIÓN DEL PLAN DE MANTENIMIENTO Y COSTOS ASOCIADOS."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4. PLANIFICACIÓN DEL PLAN DE MANTENIMIENTO Y EVALUACIÓN DE COSTOS,"En el presente capítulo se realizará un estudio de la planificación de las tareas de mantenimiento para la mitigación de las fallas, y el estudio de la evaluación económica de la propuesta del plan de mantenimiento para la empresa.
Se presentará la comparativa y las principales ventajas entre la manera de realizar el mantenimiento ya existente de la empresa ITI CHILE y la propuesta de plan de mantenimiento centrado en la confiabilidad, así como también las consecuencias esperadas de la implementación de RCM.
Del análisis técnico realizado, se concretó que la planta de pintado de la empresa usaría para cada modo de fallo, con un número de prioridad de riesgo superior a 60, una tarea de mantenimiento adecuada, basada en uno de los tipos de mantenimiento existente: Preventivo, correctivo, o predictivo.
La capacidad de la producción es de 29 [ton/semana], que deben ser pintados y transportadas continuamente por las líneas de pintado hacia el área de embalaje.
Para ejecutar esto, actualmente en la empresa se realiza un tipo de mantención correctivo o a la rotura, es decir, cuando el mecanismo deja de funcionar o se rompe, se procede a realizar el cambio por un componente nuevo.
Esto es consecuencia de que no existe un plan de mantenimiento por parte del departamento de mantención y que de esta manera, se logran en muchos casos los objetivos de producción planteados por la gerencia.
De este modo, se desconocen los niveles de producción que puede lograr la empresa utilizando un plan de mantenimiento centrado en la confiabilidad.
Difícilmente será perjudicial si se tiene en cuenta la buena predisposición de los operadores, los mecánicos, los eléctricos e ingenieros.
Si técnicamente es viable, se parte porque económicamente también lo sea, y para lograr esta comprobación, se realizará el estudio correspondiente de ambas partes, comparando éstas con las ya existentes."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.1. PLANIFICACIÓN DEL MANTENIMIENTO,"Se requiere llevar a cabo tareas de mantenimiento proactivo, cambiando la ejecución de los mantenimientos que existen en la actualidad en la empresa.
El enfoque consiste en disminuir las mantenciones correctivas, aumentando también la disponibilidad de los activos.
Tareas como la creación de rutas de lubricación para los motores y cadenas, inspecciones rutinarias para ver el estado de los componentes, y más importante, generar los procedimientos que den seguridad a quien ejecutará la tarea y que se muestre el paso a paso de la mantención es de vital importancia para tener un protocolo de seguimiento y registro de lo que se realiza.
Las competencias técnicas que requiere el trabajo son de mecánica, rodamientos y lubricación, neumática."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.1.1. POLÍTICA DE MANTENIMIENTO EN LA EMPRESA,"En la actualidad, en la empresa no existe un plan para realizar reparación a los componentes que fallan.
La forma de operar consta en que, en los tiempos de producción, simplemente se realizan reparaciones correctivas a los equipos, es decir, se reparan al fallo.
Para esto se cuenta con una sola persona, el electromecánico, encargado de realizar rondas por los equipos.
Por otra parte, los operarios, cuentan con 6 ayudantes, de los cuales sólo el operador posee conocimientos adquiridos por experiencia en la empresa.
Esto, sin duda, es peligroso, puesto que si se cuenta con personas encargadas de realizar inspecciones de rutina en frecuencias de tiempo adecuadas, más el electromecánico, se podrían prevenir fallas funcionales de los equipos, evitando peédidas de producción."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.1.2. PLANIFICACIÓN DEL MANTENIMIENTO,"La planificación de mantenimiento implica los tiempos utilizados para realizar el tipo de mantención prediseñado, la persona encargada y la tarea que debe ejecutar en el área del equipo que el modo de falla involucre.
Es importante realizar un diseño para las tareas y procedimientos que se deben llevar a cabo, con el fin de registrar las condiciones de trabajo del equipo o componente que se evaluará.
Para lograr la implementación de RCM y generar un aumento en la disponibilidad de los equipos, es de suma importancia que los formatos que se diseñen presenten con total claridad cada procedimiento y tarea que llevará a cabo el personal de la empresa a los equipos con los que cuentan, recopilando toda la información, para que ésta misma sirva como respaldo en un futuro.
Para lograr el avance que se espera, la presente planificación se trabajará con hojas de procedimientos y formato de Check List.
Esta pauta de trabajo será realizada asignando tareas en orden de prioridad de riesgo según NPR de los modos de falla, para esto se reordenó la tabla presentada en el capítulo 2 para el FMECA: Tabla 3-1 Modos de falla ordenados por número de prioridad de riesgo.
# Modo de falla.
Análisis de
 |  | criticidad.
 | --- | ---
 |  | Po | So | Pd | NPR
 | --- | --- | --- | --- | ---
 | 2.1.1 | Conductos de ventilación apagados. | 4 | 8 | 9 | 288
 | --- | --- | --- | --- | --- | ---
 | 2.2.2 Peso excesivo de los perfiles transportados. 5 6 10 300
 | 3.1.3 | Entrada a la línea demasiado pequeña para el perfil. | 8 | 7 | 5 | 280
 | 2.1.2 | Bajo nivel de temperatura. | 4 | 8 | 8 | 256
 | 2.2.1 | Desacople de una o dos cadenas al giro del motor rotativo. | 3 | 7 | 10 | 210
 | 3.1.1 | Sensor de salida desajustado. | 7 | 6 | 5 | 210
 | 3.1.2 | Cinta transportadora desacoplada. | 6 | 5 | 7 | 210
 | 1.1.2 | Velocidad de giro de los brazos |  |  |  | 
 | excedido.
 |  |  | 5 | 6 | 4 | 120

1.1.1 Desajuste del tope de salida de la línea.
3
6
5 90
Fuente: Elaboración propia, basándose en norma SAE J1739"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.1.3. MEDIDAS DE SEGURIDAD,"Para llevar a cabo un mantenimiento seguro se debe de seguir un protocolo estricto de seguridad; existen 5 puntos importantes que se deben considerar y poner en práctica:
1. Identificar y comunicar las máquinas que necesitan ser aisladas: Se deben incluir todas las fuentes de energía y los puntos de aislamiento.
Se debe, igualmente, informar a los supervisores y demás trabajadores en la zona (empleados afectados) que se seccionará el accionamiento de la maquinaria.
2. Apagar la maquinaria y el equipo, detener la máquina, desenergizar el motor, vaciar de presión una manguera.
3. Aislar todas las fuentes de energía: liberar toda la energía acumulada, tales como líquidos, gases o polvo de las bombas, tuberías, presión hidráulica o neumática en caso de existir, vapor, etc.
Se deben posicionar los interruptores de aislamiento en OFF (APAGADO) y se debe dejar una tarjeta de seguridad que impida o advierta a cualquier tercero de no energizar las máquinas.
4. Subdividir el accionamiento y etiquetar todos los puntos de aislamiento: Es necesario colocar una tarjeta de seguridad en los lugares donde sea posible advertir a cualquier tercero que no debe de energizar el equipo, la etiqueta de peligro debe incluir información de contacto del responsable que esté a cargo de la mantención, en caso de cualquier posible problema, se deben dar las explicaciones correspondientes.
5. Verificar si la máquina ha sido inhabilitada: con los interruptores en posición OFF, y los motores correspondientes des energizados, se debe accionar el botón de INICIAR para ver si la máquina arranca o no.
Luego de este último paso, los electromecánicos del área de mantenimiento pueden empezar a realizar las tareas al activo.
Además de estos procedimientos, existen 5 reglas para el personal que esté involucrado en la mantención, las cuales son:
1. Toda aquella persona que entre en el área de la intervención de la máquina debe recibir una capacitación anual en relación con la seguridad y con respecto al peligro que representa el movimiento de las piezas.
2. El personal que trabaje con la máquina debe llevar puesta ropa adecuada, se prohíbe ropa suelta y accesorios.
3. El personal que tenga el cabello largo debe llevarlo recogido.
4. El personal que esté trabajando en cintas transportadoras debe estar totalmente informado de los aspectos de seguridad de la máquina y conocer los peligros de área en que trabaje.
Las áreas de peligro deben estar demarcadas con etiquetas o adhesivos, o de ser necesario, letreros."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.2. FORMATO DE LA PAUTA DE TRABAJO,"Determinando los puntos seguros para realizar las tareas y entendiendo los riesgos que existen cuando se trata de equipos, la directiva de trabajo sólo nombra una serie de pasos que deben tomar quienes van a realizarlos.
El horario de trabajo es de vital importancia, ya que incluye las tareas a realizar, las personas que las realizan, los registros de los trabajadores con sus nombres, firmas y horarios de mantenimiento de los equipos, y proporciona las horas de trabajo que deben realizar.
Cabe destacar que este tiempo es el óptimo para realizar la tarea, y puede variar dependiendo de las condiciones del entorno de trabajo.
Este modelo marcará el anverso y el reverso de la empresa, y se lleva a cabo en las etapas de producción y en una serie de pruebas a medida que concluye la temporada.
En la imagen de la página siguiente se puede ver el formato que incluirán las instrucciones de trabajo para los trabajadores y el tiempo necesario para completarlas, en el que se realizarán las tareas de mantenimiento en orden de prioridad por nivel.
La prioridad numérica del riesgo para cada modo de falla, por lo que es posible especificar un método específico para realizar todas las operaciones involucradas en el patrón de negocios.
Tabla 3-2 Pauta de trabajo de mantención Línea de Vacío.
N.º Tarea por realizar
 | 1 | Des energizar
 | 2 | Interruptores de aislamiento

 | Realiza | Nombre | Firma | Fecha | HH
 | --- | --- | --- | --- | ---
 | Electromecánico |  |  |  | 0,5
 | Electromecánico |  |  |  | 0,5

motor eléctrico
posición OFF
botones de paradas de emergencia
 | 3 | Activar | los
 | --- | --- | ---
 | 4 | Verificación la | 

 | Electromecánico | 0,5
 | Electromecánico | 0,5

correcta inhabilitación de la maquinaria 5 inspección visual a los ejes que contienen Electromecánico 0,3
Electromecánico
los piñones transmisores de potencia Comprobar el alineamiento entre los ejes y el motor Comprobar el estado de los rodamientos Engrasar los rodamientos Verificar el estado del aceite del motor
Electromecánico
Electromecánico
Electromecánico
Verificar el estado de las cadenas que mueven los carriles
cargadores
 |  | Electromecánico | 1
 | --- | --- | ---
 | 11 | Verificar el estado de los ventiladores de los hornos | Electromecánico | 2
 | --- | --- | --- | ---
 | 12 | Inspección de vibración de los motores | Electromecánico | 2
 | 13 | Inspección de vibración de los rodamientos | Electromecánico | 0.5
 | 14 | Verificación de la temperatura de los motores | Electromecánico | 1
 |  |  | Electromecánico | 0,5
 |  |  | Electromecánico | 1
 | 15 Inspección de ruidos
 | 16 | Verificar la calibración de guías y |  | 

17 Verificar vibración en las cintas transportadoras 18 Revisión de las protecciones térmicas
 | Electromecánico | 1
 | Electromecánico | 0,5

19
20 Verificar el estado de las cámaras de pintado
 | Limpieza de filtros | de
 | pintura
 | Electromecánico | 1
 | Electromecánico | 2

Total, HH 16,8
Fuente: Elaboración propia, basándose en hoja de decisión RCM II.
3.2.1 VERIFICACIÓN AL HORNO TRANSVERSAL.
Se diseña un Check List para el horno transversal, con el fin de registrar la correcta ejecución del mantenimiento.
A continuación, se muestra el formato de las hojas de Check List para el Horno Transversal:
ITI CHILE – Coronel – 2021
 |  | Check List: Horno Transversal
 | --- | ---
 | Ítem | Actividad | Si | No
 | --- | --- | --- | ---
 | 1 | Verificar energía en el tablero eléctrico |  | 
 | 2 | Verificar Temperatura de |  | 
 | quemadores
 | 3 | Verificar la |  | 
 | ventilación
 | 4 | Verificar pernos de |  | 
 | sujeción
 | 5 | Verificar alineamiento con |  | 
 | los carriles
 | 6 | Verificar cadenas |  | 
 | de transmisión
 | 7 | Verificar aceite de |  | 
 | motor
 | 8 | Verificar rodillos de salida |  | 
 | 9 | Verificar correas |  | 
 | transportadoras
 | 10 | Verificar brazos de |  | 

entrada Observaciones
Nombre Fecha Firma
Fuente: Elaboración propia, Check List Equipo Horno Transversal"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.3. COSTOS ASOCIADOS DEL PLAN DE MANTENIMIENTO,"Esta sección evalúa el costo de realizar tareas de mantenimiento para reducir los modos de fallos específicos y la eficiencia del mantenimiento, en relación con los costos directos, como materiales.
El grupo de tareas determinará la duración de las tareas a realizar, dependiendo de si se trata de una condición, una reparación periódica, un reemplazo periódico, un diagnóstico o una reparación.
La empresa mantiene el costo de mano de obra o el costo de la hora de trabajo, por lo que el valor será muy cercano al valor real.
En este punto, se evaluarán los costos existentes por los modos de falla elegidos, traducidos en tiempo y efecto en función de cómo afecten al proceso de fabricación, para comparar con cómo se está realizando el mantenimiento actual, con los nuevos procedimientos de mantenimiento.
A continuación, en la siguiente página, se muestra la tabla 3.4.
Costos de repuestos, los costos de los materiales que se deben utilizar.
Tabla 3-4 Costos de repuestos y materiales a adquirir.
 | Repuesto | Cantidad | Precio Unitario | Total
 | --- | --- | --- | ---
 | Cinta transportadora | 10 | $ 10.990 | $ 109.900
 | Correa con superficie de agarre | 3 | $ 18.990 | $ 56.970
 | Rodillo | 5 | $ 20.990 | $ 104.950
 | Rueda con rodamiento | 5 | $ 21.990 | $ 109.950
 | Mobil Chainlube | 1 | $ 45.067 | $ 45.067
 | Mobil Grease MP | 1 | $ 40.240 | $ 40.240
 | Mobigear 600 XP 220 | 1 | $ 82.808 | $ 82.808
 |  |  | Total | $ 549.885

Fuente: Elaboración propia, basándose en cotización productos nacionales."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.3.1. ANÁLISIS ECONÓMICO DE RENTABILIDAD,"Si se realiza un plan de mantenimiento preventivo basado en la confiabilidad, se ven señales muy positivas para la empresa.
Esto significa que es posible lograr esto y hacer RCM, siempre que el foco esté en el origen, así como en el foco selectivo importante, que en este caso es el horno horizontal.
Reducción del tiempo de inactividad por falla del equipo, menor probabilidad de falla en el próximo período y vida útil actual en comparación con el período anterior, todo enfocado en medir la efectividad de la adopción de RCM, que se espera que mejore drásticamente con la adopción del mantenimiento centrado en la confiabilidad.
Como se puede apreciar en la Figura 3.7. Con la implementación del RCM, se espera lograr un 98,7% de disponibilidad que, a efectos ilustrativos, es generado por el gráfico circular.
Grafica 3-1 Disponibilidad esperada para el periodo de producción 2022- Fuente: Elaboración propia, en base a la mejora esperada por implementación del RCM II.
El aumento de la disponibilidad de los equipos en un 1%, equivale a disminuir los tiempos de parada de 15 a 10 horas, teniendo en cuenta que el costo por hora es de $2.121,5 UF, esto significaría que la empresa tendrá una ganancia de:
5 (ℎ) × 2121,5 (𝑈𝐹 𝐻) = 10,607.5 𝑈𝐹
Considerando los costos por repuestos y materiales que se deben adquirir para la implementación del plan de mantenimiento, hace un total de 17,71 UF en gastos totales.
Teniendo en cuenta que el RCM logrará aumentar la disponibilidad, aumentando en 1%, para obtener la rentabilidad de este proceso, se debe sustraer el costo total de los costos para implementar el plan de mantenimiento al ingreso total recién estudiado, entonces la ganancia esperada será de:
𝐺𝐴𝑁𝐴𝑁𝐶𝐼𝐴 = 10,607.5 𝑈𝐹 − 17,71 𝑈𝐹 = 10,589.7 𝑈𝐹"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,5. CONCLUSIONES Y RECOMENDACIONES,"Luego de haber abordado rigurosamente cada punto analizado en el presente trabajo de investigación, y teniendo en consideración los diferentes comentarios en relación a los esquemas, valores, imágenes, se permite llegar a la conclusión de que principal problema de la empresa es la falta de capacidad de planificación adecuada para los activos, creando de esta manera, un notorio desorden durante el mantenimiento de prioridad y equipo.
La propuesta de crear un plan de mantenimiento efectivo se ha presentado como una solución, la cual se encuentra respaldada por las normas SAE J1011, J1012 y SAE J1739 y que conllevarían aparejado el resultado de reducir la falla y, a su vez, aumentar el uso de la capacidad de dispositivos.
Al seleccionar un equipo crítico, el sistema de descentralización de los modos de falla, el análisis de la clave y las subfunciones para completar la tabla de decisiones de FMECA y RCM II y la tarea de prioridad de riesgo para cada modo predeterminado, puede entender cuántas tareas deben ser priorizadas para otros.
La propuesta se ha beneficiado de la compañía desde la fase de detención incorrecta para producir pérdidas significativas de ganancias, además de crear un proceso tangible para los trabajadores relacionado con el correcto proceso de mantenimiento, el cual debe ser de una manera clara y segura.
Finalmente, se puede mencionar que la aplicación RCM siempre es efectiva, mientras que el enfoque sea el correcto.
Se determina la importancia de un grupo, así como la prioridad de la tarea que minimiza la falla, pero siempre resulta de suma importancia mantener la motivación de los trabajadores, quienes son el activo precisa para que la gestión se lleve a cabo y se mantenga en función de la confiabilidad.
El desarrollo de la política de mantenimiento juega un papel relevante para RCM que puede ser sostenible a lo largo del tiempo debido a los activos y procesos que forman parte de la evolución permanente, estas políticas deben ajustarse por tiempo.
Debería leer el texto RCM II, John D del ejercicio original.
Mouble, 2004, así como las regulaciones de SAE que se denominan durante el trabajo actual para familiarizarse con la implementación del proceso de implementación de RCM."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1. Deﬁnición del Problema,"En este capı́tulo se abordará de forma general el contexto de la investigación realizada.
En primera instancia se tratará el descubrimiento de conocimiento de clientes bancarios a través de modelos predictivos y el valor que tiene esto para el negocio de la institución bancaria.
En segundo lugar, se abordará qué son los modelos de Scoring y los procesos asociados, junto al gran problema que trae desarrollar un modelo predictivo de este tipo.
Finalmente se abordarán los objetivos principales de la memoria.
Se espera durante este capı́tulo dar un marco general de la investigación realizada e informar sobre los beneﬁcios de mejorar un modelo de Scoring."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.1. Descubrimiento de Conocimiento con Modelos Predictivos,"Para desarrollar estos modelos, la Institución Bancaria asociada al proyecto cuenta con una metodologı́a que consta de varios procesos, los cuales apuntan a crear un modelo con un buen poder predictivo, con esto, se indica que todas las predicciones serán certeras y la probabilidad de errar es mı́nima.
Cuando un modelo cumple los niveles esperados de predicción, se lleva a producción, es decir, se integra al negocio correspondiente.
Con el modelo integrado se logra estimar la pérdida esperada asociada a una cartera de clientes.
Entre los modelos conocidos, los más utilizados son los modelos de Scoring ya que éstos entregan como salida un puntaje para un cliente, el cual es aplicable en varios aspectos del negocio y, por ende, facilita el uso en la gestión.
Por ejemplo, un puntaje bajo del cliente, indica que éste tiene asociado un mayor riesgo y dependiendo del negocio donde se utiliza el modelo, el riesgo se traduce con el signiﬁcado especiﬁco para un negocio o contexto.
Para desarrollar un modelo de Scoring, se ejecuta una serie de etapas antes de construirlo; el primero es la deﬁnición del problema, el cual es la etapa inicial de un proyecto donde se desarrolla un modelo de Scoring, y donde participan los analistas y las personas asociadas al negocio.
Luego se realiza la etapa denominada calidad de datos, en la cual se deﬁnen las fuentes de datos necesarias y se certiﬁcan que estos datos están aptos para reﬂejar el comportamiento del negocio estudiado, para ası́ dejar claro que el futuro modelo a desarrollar aportará en la gestión del negocio respectivo con los datos que se tienen.
Cuando las fuentes ya están certiﬁcadas para el desarrollo del modelo, se ejecuta la etapa de muestreo, en la cual se busca generar un subconjunto de todos los datos, con el cual se trabajará posteriormente para el desarrollo del modelo.
Esta muestra debe cumplir una serie de requisitos que garanticen que los resultados obtenidos sean extrapolables a todo conjunto de los datos.
Ésta es una de las primeras etapas cruciales para asegurar un buen modelo predictivo.
Teniendo lista la muestra representativa de toda la población, se continua con una etapa clave para el desarrollo del modelo llamado transformación de las variables, cuyo objetivo es ampliar un conjunto de variables generada con las fuentes de datos, para reﬂejar comportamientos complementarios y facilitar la captura de patrones de acuerdo al modelo en desarrollo.
Para lograr esto, se utilizan miradas estadı́sticas, polı́ticas y de conocimiento del negocio, las cuales permiten pre-seleccionar la cantidad de variables estudiadas, para ası́ centrar el análisis en las verdaderas candidatas a explicar cambios con el modelo.
Terminado la transformación de las variables, se ejecuta la etapa de selección de variables que tiene por objetivo reducir la cantidad de variables a analizar en la etapa posterior con el ﬁn de enfocar los esfuerzos en las mejores candidatas, para ası́ maximizar el poder predictivo de los modelos.
Para lograr esto, se aplica una serie de análisis y ﬁltros, los cuales permiten certiﬁcar que se cumplan ciertos estándares mı́nimos y supuestos necesarios para el correcto desarrollo del modelo.
Por último, luego de todos las etapas anteriores, se procede con la etapa de modelamiento y validación, donde se ejecuta una metodologı́a de modelamiento para construir un modelo de Scoring con un único indicador de riesgo que resuma la información de las variables que explican el comportamiento del negocio respectivo.
Este indicador debe tener una interpretación simple y con sentido de negocio; además, debe permitir ordenar los clientes de acuerdo a su nivel de riesgo de una forma sencilla.
La tecnica tradicional de estimación corresponde a la regresión logı́stica.
Esta técnica de modelamiento es la más común para generar un puntaje asociado al riesgo del cliente; dado esto, la Institución Bancaria también la utiliza y las etapas de pre-entrenamiento de un modelo de Scoring se enfocan a que las variables (transformadas y seleccionadas) que entrarán al modelo de regresión logı́stica, se ajusten a ésta para potenciar su funcionamiento y predicción.
El área que desarrolla los modelos ha crecido bastante con los años.
Actualmente se desarrollan modelos en masa; por ejemplo, en un año, se producen alrededor de 20 modelos junto a un grupo grande de analistas.
Las cantidades de horas invertidas en la metodologı́a para desarrollar un buen modelo antes de que llegue a producción son grandes, ya que toman varios meses, incluso con los grandes proyectos pueden tomar hasta alrededor de dos años con varios analistas trabajando en paralelo para agilizar su desarrollo.
Es por esto que la institución bancaria invierte muchos recursos en seguir manteniendo esta área, para ası́ producir cada vez más modelos.
Además, también busca que los tiempos de desarrollar un modelo se reduzcan, invirtiendo en mejores tecnologı́as para manejar los grandes volúmenes de datos que se necesitan para abordar los grandes proyectos."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.2. El problema,"La metodologı́a utilizada en la Institución Bancaria está alcanzando los lı́mites del poder predictivo en sus modelos de Scoring, ya que ésta tiene asociados ciertas etapas estandarizadas para desarrollar un modelo, de las cuales algunas están siendo poco efectivas en alcanzar un buen poder de predicción y, por ende, los modelos ﬁnales no cuentan con el suﬁciente poder predictivo que la Institución espera para que se estime bien la pérdida esperada en una cartera de clientes.
Cuando un modelo no cumple los niveles esperados de predicción se descarta, provocando la pérdida de todas las horas invertidas por los analistas en el modelo malo.
Dado que se debe alcanzar un modelo que prediga bien, el analista debe seguir iterando con las mismos etapas estandarizados hasta que el modelo aumente su poder predictivo.
Las iteraciones se vuelven tediosas debido a que hay etapas que requieren muchas horas para volver a obtener un resultado, ya que no están optimizadas en tiempo de ejecución.
Entre algunos de los problemas que limitan el poder predictivo, está que la técnica de modelamiento de regresión logı́stica no funciona en las mejores condiciones cuando las variables que explican el comportamiento del negocio no tienen monotonicidad creciente o decreciente; es por esto que en el proceso de transformación de variables se busca que todas las variables tengan atributos que reﬂejen alguna de dichas monotonicidades en el riesgo.
Pero no se puede lograr esto para todas las variables, ya que algunas tienen un comportamiento no lineal para el negocio.
Lamentablemente la mejor opción con esta técnica, es descartar las variables no lineales al menos que sea estrictamente necesario por el negocio que no sean ﬁltradas.
Otro problema de la metodologı́a, es que algunas etapas llevan un tiempo de ejecución muy alto.
Esto se mitiga por una parte, corriendo la etapa dentro de un software fuera del horario de trabajo de los analistas, pero esto solo soluciona el problema de no invertir muchas horas de trabajo presencialmente en algunos procesos.
No obstante, no existe ninguna optimización real de estas etapas o una mejora inteligente para su ejecución por lo que, cuando es necesario iterar variar veces en el mismo proceso, el analista consume mucho de su tiempo en una pequeña parte de la metodologı́a, provocando el atraso del desarrollo completo del modelo.
Cuando un modelo no es puesto en producción en las fechas establecidas al comienzo del proyecto, se produce una pérdida indirecta para la Institución Bancaria, ya que en el tiempo en que no se utiliza el modelo para su propósito especı́ﬁco, la gestión del riesgo no mejora en el negocio, por lo que se pierden millones de ganancias monetarias por la gestión no óptima en el manejo de recursos de la institución."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.3. Objetivos,
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.3.1. Objetivo principal,"Mejorar la predicción de modelos de scoring de una institución bancaria, mediante nuevas técnicas de maquinas de aprendizaje, para apoyar la toma de decisiones automatizadas en los negocios de la institución."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.3.2. Objetivos especı́ﬁcos,"Para lograr el objetivo principal propuesto es necesario, cumplir los siguientes objetivos especı́ﬁcos:
• Diseñar una metodologı́a nueva enfocada a los procesos de Transformación de Variables, Selección de Variables y Modelamiento, para generar mejoras en los modelos actuales en base a técnicas de modelamiento y no por la búsqueda de nuevas variables en etapas anteriores a la transformación.
• Implementar nuevas técnicas de modelamiento que aumenten el poder predictivo de los modelos de scoring dentro de los procesos actuales, con el ﬁn de potenciar las decisiones automatizadas en los negocios ﬁnancieros en que se ejecutan los modelos.
• Mejorar los indicadores asociados a la eﬁciencia de las etapas a rediseñar (Transformación de Variables, Selección de Variables y Modelamiento) y validar empı́ricamente el valor agregado que entregan."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.4. Beneﬁcios para la Institución Bancaria,"Con esta investigación se entregan las bases para aplicar una nueva metodologı́a con las implementaciones necesarias para alcanzar el objetivo principal propuesto, con el ﬁn de aumentar el poder predictivo de los futuros modelos y reducir su tiempo de desarrollo.
El beneﬁcio de un modelo con mejor poder predictivo se concreta en que el área donde se integre causará mayor impacto en la gestión del área y sus decisiones, lo cual se traduce en un uso eﬁciente de recursos y la correcta evaluación del riesgo de los clientes.
Con estos cambios se logrará reducir el trabajo de los analistas de desarrollo de modelos para alcanzar un modelo de scoring con buen poder predictivo.
Entre estos beneﬁcios, está la optimización de las etapas asociadas a pre-entrenamiento del modelo a través de etapas automatizadas, reduciendo los cambios manuales realizados por los analistas con las herramientas utilizadas hasta ahora.
Logrando esto, las etapas de Transformación de las Variables y Selección de Variables serán más rápidas de ejecutar, dando el beneﬁcio de invertir menos tiempo en una o varias iteraciones de ellas con el ﬁn de enfocarse en otras partes de la metodologı́a."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,1.5. Metodologı́a a utilizar y alcance,"Para lograr el objetivo de esta investigación, se aplicarán las buenas prácticas de CRISPDM; dado que todas las etapas mencionadas en este capı́tulo (presentes en la Institución Bancaria) están basadas en esta metodologı́a, es recomendable seguir basándose en ella para construir los modelos de scoring y validar su poder predictivo.
Por otro lado, la solución tomará en cuenta recomendaciones de otros autores que han investigado temas similares relacionados a construir buenos modelos predictivos para el mundo ﬁnanciero.
Por ultimo la Institución Bancaria necesita que la investigación comience con el proceso de Modelamiento dado que es el objetivo especiﬁco que más les interesa, dado que implementar nuevas técnicas de modelamiento es imposible actualmente debido a las limitaciones que trae el software SPSS Modeler respecto a nuevas técnicas.
Respecto al alcance, la metodologı́a se desarrollará en el entorno de programación Python, ya que la Institución Bancaria necesita en el futuro una metodologı́a que no dependa principalmente de un software (SPSS Modeler), si no que un entorno de programación (Scripting) con el ﬁn de aprovechar las bibliotecas que con el tiempo se han potenciado para implementar soluciones de minerı́a de datos y máquinas de aprendizaje.
Esta restricción fue impuesta con la Institución para que la investigación aproveche el potencial de Python, y ası́, demostrar a los analistas el poder de estas bibliotecas."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2. Estado del Arte,"En este capı́tulo se revisarán conceptos y conocimientos especı́ﬁcos que darán contexto al desarrollo de modelos de scoring, además de detallar las etapas en torno a la metodologı́a actual para dejar claro cómo funciona cada una de ellas.
Por último, se abarcará el estado del arte respecto a autores que trabajaron en temas de “Credit Scoring” y sus resultados de benchmarking con varias técnicas de modelamiento.
Se espera entonces informar sobre todos los conceptos necesarios para abarcar esta investigación dentro de la Institución Bancaria."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.1. Minerı́a de Datos,"La minerı́a de datos es un campo de la estadı́stica y las ciencias de la computación, que se reﬁere a un proceso que intenta descubrir patrones en grandes volúmenes de conjuntos de datos [7].
Utiliza los métodos de inteligencia artiﬁcial, máquinas de aprendizaje, estadı́stica y sistemas de bases de datos.
El objetivo general del proceso de minerı́a de datos consiste en extraer conocimiento de un conjunto de datos y transformarlo en una estructura comprensible para su uso posterior.
Este puede ser utilizado para mejorar procesos, disminuir costos, aumentar ganancias, etc.
Existen muchos software de minerı́a de datos que permiten a sus usuarios analizar datos recopilados desde muchas dimensiones o ángulos diferentes, resumiendo todo en una serie de relaciones identiﬁcadas entre las variables estudiadas.
Por lo general, la minerı́a de datos se utiliza para encontrar correlaciones o patrones entre docenas de variables, o para encajar en un contexto ciertos campos de una gran base de datos relacional.
Un proyecto de minerı́a de datos está compuesto de cinco etapas principales:
• Extraer, transformar y cargar datos en el data warehouse (ETL1).
• Almacenar y administrar los datos en un sistema de bases de datos relacional.
• Dar acceso a los datos a analistas del negocio y profesionales de TI.
• Analizar los datos con aplicaciones especializadas.
• Presentar la información en formatos útiles, como gráﬁcos o tablas."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.2. Procesos de Minerı́a de Datos,"En minerı́a de datos existen varios procesos estándares para alcanzar el objetivo de la disciplina.
Se revisarán los procesos más utilizados."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.2.1. Proceso de descubrimiento del conocimiento (KDD),"Recibe este nombre el proceso que tiene por entrada la base de datos y como salida el subconjunto de patrones que se transformarán en conocimiento, luego de la aplicación de minerı́a de datos.
Este proceso cuenta con cinco fases fundamentales que se encuentran en la ﬁgura 2.1:
1. Selección de Datos: esta etapa consiste en deﬁnir un conjunto de datos, o enfocar los esfuerzos en una serie de variables de los mismos.
En ésta, es fundamental contar con
1Por sus siglas en inglés: Extract, Transform, Load.
un conocimiento previo del negocio, que ayude a deﬁnir cuáles variables son relevantes para el estudio y cuáles no. Por ejemplo, si se desea descubrir qué clientes son más susceptibles a un esfuerzo de marketing, casi con certeza el nombre del cliente no será una variable importante para el estudio, pero sı́ el segmento económico o el nivel de ingresos del mismo.
2. Preprocesamiento de Datos: se busca limpiar los datos; esto quiere decir que se tomará una serie de acciones para que los datos no cuenten con inconsistencias u observaciones faltantes/inválidas.
Durante esta etapa se realiza una limpieza de los datos:
• Faltantes: en torno a esta situación se puede tomar una serie de acciones, como ignorar datos con observaciones faltantes, llenarlos manualmente, usar una variable global para llenarlos (como N/A, nulo, -inf, etc) o alguna solución con un estadı́stico, como poner la media del atributo con respecto a todos los datos, usar la media del atributo considerando sólo los datos de la misma clase o el valor más probable del dato.
• Inconsistentes: se generan principalmente por variaciones al momento de ingresarlos, como el uso de diferentes capitalizaciones o faltas de ortografı́a.
Una inconsistencia puede ser, por ejemplo, si en una observación de persona, su tipo de vivienda es “Departamento”, mientras que en otra es “Depar.”
o “Dpt”.
Se entiende que todas las observaciones hacen referencia a un departamento, pero por errores o decisiones humanas tienen un valor diferente.
3. Transformación de Datos: se realizan todas las transformaciones necesarias a los datos para que puedan ser interpretados de mejor manera por los algoritmos de minerı́a de datos.
Dependiendo de los algoritmos a aplicar, se requiere aplicar uno o más tipos de transformación, siendo algunas de ellas:
• Normalización: consiste en representar los valores de las observaciones en un intervalo deﬁnido; por ejemplo, normalizar los datos para que sus valores estén dentro del rango [0,1].
Este método es de particular importancia cuando se planea utilizar técnicas de clustering basadas en distancia, ya que al no aplicarse, se desbalancea la importancia de diferentes variables por culpa de las unidades de medidas usadas.
Por ejemplo, de distorsiona/transforma la distancia, dándole más importancia a una variable de mayor magnitud, como podrı́a ser el ingreso per cápita de una base de datos de clientes (orden de los cientos de miles y millones) respecto de la edad.
• Agregación: utilizada cuando se desea agrupar variables.
Por ejemplo, pasar una serie de registros de ingreso mensual a una cantidad más reducida de registros de ingreso anual.
4. Minerı́a de datos: consiste en la búsqueda de patrones de interés en alguna forma particular de representación, dependiendo del objetivo ﬁnal de la minerı́a de datos.
5. Interpretación/Evaluación: en esta etapa ﬁnal, se interpretan y evalúan los patrones encontrados, con el ﬁn de juzgar su utilidad para el objetivo ﬁnal o negocio, además de su asertividad.
Figura 2.1: Fases del descubrimiento del conocimiento (KDD)"
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.2.2. Cross-Industry Standard Process for Data Mining (CRISP-DM),"CRISP-DM [2] recibe su nombre del acrónimo en inglés y consiste en una metodologı́a con un ciclo de vida de seis etapas, la cual se observa en el ﬁgura 2.2:
1. Entendimiento del negocio: se busca comprender los objetivos y requerimientos del proyecto desde el enfoque del negocio, para luego transformarlo en un problema de minerı́a de datos y un plan preliminar para alcanzar los objetivos.
2. Entendimiento de los datos: empieza con un conjunto de datos inicial y se busca familiarizarse con ellos, identiﬁcar problemas de calidad, descubrir una primera mirada o subconjuntos interesantes con el ﬁn formular una hipótesis para información escondida.
3. Preparación de los datos: comprende todas las actividades necesarias para generar el
set de datos ﬁnal a partir de los datos en bruto.
4. Modelado: es la aplicación de una o varias técnicas de modelamiento, calibrando sus parámetros a valores óptimos.
5. Evaluación: los modelos obtenidos son juzgados y los pasos para construirlos son evaluados con el ﬁn de concluir con seguridad que efectivamente se cumple con los objetivos del negocio.
6. Despliegue: el término del modelo por lo general no signiﬁca el ﬁn del proyecto.
El conocimiento obtenido luego debe ser organizado y desplegado de forma que el cliente ﬁnal (el negocio) pueda utilizarlo.
La ventaja de CRISP-DM sobre KDD es que está más aterrizado a la aplicación en la industria dado que todas las etapas están centralizadas en la iteración constante con los datos (para cada etapa) y por ende, al momento de volver a iterar en una de las etapas del proceso para corregir algo, se reduce el esfuerzo de ésta.
Este es uno de los factores claves de porque las instituciones preﬁeren descartar KDD.
Figura 2.2: Fases de la metodologı́a CRISP-DM"
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.3. Algoritmos de Modelado para la Minerı́a de Datos Predictiva,"En esta sección se deﬁnirán, a nivel general, algunos algoritmos de aprendizaje supervisado, el cual consiste en una técnica para deducir una función a partir de datos de entrenamiento.
La salida de la función puede ser un valor numérico (para algoritmos de regresión) o una etiqueta de clase (para algoritmos de clasiﬁcación).
El objetivo del aprendizaje supervisado es crear una función capaz de predecir el valor correspondiente a cualquier objeto de entrada válido después de haber visto una serie de ejemplos, llamados datos de entrenamiento.
Para esto, tiene que generalizar a partir de los datos presentados a las situaciones no vistas previamente.
A continuación, se presentan algunas técnicas que corresponden a los tipos de algoritmos más populares [3];
• Regresión: se ocupa de modelar la relación entre las variables que se reﬁna iterativamente usando una medida de error en las predicciones hechas por el modelo.
Los métodos de regresión son un caballo de batalla de las estadı́sticas y han sido conﬁnados en el aprendizaje de la máquina estadı́stica.
Algunos de los algoritmos de regresión más populares son Ordinary Least Squares Regression (OLSR), Linear Regression, Logistic Regression y Stepwise Regression.
• Árboles de decisión: construyen un modelo de decisiones basadas en valores reales de atributos en los datos.
Estos algoritmos generan bifurcaciones en estructuras de árbol hasta que se tome una decisión de predicción para un registro dado.
Los algoritmos de árboles de decisión más populares son Classiﬁcation and Regression Tree (CART),
Iterative Dichotomiser 3 (ID3), C5.0, Chi-squared Automatic Interaction Detection (CHAID) y Conditional Decision Trees.
• Bayesianos: aplican explı́citamente el teorema de Bayes para problemas como la clasiﬁcación y la regresión.
Los algoritmos bayesianos más populares son Naive Bayes, Gaussian Naive Bayes, Multinomial Naive Bayes y Bayesian Network (BN).
• Redes neuronales artiﬁciales: son modelos que se inspiran en la estructura y/o función de las redes neuronales biológicas.
Son una clase de concordancia de patrones que se usan comúnmente para los problemas de regresión y clasiﬁcación, aunque realmente son un enorme sub-campo compuesto de cientos de algoritmos y variaciones para todo tipo de tipos de problemas.
Los algoritmos más populares son Perceptrón,
Back-Propagation, Hopﬁeld Network y Radial Basis Function Network (RBFN).
• Ensamblaje: son modelos compuestos de muchos otros más débiles que son independientemente entrenados y cuyas predicciones se combinan de alguna manera para hacer la predicción general.
Se pone mucho empeño en qué tipos de modelos débiles se deben combinar y en las formas de combinarlos.
Los algoritmos de ensamblaje más populares son Boosting, Bootstrapped Aggregation (Bagging), AdaBoost, Gradient Boosting Machines (GBM), Random Forest, Extra Tree.
• Máquinas de vectores de soporte (SVM): una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases lo más posible mediante un hiperplano de separación deﬁnido como el vector entre los dos puntos, de las dos clases, más cercanos al que se llama vector soporte.
Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en función de los espacios a los que pertenezcan, pueden ser clasiﬁcadas a una u otra clase.
Más formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso inﬁnita) que puede ser utilizado en problemas de clasiﬁcación o regresión.
Una buena separación entre las clases permitirá una clasiﬁcación correcta."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.4. Regresión Logı́stica,"La regresión logı́stica es un modelo estadı́stico de regresión que posee una variable dependiente categórica [5], la que puede ser de naturaleza dicotómica (regresión logı́stica binaria o binomial) o con más valores (regresión logı́stica multinomial).
El modelo estima la variable de respuesta mediante una probabilidad, la cual se ajusta mediante la función logı́stica [4].
Las variables explicativas, independientes o covariables, deben ser dicotómicas, por lo que en caso de que la naturaleza de algunas de éstas sea de tipo continua o presente más de una categorı́a, se debe aplicar una transformación de los datos, que permita codiﬁcar la información en variables binarias.
Por sus caracterı́sticas, los modelos de regresión logı́stica pueden ser utilizados para los siguientes objetivos:
• Cuantiﬁcar la importancia de la relación existente entre cada una de las covariables y la variable dependiente, lo que lleva implı́cito también clariﬁcar la existencia de interacción y confusión entre covariables respecto a la variable dependiente, es decir, conocer el odds ratio para cada variable explicativa.
• Clasiﬁcar individuos dentro de las categorı́as (presente/ausente) de la variable dependiente, según la probabilidad que tenga de pertenecer a una de ellas dada la presencia de determinadas covariables.
La regresión logı́stica es una de las herramientas estadı́stica más populares en la industria debido a su capacidad para el análisis de datos y su fácil explicación al negocio.
El objetivo principal que resuelve esta técnica es el de modelar cómo inﬂuye en la probabilidad de aparición de un suceso, habitualmente dicotómico, la presencia o no de diversos factores y el valor o nivel de los mismos.
También puede ser usada para estimar la probabilidad de aparición de cada una de las posibilidades de un suceso con más de dos categorı́as (politómico).
La regresión logı́stica tiene ciertos supuestos que es recomendable tener en cuenta para desarrollar un buen modelo en base a esta técnica, los que se exponen a continuación:
• Puede manejar cualquier tipo de relación no necesariamente lineal, ya que aplica una transformación logarı́tmica no lineal.
• Las variables explicativas pueden ser continuas o discretas (categóricas u ordinales) y no necesitan ser independientes, pero en caso de serlo, la regresión da una solución estable.
• Se debe tener especial consideración en que la relación entre la variable independiente y la probabilidad del suceso no cambie de sentido, ya que en este caso el modelo logı́stico deja de tener la interpretabilidad deseada.
• Si las variables que intervienen están muy correlacionadas, el modelo logı́stico estará desprovisto de sentido y los valores de sus coeﬁcientes no serán interpretables."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.5. Preprocesamiento y Selección de variables,"Para la preparación de los datos que se utilizarán en un modelo, es necesario realizar un proceso exhaustivo para elegir los mejores datos, transformarlos a una forma de fácil interpretación para el modelo y seleccionar a los mejores candidatos, con el ﬁn de que la técnica de modelamiento se utilice con las mejores variables y se reduzca su dimensionalidad.
Las herramientas utilizadas se centran en tres etapas: Muestreo, Transformación de las variables y Selección de variables.
A continuación, se detallarán algunas herramientas utilizadas en estas últimas dos etapas de preparación de los datos."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.5.1. Transformación de las Variables,"El tratamiento de variables es una etapa clave dentro del proceso de desarrollo de un nuevo modelo.
Ésta tiene por objetivo ampliar un conjunto de variables generada con las fuentes de datos, para reﬂejar comportamientos complementarios y facilitar la captura de patrones de acuerdo al modelo en desarrollo.
Esta etapa tiene, además, varias actividades asociadas a ﬁltros preliminares de variables en función de su poder predictivo.
Para esto se utilizan miradas estadı́sticas, polı́ticas y de conocimiento del negocio, las cuales permiten disminuir la cantidad de variables estudiadas y centrar el análisis en las verdaderas candidatas a explicar cambios en la variable dependiente.
Para lograr esto, la transformación de variables debe pasar por tres actividades, la primera es la creación de nuevas variables, la segunda es la discretización de las variables y la tercera es el análisis bivariado, las que se detallarán brevemente a continuación: Creación de nuevas variables
Respecto al trabajo realizado junto al negocio, se construyen nuevas variables explicativas del negocio para una posterior evaluación del poder predictivo luego del análisis bivariado.
Discretización
Esta actividad consiste en la transformación de las variables a tramos o categorı́as con el ﬁn de poder aplicar un análisis bivariado posteriormente.
Para asignar un puntaje en base a cada variable, es necesario discretizar tanto las variables continuas como categóricas; para esto, se aplican técnicas de tramiﬁcación como, por ejemplo, arboles de clasiﬁcación y regresión, que se explican brevemente a continuación (en los cuadros 2.1 a 2.2 se ejempliﬁcan resultados de discretización):
Arboles de Clasiﬁcación y Regresión: uso de algoritmos de árbol con el ﬁn de generar y/o identiﬁcar grupos de individuos que tengan caracterı́sticas similares.
La ventaja de estos algoritmos es que la noción de similitud es calculada mediante indicadores estadı́sticos.
Los arboles de regresión son técnicas heurı́sticas que utilizan herramientas estadı́sticas para generar submodelos para explicar una variable de respuesta (ver ﬁgura 2.3).
Algunas de estas técnicas son:
Figura 2.3: Ejemplo de árbol de clasiﬁcación para generar tramos/categorı́as 2.3 [Fuente: https://www.stat.auckland.ac.nz/paul/RGraphics/chapter1] • CHAID: es una técnica heurı́stica que permite generar segmentos en las variables de análisis.
La técnica CHAID mezcla las técnicas de árbol AID con el test de Chi Cuadrado para fundir clases.
La técnica se basa en tres pasos: fusión, división y detención, donde la profundidad del árbol se obtiene repitiendo estos pasos de manera recursiva.
Los pasos se detallan a continuación:
- Fusión: el objetivo de este paso es fusionar todas aquellas categorı́as que no sean estadı́sticamente signiﬁcativas.
Cada categorı́a considerada como diferente pasa a ser un nodo de división para el árbol que se entregara.
- División: la “mejor” división para cada predictor es encontrada en la etapa de fusión.
La etapa de división sirve para seleccionar el mejor predictor.
La selección se basa en la comparación de los p-valores obtenidos en última etapa de fusión.
- Detención: este paso de detención veriﬁca si el proceso de crecimiento del árbol deberı́a parar de acuerdo a una regla de detención preestablecida.
• CRT (Classiﬁcation and Regresion Tree): los arboles de clasiﬁcación y regresión son técnicas que, al igual que las técnicas CHAID, buscan generar segmentos en las variables explicativas para describir a una variable de respuesta.
La gran diferencia entre ambas es el algoritmo que se utiliza para la construcción de estos.
La diferencia es que CHAID tiene tres etapas y CRT, en cambio, utiliza una división recursiva para seleccionar la mejor partición sobre las variables explicativas, la cual permite explicar a la variable de respuesta.
Cuadro 2.1: Ejemplo de discretización de una variable continua Cuadro 2.2: Ejemplo de discretización de una variable categórica
 | Categorı́a | Buenos Indeterminados | Malos Total
 | --- | --- | ---
 | (-inf, -99.0] | 211 | 243 | 1113 | 1567
 | --- | --- | --- | --- | ---
 | (-99.0, 10.7] | 645 | 378 | 2865 | 3888
 | (10.7, 365.5] | 95 | 32 | 248 | 375
 | (365.5, +inf) | 46 | 16 | 79 | 141
 | Total | 997 | 669 | 4305 | 5971

 | Categorı́a | Buenos Indeterminados | Malos Total
 | --- | --- | ---
 | Media | 332 | 152 | 1870 | 2354
 | --- | --- | --- | --- | ---
 | Técnica | 553 | 436 | 2186 | 3175
 | Universitaria | 112 | 81 | 249 | 442
 | Total | 997 | 669 | 4305 | 5971

Análisis Bivariado
El análisis bivariado es una actividad donde se calculan varios indicadores estadı́sticos en los tramos/categorı́as para analizar el poder predictivo de cada variable.
Los indicadores son los siguientes:
• Recuento de registros por tramo y desempeño por categorı́a.
• Porcentaje de registros por tramo y desempeño por categorı́a.
• Porcentaje acumulado de registros por tramo y desempeño por categorı́a.
• Tasas de Malos (Bad Rate)
• Odds Ratio por tramo
• Weight of Evidence (WoE)
• Information Value (IV)
• Diferencias absolutas
• KS
A continuacı́on, se deﬁnen los más importantes para el análisis bivariado.
• Bad Rate: determina la tasa de clientes malos dentro de una categorı́a, calculada entre el total de fracasos sobre los fracasos más los éxitos.
En una correcta segmentación el orden de los Bad Rates, deben presentar una tendencia lineal.
Se calcula de la siguiente forma:
 | BadRate = | Fracasos | 
 | --- | --- | ---
 | Fracasos − | Exitos | (2.1)

• Weights of Evidence (WoE): proporciona herramientas ﬂexibles para recodiﬁcar los valores en las variables predictoras continuas y categóricas en categorı́as discretas de forma automática, y asignar a cada una un valor WoE único.
Esta recodiﬁcación se lleva a cabo de manera que produzca las mayores diferencias entre los grupos recodiﬁcados con respecto a los valores de WoE a través de algoritmos de clasiﬁcación óptimo.
Se obtiene de la siguiente forma:
 |  | WoE = ln( | CNE CE) (2.2)
 | --- | --- | ---
 | donde:
 | CNE: es la proporción de fracaso en la categorı́a (casos no exitosos).
 | CE: es la proporción de éxito en la categorı́a (casos exitosos).
 | • Information Value (IV): es un indicador para determinar el grado de vinculación entre una variable y el éxito de respuesta. El valor calculado de este estadı́stico entrega una medida de fuerza de asociación entre la variable y la respuesta [1]. Está basado en la medida de divergencia de Kullback, la cual es utilizada en teorı́a de probabilidad y teorı́a de la información. En el contexto de desarrollar un modelo, es natural que algunas variables sean menos importantes que otras frente a una variable de respuesta, por lo que el IV permite comparar la potencia de dos o más variables. Se calcula de la siguiente manera:
 | IV = ∑
 |  | i (CNEi −CEi) ∗WoEi | (2.3)

Estos tres indicadores son expuestos en una tabla bivariada, la cual muestra la información para cada variable de forma ordenada y apropiada para que los analistas veriﬁquen que la variable esta apta para continuar en el proceso.
En el caso de que la variable no cumpla con ciertas cualidades, ésta debe ser retramiﬁcada hasta que cumpla todos los requisitos; en caso contrario, debe ser ﬁltrada.
Los requisitos de una buena variable están asociados a que esta sea monotónica para la regresión logı́stica; para esto, cada tramo de la variable debe tener un Bad Rate y un WoE que sean monotonicamente crecientes o decrecientes.
Cuando una variable no es de esta forma, se puede corregir mediante una retramiﬁcación, es decir, la unión entre dos tramos vecinos.
El análisis bivariado es una actividad iterativa, ya que es frecuente que los analistas deban iterar varias veces para lograr que todas las variables se comporten de forma monotónica."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.5.2. Selección de Variables,"Para seleccionar variables se aplican ciertos ﬁltros iniciales antes de aplicar un algoritmo de selección; los ﬁltros son los siguientes:
• Análisis de la concentración de datos dentro de las categorı́as: existe un estándar asociado a la proporción mı́nima de casos por tramo, el cual establece un ﬁltro a las variables que presenten menos de un 1 % de los casos agrupados en una misma categorı́a.
Este aspecto por lo general se conﬁgura en los software estadı́sticos, lo cual impide una categorización con estas caracterı́sticas para las variables continuas; sin embargo, puede existir la posibilidad de que se genere este caso para las variables categóricas.
• Análisis del Poder predictivo de la Variable: existe un estándar de modelamiento, el cual establece un valor mı́nimo de 2 % de IV para que la variable sea considerada predictiva.
• Análisis de la Estabilidad de la Variable: tiene por objetivo validar que los tramos deﬁnidos en el Análisis Bivariado se mantienen estables en la población.
Para esto, se calcula el Índice de Estabilidad Poblacional (IEP) para cada variable.
• Filtro por ı́ndice de Estabilidad Poblacional (IEP): es un indicador que mide la similitud entre dos poblaciones distintas.
Su uso permite saber si la categorización óptima aplicada a una variable se mantiene estable en un instante de tiempo determinado.
La fórmula que determina este indicador es el mismo que el IV, donde la principal diferencia entre estos dos indicadores es la interpretación que se le da a cada uno.
Se puede obtener de la siguiente manera:
IEP = ∑ (PEi − PRi) ∗ ln(PEi PRi ) (2.4) donde: PE: es la proporción esperada de la categorı́a.
PR: es la proporción real de la categorı́a.
La forma de interpretar este indicador se puede observar del cuadro 2.3.
Cuando el IEP es mayor a 0,25, la variable debe ser ﬁltrada debido a que no representa a la población.
Para el caso de un IEP de [0,1; 0,25] debe ser decidido por criterio experto del analista para ﬁltrar o no.
Cuadro 2.3: Signiﬁcado del valor de IEP
 | IEP | Población |  | 
 | --- | --- | --- | ---
 | <0.1 Sin cambios en la población
 | 0.1 a 0.25 Cambio menor en la población

>0.25 La población cambió
Para garantizar la aplicabilidad de los tramos, una vez puesto en producción el modelo, se debe maximizar la cantidad de periodos donde se realiza el análisis de estabilidad.
Para esto se utilizan en conjunto las bases de desarrollo y desempeño, además de toda la información nueva obtenida para periodos posteriores a los considerados en la muestra.
Esto permite analizar la estabilidad por periodo de cada variable, y estudiar posibles comportamientos anómalos en base al estándar deﬁnido.
Luego de los ﬁltros iniciales, se pueden seguir aplicando técnicas de selección de variables.
Algunas de estas son las siguientes:
• Selección de Variables por Componentes Principales: una forma de agrupar variables en base a un criterio estadı́stico, es mediante la aplicación de un Análisis de Componentes Principales (PCA) en el cual se asocian variables a diferentes factores y se interpretan como familias de variables.
Un análisis de componentes principales tiene sentido si existen altas correlaciones entre las variables, ya que esto es indicativo de que hay información redundante y, por lo tanto, pocos factores explicarán gran parte de la variabilidad total.
La elección de los factores se realiza de tal forma que el primero recoja la mayor proporción posible de la variabilidad original, luego el segundo factor debe recoger la máxima variabilidad posible no recogida por el primero, y ası́ sucesivamente.
Del total de factores se elegirán aquellos que recojan el porcentaje de variabilidad que se considere suﬁciente.
A éstos se les denominará componentes principales.
• Selección por Correlación: se llama correlación al grado de dependencia mutua entre dos variables; por su parte, el coeﬁciente de correlación intenta medir la intensidad con que dos variables están relacionadas.
Este concepto está directamente relacionado con el concepto de curva de regresión.
Mediante la regresión, se expresa la estructura funcional de la relación existente entre las variables, ajustando a la nube de puntos dada por los pares de valores de las dos variables a una curva de la mejor forma posible.
El ajuste será de la forma Y
=
f (X)+e o
X
= f (XY)+e, donde e es el error.
El coeﬁciente de correlación mide la calidad de este ajuste.
Cuando la curva es recta, la regresión es lineal; en este caso el coeﬁciente de correlación se llamará coeﬁciente de correlación lineal y mide el grado de asociación lineal entre las variables.
Algunos coeﬁcientes de correlación útiles son los siguientes:
• Correlación de Pearson: este coeﬁciente, pensado para variables cuantitativas (escala mı́nima de intervalo), es un ı́ndice que mide el grado de covariación entre distintas variables relacionadas linealmente, que poseen una distribución normal bivariada conjunta.
Esto signiﬁca que puede haber variables fuertemente relacionadas, pero no de forma lineal, en cuyo caso se sugiere no proceder a aplicarse la correlación de Pearson.
• Correlación Rho de Spearman: es la versión no paramétrica del coeﬁciente de correlación de Pearson, que se basa en los rangos de los datos en lugar de hacerlo en los valores reales.
Resulta apropiado para datos ordinales o los de intervalo que no satisfagan el supuesto de normalidad.
La ventaja de Spearman es que, al ser una técnica no paramétrica, es de libre distribución probabilı́stica.
Además, los supuestos son menos estrictos y es robusto a la presencia de outliers (es decir, permite ciertos desvı́os del patrón normal).
La manifestación de una relación causa-efecto es posible sólo a través de la comprensión de la relación natural que existe entre las variables y no debe manifestarse solamente por la existencia de una fuerte correlación.
• Correlación Tau-b de Kendall: es una medida no paramétrica de asociación para variables ordinales o de rangos que tiene en consideración los empates.
El signo del coeﬁciente indica la dirección de relación y su valor absoluto indica la magnitud de la misma, de tal modo que los mayores valores absolutos indican relaciones más fuertes.
Los valores posibles van de -1 a 1, pero un valor de -1 o
+1 sólo se puede obtener a partir de tablas cuadradas.
Cuando dos o más variables explicativas de un modelo están altamente correlacionadas en la muestra, es muy difı́cil separar el efecto parcial de cada una de estas variables sobre la variable dependiente.
La información muestral que incorpora una de estas variables es casi la misma que el resto de las correlacionadas con ella.
En el caso extremo de multicolinealidad exacta, no es posible estimar separadamente estos efectos sino una combinación lineal de ellos.
La selección de variables por correlación, es aplicada a las variables con el ﬁn de quitar los atributos redundantes, que pueden estar repitiendo la misma información a los modelos.
Existe un estándar asociado a la interpretación de los coeﬁcientes de correlación de Pearson, Spearman y Tau-b de Kendall, los cuales permiten establecer, si el conjunto de variables a utilizar en el modelo presenta problemas de alta correlación.
Para las variables cuantitativas simétricas, normalmente distribuidas, se utiliza el coeﬁciente de correlación de Pearson; por su parte, si los datos no están normalmente distribuidos o tienen categorı́as ordenadas, se utilizan la Tau-b de Kendall o de Spearman, las cuales miden la asociación entre órdenes de los rangos.
Los coeﬁcientes pueden tomar valores entre -1 y +1, lo cual se interpreta como una relación negativa y positiva perfecta, respectivamente.
Existe un estándar establecido para la inclusión de una variable en un modelo, el cual establece un valor máximo de 0.5 en el ı́ndice para una correlación positiva, y un estándar mı́nimo de -0.5 para el caso de una correlación negativa, valores que se encuentran deﬁnidos en el cuadro 2.4.
Cuadro 2.4: Signiﬁcado del valor del coeﬁciente de correlación
 |  | Correlación
 | --- | ---
 | Valor Rango | Fuerza y Dirección | r = +1 | Perfecta
 | --- | --- | --- | ---
 | r = 0 | Sin Correlación
 | --- | ---
 | -0.5 -0.9 <= r -1 <r <-0.9 r = | <= r <-0.5 -1 | <0 | Débil Moderada Fuerte Perfecta | Negativa
 | --- | --- | --- | --- | ---
 | 0.9 <r <1 Fuerte 0.5 <r <= 0.9 Moderada 0 <r <= 0.5 Débil Positiva

El estándar establece como requisito que las variables candidatas sólo puedan poseer una correlación débil.
Es posible que, para casos de borde y en base a las caracterı́sticas particulares de cada variable, los analistas y el negocio decidan mantener una variable a pesar que posee una correlación deﬁnida como Moderada.
Sin embargo, la regla general es que éstas deben ser eliminadas del conjunto de variables a evaluar en el modelo.
Este selección busca eliminar problemas de correlación, asignando prioridad a las variables con mejor poder predictivo, de forma que se realice una selección inteligente, que elimine problemas por correlación, manteniendo las mejores variables candidatas.
El proceso puede ser realizado tanto sobre las variables brutas, es decir en su forma original, como con los tramos deﬁnidos en el Análisis Bivariado.
El criterio de decisión sobre el tipo de variables aplicadas, depende de la cantidad de variables candidatas a evaluar en el modelo, ya que en caso de que no se ﬁltren muchas variables, y en consecuencia queden muchas a evaluar, se realizan dos pasos: donde en primer lugar se eliminan las correlacionadas por variables brutas, y luego se aplica la selección sobre las variables tramiﬁcadas."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.6. Descubrimiento de conocimiento de clientes bancarios a través de Modelos,"Actualmente en Chile, las instituciones bancarias buscan averiguar el riesgo asociado a sus clientes en torno a los productos que ofrecen (créditos) para estimar un perﬁl de deudor para una cartera de clientes.
Con esta información se clasiﬁca al cliente en un perﬁl y se estima la pérdida esperada en conjunto a varios clientes.
Para lograr esta predicción, se desarrollan modelos de provisiones para calcular ciertos indicadores de pérdida, los cuales están normados [8] por la SBIF2 con reglas asociadas al tipo de modelo de provisión que se construirá con analistas de desarrollo de modelos en la Institución Bancaria.
El objetivo de que estos modelos estén regulados por la SBIF, es para que cumplan ciertos estándares básicos del mundo ﬁnanciero y no se salgan de una predicción “lógica” para el negocio especı́ﬁco que impactará él modelo.
Por otra parte, el gran aporte de un modelo al negocio es la estimación de valores para el futuro, con el ﬁn de mejorar la gestión realizada en la toma de decisiones, y ası́, no subestimar o sobreestimar el gasto asociado a los riesgos de pérdida; mientras mejor sea la estimación, menor será el riesgo asociado al negocio y, por ende, habrá menos pérdidas monetarias.
Los modelos se pueden aplican en distintas partes del negocio dentro de una institución bancaria, ya sea para otorgar un crédito a un cliente, ver el comportamiento de pago de un cliente o la forma de llegar a cobrarle a un cliente, entre otros.
2SBIF: Superintendencia de Bancos e Instituciones Financieras, es el organismo encargado de supervisar a las empresas bancarias, ası́ como de otras entidades, en resguardo de los depositantes u otros acreedores y del interés público y su misión es velar por el buen funcionamiento del sistema ﬁnanciero [Fuente: sbif.cl]."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.6.1. Modelos de Scoring y la predicción del riesgo,"Modelos de Scoring y la predicción del riesgo
Una institución bancaria, al tener modelos para predecir el riesgo asociado a sus clientes, puede tomar decisiones inteligentes y certeras para disminuir su riesgo al mejorar su gestión para reducir la cartera de los distintos deudores.
Para esto se desarrollan ciertos modelos, como los de Scoring, los cuales reciben como entrada información relevante del cliente y como salida entregan un puntaje el cual se utiliza para clasiﬁcar a un cliente en una de las categorı́as de deudor establecida por la SBIF (Figura 2.4).
Conociendo esta categorı́a asociada al cliente, se obtienen indicadores para estimar la perdida esperada por él cliente.
Figura 2.4: Indicadores para los perﬁles de deudores en 2 tipos de carteras
Gracias a la utilización de los modelos de Scoring dentro de las gestiones de la institución bancaria, se logra constituir oportunamente las provisiones necesarias y suﬁcientes para cubrir las pérdidas esperadas asociadas a las caracterı́sticas de los deudores y sus créditos, que determinan el comportamiento de pago y la posterior recuperación de las perdidas.
Para desarrollar un modelo de Scoring, las instituciones utilizan sus propias metodologı́as de minerı́a de datos para lograr extraer el conocimiento de las bases de datos de sus clientes.
Dependiendo de la efectividad de la metodologı́a utilizada por los analistas, se logra entrenar un modelo que tiene asociado un poder predictivo, el cual es el indicador más relevante para decir con precisión si el modelo logrará predecir bien o no. Para llegar a calcular este indicador se utilizan varios métodos estadı́sticos, los cuales se aplican para validar el modelo."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,2.6.2. Benchmarking de técnicas de modelamiento en Credit Scoring,"En la comunidad cientı́ﬁca, existen varios investigadores que se han dedicado a comparar el rendimiento y la capacidad de predicción de varias técnicas de modelamiento.
La técnica que siempre se compara con el resto es la regresión logı́stica, ya que esta técnica es la más utilizada en la industria para desarrollar modelos de Scoring para clasiﬁcar clientes por riesgo.
Stefan Lessmann, Bart Besens, Hsin-VonnSeow y Lyn C. Thomas [6], a través de un largo trabajo de varios años, recopilaron los resultados de varios autores expuestos en el siglo XXI en artı́culos cientı́ﬁcos (Figura 2.5).
Estos resultados exponen la comparación entre 41 técnicas de modelamiento, expuestas en la ﬁgura 2.6.
Estas 41 técnicas se agrupan por tres tipos; el primero corresponde a 16 clasiﬁcadores individuales, el segundo a 8 técnicas de ensamblaje homogéneas, y el ultimo tipo a 17 técnicas de ensamblaje heterogéneas.
En base a las métricas de desempeño de AUC, PCC, BS, H, PG y KS, se observa que las mejores técnicas son las que mantienen estos indicadores al mı́nimo.
Dado estos resultados, los algoritmos con mejor desempeño son los de ensamblaje heterogéneo directamente estático, siendo el mejor Hill-climbing Ensemble Selection with Bootstrap Sampling (HCESBag).
Finalmente, los autores escogen las técnicas de Multilayer Perceptron Artiﬁcial Neural Network (ANN), LogisticRegression (LR), Random Forest (RF) y HCES-Bag para realizar una comparación completa respecto a estas buenas técnicas para credit scoring.
El resultado se expone en la ﬁgura 2.7.
Como puede observarse en ésta, la regresión logı́stica es superada por todas estas técnicas en desempeño, evidenciando que las nuevas técnicas de este siglo han logrado superar a la regresión logistica para desarrollar un modelo de scoring.
Lamentablemente las técnicas más nuevas como HCES-Bag y RF no se encuentran en los software de minerı́a de datos debido a lo reciente y complicado de estandarizar estas técnicas computacionalmente.
2.5: Técnicas de modelamiento utilizadas para el estudio de benchmarking
2.5 Fuente: [Stefan Lessmanna, H Seowb, Bart Baesenscd, and Lyn C Thomas D. Benchmarkingstate-of-the-art classiﬁcation algorithms for credit scoring: A ten-year update.
In Credit Research Centre, Conference Archive, 2013]
2.6: Resultados técnicas utilizadas para el estudio de benchmarking
2.6 Fuente: [Stefan Lessmanna, H Seowb, Bart Baesenscd, and Lyn C Thomas D. Benchmarkingstate-of-the-art classiﬁcation algorithms for credit scoring: A ten-year update.
In Credit Research Centre, Conference Archive, 2013]
2.7: Comparación entre las 4 mejores técnicas del estudio de benchmarking
2.7 Fuente: [Stefan Lessmanna, H Seowb, Bart Baesenscd, and Lyn C Thomas D. Benchmarkingstate-of-the-art classiﬁcation algorithms for credit scoring: A ten-year update.
In Credit Research Centre, Conference Archive, 2013]"
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3. Propuesta de Solución,"En este capı́tulo se aborda como será el diseño de la solución.
En primera instancia se habla sobre una metodologı́a adaptada de CRISP-DM, luego las etapas asociadas a la metodologia, las métricas de evaluación que se utilizarán en cada etapa, y ﬁnalmente las técnicas de modelamiento que se utilizarán junto con sus parámetros."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.1. CRISP-DM con enfoque inverso,"La metodologı́a actual basada en CRISP-DM de la institución bancaria está compuesta por las siguientes etapas:
• Entendimiento del negocio
• Calidad de Datos
• Muestreo
• Transformación de Variables
• Selección de Variables
• Modelamiento y Validación
• Puesta en producción
• Integración a la Gestión
La diferencia entre la metodologı́a de la Institución bancaria y CRISP-DM está en las etapas de Muestreo, Transformación de Variables y Selección de Variables, ya que estas fueron adaptadas en base a buenas prácticas que fueron descubriendo con los años para desarrollar sus modelos y con innovaciones de sus analistas; por lo tanto, se volvieron a la medida para su contexto de modelos de credit scoring.
El enfoque de esta investigación es rediseñar, implementar y evaluar los procesos de Transformación de Variables, Selección de Variables y Modelamiento.
La investigación aborda las etapas en el orden inverso de su ejecución con el ﬁn de basar los resultados en la evaluación del modelo a medida que se avanza con las implementaciones.
Figura 3.1: Orden de ejecución natural de los procesos según la metodologı́a actual
Es necesario mejorar estas tres etapas para generar modelos con mejor predicción para los negocios de la institución, ya que en ellas se generan y escojen las variables que van a explicar el resultado de un modelo; por ende, si el modelo cambia (en técnica), se vuelve vital que las variables sean las mejores para este modelo y además, vengan en una forma que potencie el descubrimiento de conocimiento."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.2. Etapas de implementación de la solución,"Para lograr los objetivos de esta investigación en las etapas de Transformación de Variables, Selección de Variables y Modelamiento, se implementarán secuencialmente en las siguientes 3 etapas:
• Etapa 1: modiﬁcar la técnica de Modelamiento y evaluar con distintas métricas.
• Etapa 2: modiﬁcar el proceso de Selección de Variables + Modelamiento y evaluar con distintas métricas.
• Etapa 3: modiﬁcar el proceso de Transformación de Variables + Selección de Variables + Modelamiento y evaluar con distintas métricas.
En la ﬁgura 3.2 se puede observar que la etapa 1 esta encerrada por el color verde (solo aborda Modelamiento), la etapa 2 por el color azul (solo aborda Modelamiento y Selección de Variables) y la etapa 3 por el color rojo (aborda las 3 etapas juntas).
Figura 3.2: Etapas desarrolladas en la investigación
Estas etapas siguen un enfoque en reversa para abordar el problema, es decir, desde el ﬁnal hacia el comienzo, con el ﬁn de abordar las tres etapas de forma progresiva para comparar en cada etapa lo siguiente:
• Etapa 1: comparar la técnica de Regresión Logı́stica vs nuevas técnicas, y observar las ganancias.
• Etapa 2: comparar los resultados de la etapa vs etapa 1 al implementar nuevas técnicas de selección de variables y observar las ganancias.
• Etapa 3: comparar los resultados de la etapa con resultados anteriores al modiﬁcar la transformación de variables existente actualmente."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.3. Métricas de evaluación para las tres etapas,"Para los resultados de la implementación, se evalúa el desempeño de los modelos utilizando métricas tradicionales como Accuracy, AUC Score, KS, Precision, Recall y F1-Score.
Cabe destacar que las tres primeras se evalúan para dos instancias; la primera es para el conjunto de datos que se utilizan para entrenar el modelo (datos de entrenamiento) y la segunda para el conjunto restante (datos de prueba).
Todos los indicadores que se utilizan tienen una escala porcentual, por lo que su dominio es [0,1], donde 0 equivale a
0
% y 1 a 100 % respectivamente."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.3.1. Accuracy,"El accuracy se utiliza para evaluar dos instancias, los datos de entrenamiento y de prueba.
Mediante este indicador se puede saber qué tanto acierta o se equivoca el modelo en ambos datasets; además, al tener ambos indicadores midiendo lo mismo, se puede saber si el modelo tiene sobreajuste o no, viendo que tanta diferencia hay entre los dos valores.
 | Accuracy = | VP + | VN
 | --- | --- | ---
 | VP + | VN + | FP + | FN = | Prediciones correctas Todas las predicciones | (3.1)
 | --- | --- | --- | --- | --- | ---

donde:
• VP: Verdaderos positivos
• VN: Verdaderos negativos
• FP: Falsos positivos
• FN: Falsos negativos
Esta métrica se incluye debido a que es la métrica estándar para ver si un modelo está sobreajustado o no."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.3.2. AUC Score,"Es un indicador que mide la razón de los verdaderos positivos y falsos positivos de un modelo; este valor indica el área bajo la curva y puede estar entre [0.5,1.0], donde 0.5 indica que el modelo no aporta nada y el 1.0 que el modelo discrimina sin errores entre clientes”Buenos” y”Malos”.
Este indicador puede aplicarse para los datos de entrenamiento y de prueba; además, también mide sobreajuste al comparar los 2 valores.
Figura 3.3: Ejemplos de curva ROC"
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.3.3. KS,"El indicador KS (Kolmogorov-Smirnov) mide si el modelo tiene buena capacidad de discriminación para la variable dependiente (el target).
Para el caso de credit scoring evalúa si los clientes “Buenos” y “Malos” se acumulan en los extremos opuestos del puntaje [10].
KS corresponde a la distancia máxima que existe entre la distribución acumulada de clientes “Buenos” y “Malos”.
Al igual que el accuracy y el AUC Score, este indicador puede aplicarse en los 2 datasets y medir sobreajuste.
Figura 3.4: Ejemplo de medición de KS
Esta métrica sera la principal a analizar en los trabajos a realizar, debido a que es la más importante para el área de desarrollo de modelos de la institución bancaria para decidir si el modelo está siendo mejor que una versión antigua o similar, por lo que permite comparar fácilmente modelos entrenados con las mismas variables."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,"3.3.4. Precision, Recall y F1-Score","La precision mide la proporción de todas las predicciones positivas, con el ﬁn de medir cuántas predicciones positivas son observaciones positivas.
El recall mide la proporción de los verdaderos positivos y el F1-Score corresponde a una media armónica entre la precision y el recall.
Las ecuaciones para calcular estos indicadores son las siguientes:
 | Precision = | T P
 | --- | ---
 | T P + | FP = | Predicciones correctas positivas Todas las predicciones positivas | (3.2)
 | --- | --- | --- | ---
 | Recall = | T P |  | 
 | T P + | FN = | Predicciones correctas positivas Todas las observaciones positivas | (3.3)
 |  | S core = 2 ∗ | Precision ∗ Recall Precision + Recall | (3.4)

Estas tres métricas son importantes de monitorear en los resultados debido a que provienen de la matriz de confusión (al igual que el accuracy) y permiten descartar modelos mal entrenados rápidamente si las tres métricas entregan una mala evaluación en sus indicadores."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,3.4. Parámetros de las técnicas de modelamiento,"Parámetros de las técnicas de modelamiento
Para el desarrollo de la solución en las etapas, se utilizan las siguientes técnicas de modelamiento para comparar con la Logistic Regression.
• Random Forest (ensamblaje)
• Extra Tree (ensamblaje)
• Bagging (ensamblaje)
• Gradient Boosting (ensamblaje)
• AdaBoost (ensamblaje)
• Decision Tree
• Bernoulli Naive Bayes
• Gaussian Naive Bayes
El motivo de usar estas ocho técnicas se basa en una preselección realizada junto a los analistas, ya que ellos están interesados principalmente por las técnicas de ensamblaje, ya que la literatura de los últimos años ha demostrado beneﬁcios en este tipo de técnicas y esto se complementa con lo encontrado en el estado del arte.
Para los arboles de decisión, es una técnica de modelado que siguen utilizando y aconsejaron tenerla en cuenta.
Por ultimo, para Naive Bayes recomendaron mantenerla porque respeta varios supuestos de la regresión logı́stica.
Los parámetros utilizados en todos experimentos son:
• Logistic Regression:
• Penalty:”l2”; se usa para especiﬁcar la norma utilizada en la penalización.
• Dual: False; formulación dual o primaria.
• Tol: 0.0001; tolerancia para detener los criterios.
• C: 1.0; inverso de la fuerza de regularización.
• Solver: liblinear; algoritmo para usar en el problema de optimización.
• Random state: 1
• Random Forest:
• N estimators: 100; la cantidad de árboles en el bosque.
• Criterion:”gini”; la función para medir la calidad de una división.
• Min samples split: 2; el número mı́nimo de muestras requeridas para dividir un nodo interno.
• Min samples leaf : 1; el número mı́nimo de muestras requeridas para estar en un nodo hoja.
• Random state: 1
• Extra Tree:
• N estimators: 100; la cantidad de árboles en el bosque.
• Criterion:”gini”; la función para medir la calidad de una división.
• Min samples split: 2; el número mı́nimo de muestras requeridas para dividir un nodo interno.
• Min samples leaf : 1; el número mı́nimo de muestras requeridas para estar en un nodo hoja.
Random state: 1
• Bagging:
• Base estimator: DecisionTreeClassiﬁer; el estimador base para encajar en subconjuntos aleatorios del conjunto de datos.
• N estimators: 100; el número de estimadores base en el conjunto.
• Max samples: 1.0; el número de muestras para extraer de X para entrenar a cada estimador base.
• Max features: 1.0; la cantidad de caracterı́sticas que se deben extraer de X para entrenar a cada estimador base.
• Random state: 1
• Gradient Boosting:
• Loss:”deviance”; función de pérdida para ser optimizado.
• Learning rate: 0.1; la tasa de aprendizaje reduce la contribución de cada árbol según el ı́ndice de aprendizaje.
• N estimators: 100; el número de etapas de refuerzo para realizar.
El aumento de gradiente es bastante robusto para un ajuste excesivo, por lo que un número grande generalmente da como resultado un mejor rendimiento.
• Max depth: 3; profundidad máxima de los estimadores de regresión individuales.
• Criterion: “friendman mse”; la función para medir la calidad de una división.
• Random state: 1
• AdaBoost:
• Base estimator: DecisionTreeClassiﬁer; el estimador base a partir del cual se construye el conjunto potenciado.
• N estimators: 100; el número máximo de estimadores en los que se ﬁnaliza la potenciación.
• Learning rate: 0.1; la tasa de aprendizaje reduce la contribución de cada árbol según el ı́ndice de aprendizaje.
Algorithm: SAMME.R; algoritmo real de impulso.
• Random state: 1
• Decision Tree:
• Criterion: “gini”; la función para medir la calidad de una división.
• Splitter: “best”; la estrategia utilizada para elegir la división en cada nodo.
• Min samples split: 2; el número mı́nimo de muestras requeridas para dividir un nodo interno.
• Min samples leaf : 1; el número mı́nimo de muestras requeridas para estar en un nodo hoja.
• Random state: 1
• Bernoulli Naive Bayes:
• Alpha: 1.0; parámetro de suavizado de aditivos (Laplace / Lidstone).
• Binarize: 0; umbral para binarización (mapeo a booleanos) de caracterı́sticas de muestra.
• Gaussian Naive Bayes:
• Priors: None; probabilidades previas de las clases.
Cabe destacar que estos parámetros de entrada fueron probados antes de comenzar los experimentos, luego de ser considerados óptimos para los experimentos de esta investigación.
Respecto al Random state (presente en las técnicas de ensamblaje, Logistic Regression y Decision Tree), este parámetro se utiliza como semilla para poder replicar los experimentos, es decir, se controla la aleatoriedad del algoritmo.
El N estimators (presente en las técnicas de ensamblaje) fue decidido junto a los analistas para que las técnicas de ensamblaje tengan un numero grande de estimadores.
En los demás parámetros se toma la decisión en conjunto con los analistas de conﬁar en los predeterminados por la API de Scikit-learn [9]; dado que el objetivo de la investigación no busca centrar los esfuerzos en mejorar los hiper parámetros."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,4. Implementación y Validación,"En este capitulo se abordan las tres etapas desarrolladas para encontrar beneﬁcios con las implementaciones realizadas para los procesos de Modelamiento, Selección de Variables y Transformación de Variables.
El ﬁn de separar por las etapas es encontrar individualmente los beneﬁcios para cada proceso, para ﬁnalmente proponer variantes para cada una de ellas con herramientas que generen una ventaja en el desarrollo de modelos de scoring para los analistas.
Para comenzar este capitulo se detallan, a grandes rasgos, los datasets a usar; luego se presentan las evaluaciones con las métricas para cada etapa y ﬁnalmente se resumirá los beneﬁcios encontrados en conjunto."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,4.1. Descripción de los Datasets entregados por la Institución Bancaria,"La institución, para aportar al logro de los objetivos del experimento, proporciona datos anonimizados en dos versiones; la primera consiste en dataset con todas las variables obtenidas de un proyecto real luego de la transformación de variables realizada.
La segunda versión corresponde a las variables seleccionadas para Logistic Regression en el mismo proyecto.
El propósito de esto último es utilizar la versión 2 para la etapa 1, con el ﬁn de emplear una selección de variables previa antes de entrenar el modelo (la selección viene realizada por los analistas con su metodologı́a actual).
La versión 1 se utiliza en la etapa 2 con el motivo de aplicar una técnica de selección de variables implementada con todas las variables.
El detalle de los datasets se observan en el cuadro 4.1.
Cuadro 4.1: Información datasets utilizados en sus dos versiones
 | Dataset | Número de variables
 | --- | ---
 | Número Volumen | Versión 1 | Versión 2
 | --- | --- | ---
 | 1 118.307 | 233 | 5
 | 2 54.773 | 206 | 9
 | 3 31.205 | 225 | 9
 | 4 29.244 | 189 | 11
 | 5 23.192 | 159 | 9
 | 6 65.539 | 117 | 14"
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,4.2. Etapa 1: Modelamiento,"La técnica de modelamiento usada actualmente es la Regresión Logı́stica, la cual se utiliza punto de referencia para comparar con otras técnicas.
Al existir una gran variedad de técnicas de modelamiento, se decide escoger 8 técnicas para comparar con la regresión logı́stica, de las cuales 5 son de ensamblaje, 1 de árbol de decisión y 2 de Naive Bayes.
Las técnicas utilizadas para esta etapa son las siguientes:
• Logistic Regression
• Random Forest (ensamblaje)
• Extra Tree (ensamblaje)
• Bagging (ensamblaje)
• Gradient Boosting (ensamblaje)
• AdaBoost (ensamblaje)
• Decision Tree
• Bernoulli Naive Bayes
• Gaussian Naive Bayes
El experimento consiste en entrenar un modelo mediante regresión logı́stica para tenerlo como punto de referencia en esta etapa; luego, se entrenan las 8 técnicas restantes con el ﬁn de competir con la regresión.
Para comparar se utilizan las métricas deﬁnidas luego de entrenar cada modelo; los resultados para cada dataset (versión 2) se encuentran en los cuadros 4.2 a 4.7.
Cuadro 4.4: Evaluación de técnicas de modelamiento en el Dataset 3 (9 variables)
Cuadro 4.2: Evaluación de técnicas de modelamiento en el Dataset 1 (5 variables) Techniques / Metrics Train Acc Test Acc Train AUC Test AUC Train KS Test KS Precision Recall F1-score
 | Logistic Regression | 0,8658 | 0,8648 | 0,8646 | 0,8632 | 0,597 | 0,594 | 0,85 | 0,86 | 0,85
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,8667 | 0,8654 | 0,8734 | 0,8705 | 0,6097 | 0,6056 | 0,85 | 0,87 | 0,85
 | Extra Tree | 0,8667 | 0,8655 | 0,8734 | 0,8705 | 0,6097 | 0,6056 | 0,85 | 0,87 | 0,85
 | Bagging | 0,8667 | 0,8654 | 0,8734 | 0,8705 | 0,6097 | 0,6056 | 0,85 | 0,87 | 0,85
 | Gradient Boosting | 0,8661 | 0,8652 | 0,8696 | 0,8679 | 0,6026 | 0,5995 | 0,85 | 0,87 | 0,85
 | AdaBoost | 0,8655 | 0,8646 | 0,8648 | 0,8635 | 0,5978 | 0,5947 | 0,85 | 0,86 | 0,85
 | Decision Tree | 0,8667 | 0,8655 | 0,8734 | 0,8705 | 0,6097 | 0,6056 | 0,85 | 0,87 | 0,85
 | Bernoulli Naive Bayes | 0,8191 | 0,8173 | 0,821 | 0,8176 | 0,557 | 0,5513 | 0,83 | 0,82 | 0,82
 | Gaussian Naive Bayes | 0,815 | 0,8114 | 0,846 | 0,8427 | 0,5872 | 0,5824 | 0,84 | 0,81 | 0,82

Cuadro 4.3: Evaluación de técnicas de modelamiento en el Dataset 2 (9 variables) Techniques / Metrics Train Acc Test Acc Train AUC Test AUC Train KS Test KS Precision Recall F1-score
 | Logistic Regression | 0,907 | 0,9063 | 0,8271 | 0,8252 | 0,5189 | 0,5165 | 0,9 | 0,91 | 0,88
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,9262 | 0,9275 | 0,9231 | 0,9177 | 0,6628 | 0,6521 | 0,92 | 0,93 | 0,92
 | Extra Tree | 0,9263 | 0,9271 | 0,9233 | 0,9179 | 0,6632 | 0,6527 | 0,92 | 0,93 | 0,92
 | Bagging | 0,9262 | 0,9275 | 0,9231 | 0,9174 | 0,6627 | 0,6519 | 0,92 | 0,93 | 0,92
 | Gradient Boosting | 0,9114 | 0,9115 | 0,8521 | 0,8466 | 0,5493 | 0,5412 | 0,91 | 0,91 | 0,89
 | AdaBoost | 0,9075 | 0,908 | 0,8324 | 0,8301 | 0,5174 | 0,5134 | 0,9 | 0,91 | 0,89
 | Decision Tree | 0,9263 | 0,927 | 0,9233 | 0,9173 | 0,6632 | 0,6526 | 0,92 | 0,93 | 0,92
 | Bernoulli Naive Bayes | 0,884 | 0,8869 | 0,8092 | 0,8032 | 0,4869 | 0,4759 | 0,88 | 0,89 | 0,88
 | Gaussian Naive Bayes | 0,8594 | 0,8597 | 0,8061 | 0,8031 | 0,4963 | 0,4813 | 0,88 | 0,86 | 0,87

Techniques / Metrics Train Acc Test Acc Train AUC Test AUC Train KS Test KS Precision Recall F1-score
 | Logistic Regression | 0,8991 | 0,8982 | 0,9212 | 0,9147 | 0,7165 | 0,6979 | 0,89 | 0,9 | 0,89
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,9504 | 0,9425 | 0,9813 | 0,9721 | 0,8552 | 0,8192 | 0,94 | 0,94 | 0,94
 | Extra Tree | 0,9504 | 0,9433 | 0,9818 | 0,9698 | 0,8586 | 0,8193 | 0,94 | 0,94 | 0,94
 | Bagging | 0,9504 | 0,9418 | 0,9812 | 0,9716 | 0,8546 | 0,8187 | 0,94 | 0,94 | 0,94
 | Gradient Boosting | 0,9089 | 0,9042 | 0,936 | 0,9273 | 0,7295 | 0,7102 | 0,89 | 0,9 | 0,9
 | AdaBoost | 0,8989 | 0,8959 | 0,9213 | 0,9144 | 0,7082 | 0,6877 | 0,89 | 0,9 | 0,89
 | Decision Tree | 0,9504 | 0,9422 | 0,9818 | 0,9675 | 0,8586 | 0,8209 | 0,94 | 0,94 | 0,94
 | Bernoulli Naive Bayes | 0,8912 | 0,886 | 0,9075 | 0,8989 | 0,6627 | 0,6494 | 0,89 | 0,89 | 0,89
 | Gaussian Naive Bayes | 0,8798 | 0,8733 | 0,9097 | 0,9019 | 0,6854 | 0,6721 | 0,89 | 0,87 | 0,88

Cuadro 4.5: Evaluación de técnicas de modelamiento en el Dataset 4 (11 variables) Techniques / Metrics Train Acc Test Acc Train AUC Test AUC Train KS Test KS Precision Recall F1-score
 | Logistic Regression | 0,6882 | 0,6949 | 0,7289 | 0,7356 | 0,3396 | 0,3568 | 0,68 | 0,69 | 0,68
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,8531 | 0,8346 | 0,9346 | 0,9147 | 0,6794 | 0,6502 | 0,83 | 0,83 | 0,83
 | Extra Tree | 0,8532 | 0,8335 | 0,9355 | 0,9138 | 0,6798 | 0,6536 | 0,83 | 0,83 | 0,83
 | Bagging | 0,8531 | 0,8343 | 0,9345 | 0,9137 | 0,6793 | 0,6471 | 0,83 | 0,83 | 0,83
 | Gradient Boosting | 0,7122 | 0,712 | 0,7682 | 0,767 | 0,4011 | 0,4033 | 0,7 | 0,71 | 0,7
 | AdaBoost | 0,6956 | 0,6995 | 0,7363 | 0,7394 | 0,3453 | 0,3628 | 0,69 | 0,7 | 0,68
 | Decision Tree | 0,8532 | 0,8326 | 0,9355 | 0,9053 | 0,6798 | 0,6448 | 0,83 | 0,83 | 0,83
 | Bernoulli Naive Bayes | 0,6694 | 0,6757 | 0,7139 | 0,7222 | 0,3111 | 0,3276 | 0,68 | 0,68 | 0,68
 | Gaussian Naive Bayes | 0,6594 | 0,6665 | 0,7145 | 0,7184 | 0,3265 | 0,3389 | 0,69 | 0,67 | 0,67

Cuadro 4.6: Evaluación de técnicas de modelamiento en el Dataset 5 (9 variables) Techniques / Metrics Train Acc Test Acc Train AUC Test AUC Train KS Test KS Precision Recall F1-score
 | Logistic Regression | 0,7238 | 0,7314 | 0,7334 | 0,7446 | 0,3458 | 0,3575 | 0,72 | 0,73 | 0,71
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,7843 | 0,7876 | 0,844 | 0,8429 | 0,4998 | 0,5039 | 0,79 | 0,79 | 0,77
 | Extra Tree | 0,7843 | 0,7872 | 0,8441 | 0,8432 | 0,4998 | 0,503 | 0,79 | 0,79 | 0,77
 | Bagging | 0,7843 | 0,7876 | 0,844 | 0,8428 | 0,4998 | 0,503 | 0,79 | 0,79 | 0,77
 | Gradient Boosting | 0,7462 | 0,7521 | 0,7765 | 0,7834 | 0,4291 | 0,4365 | 0,75 | 0,75 | 0,73
 | AdaBoost | 0,7218 | 0,7288 | 0,7375 | 0,7465 | 0,3558 | 0,3684 | 0,72 | 0,73 | 0,71
 | Decision Tree | 0,7843 | 0,7872 | 0,8441 | 0,8431 | 0,4998 | 0,503 | 0,79 | 0,79 | 0,77
 | Bernoulli Naive Bayes | 0,7276 | 0,7287 | 0,7242 | 0,7362 | 0,3378 | 0,3533 | 0,72 | 0,73 | 0,71
 | Gaussian Naive Bayes | 0,691 | 0,6946 | 0,7205 | 0,7301 | 0,3458 | 0,3561 | 0,69 | 0,69 | 0,69

Cuadro 4.7: Evaluación de técnicas de modelamiento en el Dataset 6 (9 variables)
Techniques / Metrics Train Acc Test Acc Train AUC Test AUC Train KS Test KS Precision Recall F1-score
 | Logistic Regression | 0,9377 | 0,9385 | 0,8589 | 0,8604 | 0,5882 | 0,5787 | 0,92 | 0,94 | 0,92
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,9532 | 0,9534 | 0,9467 | 0,9412 | 0,7378 | 0,7334 | 0,95 | 0,95 | 0,94
 | Extra Tree | 0,9532 | 0,954 | 0,9469 | 0,9411 | 0,7382 | 0,7331 | 0,95 | 0,95 | 0,94
 | Bagging | 0,9532 | 0,9535 | 0,9467 | 0,9411 | 0,7377 | 0,7335 | 0,95 | 0,95 | 0,94
 | Gradient Boosting | 0,942 | 0,9426 | 0,8843 | 0,8818 | 0,6175 | 0,6127 | 0,94 | 0,94 | 0,92
 | AdaBoost | 0,9366 | 0,938 | 0,8617 | 0,8614 | 0,5811 | 0,5804 | 0,92 | 0,94 | 0,92
 | Decision Tree | 0,9532 | 0,9539 | 0,9469 | 0,9407 | 0,7382 | 0,7327 | 0,95 | 0,95 | 0,94
 | Bernoulli Naive Bayes | 0,9117 | 0,9095 | 0,8274 | 0,8273 | 0,5174 | 0,5201 | 0,91 | 0,91 | 0,91
 | Gaussian Naive Bayes | 0,8873 | 0,8853 | 0,8369 | 0,8405 | 0,5454 | 0,5459 | 0,91 | 0,89 | 0,9

Como se puede observar en los Cuadros 4.2, 4.5 y 4.6, existe una mejora de desempeño de todas las métricas para las técnicas de ensamblaje (Random Forest, Extra Tree, Bagging, Gradient Boosting y AdaBoost) y en Decision Tree al comparar con Logistic Regression; esta ganancia es más signiﬁcativa en Random Forest, Extra Tree y Bagging, además se puede notar que a mayor cantidad de variables (mayor dimensionalidad), más signiﬁcativa es la diferencia de las métricas de desempeño de estas tres técnicas respecto de Logistic Regression.
En los Cuadros 4.3, 4.4 y 4.7 se puede observar resultados similares en mejora de desempeño, excepto para AdaBoost, donde esta técnica obtiene un desempeño equivalente a la regresión (no hay mejora signiﬁcativa).
Por ultimo, se destaca que las dos técnicas de Naive Bayes obtienen peor desempeño en todas las métricas para todos los experimentos.
Para el análisis cuantitativo de las métricas, se comparan las técnicas en base a la métrica más importante considerada por los analistas de modelos, la cual es el KS; los resultados se pueden ver en las ﬁguras 4.1 a 4.6.
Para facilitar el análisis cuantitativo, se ﬁja como punto de referencia (lı́nea de benchmark roja) el Test KS de la regresión logı́stica, dado que representa el poder de discriminación entre”buenos” y”malos” del modelo a comparar por el resto de las técnicas.
Con los resultados de las ﬁguras 4.1 a 4.6 se puede corroborrar visualmente el análisis realizado con los cuadros 4.2 a 4.7, es decir, las técnicas de Random Forest, Extra Tree, Bagging y Decision Tree son las que entregan un mejor desempeño, ya que la diferencia en ganancia de desempeño es signiﬁcativa.
Además se puede observar, que a mayor complejidad del problema a resolver (más variables), mayor llega a ser la diferencia de desempeño entre la regresión logı́stica y las técnicas con alto desempeño; en particular, las técnicas basadas en árboles logran segmentar mejor los comportamientos que tienen los datos.
Por otro lado, se puede observar que ninguna técnica presenta sobreajuste, dado que la diferencia entre Train KS y Test KS, en particular, es poco signiﬁcativa; por lo tanto, los modelos aprendieron a clasiﬁcar los clientes”buenos” y”malos” a nivel general y no sólo para los datos utilizados.
Respecto al área bajo la curva ROC, las diferencias de desempeño se pueden observar en las ﬁguras 4.7 a 4.12.
Como puede observarse, en general para los 6 datasets hay ganancia de desempeño en AUC Score para Random Forest, Extra Tree, Bagging y Decision Tree, pero no tanta diferencia como el KS; pero se corrobora que estas técnicas están discriminando mejor a los clientes”buenos” y”malos”, debido a que a mayor AUC Score, mayor es la tasa entre verdaderos positivos y falsos positivos.
Figura 4.1: Desempeño del KS para Logistic Regression y otras técnicas en el Dataset 1 4.2: Desempeño del KS para Logistic Regression y otras técnicas en el Dataset 2
Figura 4.3: Desempeño del KS para Logistic Regression y otras técnicas en el Dataset 3 4.4: Desempeño del KS para Logistic Regression y otras técnicas en el Dataset 4
Figura 4.5: Desempeño del KS para Logistic Regression y otras técnicas en el Dataset 5 4.6: Desempeño del KS para Logistic Regression y otras técnicas en el Dataset 6
Figura 4.7: Desempeño del AUC Score en el Dataset 1 entre las técnicas Figura 4.9: Desempeño del AUC Score en el Dataset 3 entre las técnicas Figura 4.11: Desempeño del AUC Score en el Dataset 5 entre las técnicas 4.8: Desempeño del AUC Score en el Dataset 2 entre las técnicas 4.10: Desempeño del AUC Score en el Dataset 4 entre las técnicas 4.12: Desempeño del AUC Score en el Dataset 6 entre las técnicas
Finalmente se comprueba, al igual que en las ﬁguras del KS, que no existe sobreajuste debido a la diferencia poca signiﬁcativa entre Train AUC y Test AUC para cada técnica de modelamiento.
En esta etapa se comprueba empı́ricamente que las técnicas de modelamiento que traen beneﬁcios respecto a Logistic Regression son las de ensamblaje, especı́ﬁcamente Random Forest, Extra Tree y Bagging, ya que el KS y el AUC Score siempre son mejores en los seis datasets utilizados y además, no presentan sobreajuste.
Se tendrán en cuenta estos resultados para la siguiente etapa."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,4.3. Etapa 2: Selección de Variables + Modelamiento,"Esta etapa tiene el propósito de implementar nuevas técnicas de selección de variables, por lo que se reduce la cantidad de técnicas de modelamiento para utilizar las métricas, con el objetivo de enfocar los esfuerzos de la investigación en la selección y no en el modelamiento.
Las técnicas de modelamiento utilizadas en esta etapa son las siguientes:
• Logistic Regression
• Random Forest
• Extra Tree
• Gradient Boosting
Respecto a Bagging, a pesar de tener buenos resultados en la etapa anterior, no tiene todas las funciones necesarias en la biblioteca de Python para obtener los mismos tipos de resultados de las técnicas seleccionadas.
Dado el mismo argumento de reducir las técnicas, también se reducen los datasets a utilizar, seleccionando los siguientes entre los seis iniciales (siguiendo la misma referencia numérica) en su primera versión:
• Dataset 1: 118.307 registros, donde la primera versión cuenta con 233 variables.
• Dataset 2: 54.773 registros, donde la primera versión cuenta con 206 variables.
• Dataset 6: 65.539 registros, donde la primera versión cuenta con 117 variables.
Las técnicas de selección de variables implementadas para la investigación son las siguientes:
• Principal Component Analysis (PCA): se basa en la agrupación de variables que están correlacionadas en un mismo componente; luego, para cada componente que se genera, se selecciona la variable más predictiva con el ﬁn que el output sean variables que no estén correlacionadas entre si y sean las mejores de cada componente.
• Ranking de Importancia”Técnica” + Convergencia KS (CP KS 1): esta técnica de selección se basa en dos etapas, en la primera se calcula la importancia de todas las variables con una técnica de ensamblaje, y se genera un ranking de las variables dada su importancia.
La segunda etapa se toma este ranking y se va ingresando una sola variable por iteración y en ese mismo paso se calcula el KS del modelo.
Desde la segunda iteración en adelante se compara el KS del modelo actual con n variables y el KS del modelo anterior de n
−
1 variables, en el caso que el KS no haya variado signiﬁcativamente, el algoritmo se detiene y se retorna las n variables de la iteración.
• Ranking KS + Convergencia KS (CP KS 2): es similar a la técnica CP KS 1, pero el
ranking de importancia se reemplaza por un ranking del KS de cada variable en orden descendente.
• Ranking IV + Convergencia KS (CP KS 3): es similar a las técnicas CP KS 1 y 2, pero el ranking de importancia/KS se reemplaza por un ranking del IV de cada variable en orden descendente.
• Correlación con ranking de Importancia”Técnica”(Correlación 1): esta técnica calcula la matriz de correlación entre todas las variables con los coeﬁcientes de correlación de Pearson, Spearman o Tau-b de Kendall; luego, se genera un ranking de importancia para cada variable.
La metodologı́a de selección de variables consiste en tomar la mejor candidata en base a su importancia y ﬁltrar del set de datos las que posean una correlación mayor o igual a 0,5.
Luego de forma iterativa se realiza el mismo procedimiento con la siguiente mejor candidata del conjunto de variables actualizado, luego de la aplicación del ﬁltro.
• Correlación con Ranking IV (Correlación 2): es similar a la técnica de correlación anterior, pero el ranking con los IV de cada variable.
• Correlación con Ranking KS (Correlación 3): Es similar a la técnica de correlación anterior, pero el ranking con los KS de cada variable.
• Correlación con Resultados anteriores de los 3 métodos de correlación (Correlación
4): esta técnica es una combinación de los tres métodos de correlación anteriores, es decir, el input de variables es el output de las tres técnicas anteriores.
En base a estas variables se genera un ranking por importancia utilizando una técnica de ensamblaje, luego se calcula la matriz de correlación entre las variables y se repite el procedimiento de ﬁltrado que las tres técnicas anteriores.
Cabe destacar que las técnicas de (PCA) y Correlación con ranking IV (Correlación 2) ya están presentes en la metodologı́a actual, pero son implementadas dentro de un software estadı́stico, por lo que para ﬁnes de la investigación no se considera un punto de referencia, si no como una técnica de selección más implementada junto al resto.
Antes de comenzar el análisis se destaca que algunos de estos métodos de selección implican utilizar una técnica de modelamiento dentro del algoritmo; por ejemplo, para obtener la importancia por una”Técnica” (en las técnicas CP KS 1, Correlación 1 y Correlación 4), se requiere el uso de una técnica de modelamiento para su ejecución.
Por lo tanto, para evaluar el desempeño del método con un modelo, se utiliza la misma técnica usada en el algoritmo con el ﬁn de no perjudicarlo.
Con esto, estas tres técnicas de selección aplican un modelamiento dentro de su algoritmo.
Los resultados de las 8 técnicas de selección de variables en los 3 datasets se presentan en los cuadros 4.8 y 4.9.
Cuadro 4.8: Resultados de las técnicas de selección de variables en el Dataset 1 (parte 1) Cuadro 4.9: Resultados de las técnicas de selección de variables en el Dataset 1 (parte 2)
 |  | Random Forest | Extra Tree
 | --- | --- | ---
 |  | N Variables | Train KS | Test KS | N Variables | Train KS | Test KS
 | --- | --- | --- | --- | --- | --- | ---
 | Etapa 0 | 5 | 0,597 | 0,594 | 5 | 0,597 | 0,594
 | Etapa 1 | 5 | 0,6097 | 0,6056 | 5 | 0,6097 | 0,6056
 | PCA | 7 | 0,7134 | 0,697 | 7 | 0,7134 | 0,6965
 | CP KS 1 | 6 | 0,6149 | 0,611 | 6 | 0,6191 | 0,6148
 | CP KS 2 | 6 | 0,6147 | 0,6088 | 6 | 0,6147 | 0,6088
 | CP KS 3 | 6 | 0,6119 | 0,6064 | 6 | 0,6119 | 0,6064
 | Correlación 1 | 10 | 0,7545 | 0,7371 | 10 | 0,7101 | 0,6895
 | Correlación 2 | 10 | 0,7403 | 0,7234 | 10 | 0,7405 | 0,7229
 | Correlación 3 | 10 | 0,7045 | 0,6877 | 10 | 0,7046 | 0,6872
 | Correlación 4 | 10 | 0,7644 | 0,7474 | 10 | 0,7607 | 0,7386

 |  | Gradient Boosting | Logistic Regression
 | --- | --- | ---
 |  | N Variables | Train KS | Test KS | N Variables | Train KS | Test KS
 | --- | --- | --- | --- | --- | --- | ---
 | Etapa 0 | 5 | 0,597 | 0,594 | 5 | 0,597 | 0,594
 | Etapa 1 | 5 | 0,6026 | 0,5995 | - | - | -
 | PCA | 7 | 0,6265 | 0,6246 | 7 | 0,6178 | 0,6136
 | CP KS 1 | 6 | 0,6258 | 0,6248 | 6 | 0,6119 | 0,6069
 | CP KS 2 | 6 | 0,6147 | 0,6088 | 6 | 0,6129 | 0,6072
 | CP KS 3 | 6 | 0,6119 | 0,6064 | 6 | 0,6106 | 0,6054
 | Correlación 1 | 10 | 0,6066 | 0,5995 | 10 | 0,5972 | 0,5929
 | Correlación 2 | 10 | 0,6092 | 0,6047 | 10 | 0,5983 | 0,5943
 | Correlación 3 | 10 | 0,6064 | 0,6057 | 10 | 0,5972 | 0,5932
 | Correlación 4 | 10 | 0,612 | 0,6107 | 10 | 0,5983 | 0,5941

Como se puede ver, para el Dataset 1 la mayorı́a de las técnicas entrega una mejora en el desempeño del KS, pero con mayor cantidad de variables respecto a la etapa 1.
Cabe destacar que las cuatro técnicas de selección por correlación devuelven 10 variables (distintas entre si), las cuales no generan mucha ganancia para las técnicas de Logistic Regression y Gradient Boosting, pero sı́ para Random Forest y Extra Tree, demostrando que al complejizar el modelo con más variables, ambas técnicas logran discriminar mucho mejor a un cliente”bueno” o”malo”.
Otro detalle es que las cinco variables extras seleccionadas por las técnicas de correlación no generan una ganancia signiﬁcativa respecto a las otras cinco variables del entregadas en la metodologı́a actual con Logistic Regression y Gradient Boosting.
Para el Dataset 2 (cuadro 4.10) se puede observar que las técnicas de correlación seleccionan entre 40 y 42 variables, lo cual genera un problema en no lograr reducir la complejidad del modelo signiﬁcativamente; dado esto, Random Forest y Extra Tree obtienen buenos resultados por esta complejidad, pero el modelo no es interpretable por analistas y el negocio asociado dada la cantidad excesiva de variables.
Cuadro 4.10: Resultados de las técnicas de selección de variables en el Dataset 2 (parte 1)
 |  | Random Forest | Extra Tree
 | --- | --- | ---
 | Etapa 0 | N Variables 9 | Train KS 0,5189 | Test KS 0,5165 | N Variables 9 | Train KS 0,5189 | Test KS 0,5165
 | --- | --- | --- | --- | --- | --- | ---
 | Etapa 1 | 9 | 0,6628 | 0,6521 | 9 | 0,6632 | 0,6527
 | PCA | 15 | 0,963 | 0,9545 | 15 | 0,9638 | 0,9573
 | CP KS 1 | 20 | 0,9458 | 0,9381 | 24 | 0,961 | 0,9578
 | CP KS 2 | 19 | 0,9101 | 0,9058 | 19 | 0,9108 | 0,9075
 | CP KS 3 | 20 | 0,9212 | 0,9194 | 20 | 0,9222 | 0,9216
 | Correlación 1 | 42 | 0,9986 | 0,9915 | 42 | 0,9998 | 0,9964
 | Correlación 2 | 40 | 0,9984 | 0,99 | 40 | 0,9989 | 0,995
 | Correlación 3 | 40 | 0,9988 | 0,9885 | 40 | 0,9991 | 0,9952
 | Correlación 4 | 42 | 0,9991 | 0,9912 | 42 | 0,9998 | 0,997

Cuadro 4.11: Resultados de las técnicas de selección de variables en el Dataset 2 (parte 2)
 |  | Gradient Boosting | Logistic Regression
 | --- | --- | ---
 | Etapa 0 | N Variables 9 | Train KS 0,5189 | Test KS 0,5165 | N Variables 9 | Train KS 0,5189 | Test KS 0,5165
 | --- | --- | --- | --- | --- | --- | ---
 | Etapa 1 | 9 | 0,5493 | 0,5412 | - | - | -
 | PCA | 15 | 0,6713 | 0,6583 | 15 | 0,6184 | 0,617
 | CP KS 1 | 9 | 0,6392 | 0,633 | 13 | 0,6344 | 0,6336
 | CP KS 2 | 16 | 0,6823 | 0,6796 | 10 | 0,6114 | 0,6132
 | CP KS 3 | 10 | 0,6499 | 0,6502 | 11 | 0,6185 | 0,6251
 | Correlación 1 | 41 | 0,7149 | 0,7091 | 41 | 0,6437 | 0,6393
 | Correlación 2 | 40 | 0,7216 | 0,7162 | 40 | 0,6416 | 0,6376
 | Correlación 3 | 40 | 0,7194 | 0,7139 | 40 | 0,6421 | 0,6359
 | Correlación 4 | 41 | 0,7198 | 0,7125 | 41 | 0,6419 | 0,6375

4.13: Comparación del KS en el Dataset 2 y número de variables post-selección para
Gradient Boosting y Logistic Regression
Cuadro 4.12: Resultados de las técnicas de selección de variables en el Dataset 6 (parte 1) Cuadro 4.13: Resultados de las técnicas de selección de variables en el Dataset 6 (parte 2)
 |  | Random Forest | Extra Tree
 | --- | --- | ---
 | Etapa 0 | N Variables 14 | Train KS 0,5882 | Test KS 0,5787 | N Variables 14 | Train KS 0,5882 | Test KS 0,5787
 | --- | --- | --- | --- | --- | --- | ---
 | Etapa 1 | 14 | 0,7378 | 0,7334 | 14 | 0,7382 | 0,7331
 | PCA | 10 | 0,8393 | 0,8368 | 10 | 0,8395 | 0,8355
 | CP KS 1 | 22 | 0,9754 | 0,9708 | 8 | 0,6922 | 0,6893
 | CP KS 2 | 6 | 0,6585 | 0,6554 | 6 | 0,6586 | 0,6555
 | CP KS 3 | 7 | 0,6942 | 0,6872 | 7 | 0,6942 | 0,6873
 | Correlación 1 | 21 | 0,8956 | 0,8826 | 21 | 0,8908 | 0,8764
 | Correlación 2 | 20 | 0,882 | 0,8633 | 20 | 0,8825 | 0,8657
 | Correlación 3 | 19 | 0,8544 | 0,8359 | 19 | 0,8548 | 0,8351
 | Correlación 4 | 21 | 0,8958 | 0,8774 | 20 | 0,8869 | 0,871

 |  | Gradient Boosting | Logistic Regression
 | --- | --- | ---
 | Etapa 0 | N Variables 14 | Train KS 0,5882 | Test KS 0,5787 | N Variables 14 | Train KS 0,5882 | Test KS 0,5787
 | --- | --- | --- | --- | --- | --- | ---
 | Etapa 1 | 14 | 0,6175 | 0,6127 | - | - | -
 | PCA | 10 | 0,701 | 0,689 | 10 | 0,6722 | 0,6651
 | CP KS 1 | 12 | 0,7218 | 0,7296 | 12 | 0,6848 | 0,6913
 | CP KS 2 | 6 | 0,6553 | 0,6571 | 6 | 0,6426 | 0,6403
 | CP KS 3 | 6 | 0,6743 | 0,6775 | 6 | 0,6431 | 0,6426
 | Correlación 1 | 21 | 0,5628 | 0,5505 | 21 | 0,4992 | 0,4846
 | Correlación 2 | 20 | 0,5613 | 0,5318 | 20 | 0,4966 | 0,483
 | Correlación 3 | 19 | 0,5596 | 0,5355 | 19 | 0,5005 | 0,4905
 | Correlación 4 | 19 | 0,5596 | 0,5436 | 19 | 0,4864 | 0,4707

Figura 4.14: Comparación del KS en el Dataset 6 y número de variables post-selección para
Random Forest y Extra Tree
Comparando las ocho técnicas se puede observar que PCA selecciona 15 variables (ﬁgura 4.13), las cuales tienen un buen desempeño en KS, y supera con signiﬁcancia a los modelos entrenados con 9 variables en la etapa 0 y 1.
Para el Dataset 6, se puede observar que las ocho técnicas entregan buenos resultados para Random Forest y Extra Tree, pero para las otras dos técnicas los métodos de selección de correlación no aportan variables que logren superar al modelo entrenado con la metodologı́a actual (etapa 0).
Se puede destacar que hay técnicas de selección que obtuvieron menos variables que la Etapa 1 y el desempeño en KS logra subir signiﬁcativamente, por lo que se encontraron entre 6 y 12 variables que impactan mucho más en el modelo.
Los resultados de esta etapa son bien diversos.
Esto puede justiﬁcarse debido a que las variables que quedan después de una técnica de selección inﬂuyen mucho en los resultados ﬁnales, por lo que en base a los experimentos no se puede declarar que una de las ocho técnicas de selección es la mejor, aunque las cuatro técnicas de correlación tienden a ser superadas por las otras cuatro al tener en cuenta el trade oﬀ entre ganar desempeño vs seleccionar pocas variables.
Cabe destacar que las tres técnicas de CP KS son las que tienden a destacarse entre las ocho técnicas de selección; además, para el área de desarrollo de modelos de la Institución Bancaria es una innovación importante, ya que estas técnicas de selección contienen en cierto grado, el beneﬁcio de las técnicas de ensamblaje (Random Forest, Extra Tree y Gradient Boosting), como por ejemplo, calcular una importancia para cada variable luego de utilizarlas dentro de un modelo."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,4.4. Etapa 3: Transformación de las Variables + Selección de Variables + Modelamiento,"• Transformación WoE: variables continuas categóricas con transformación WoE, tal como se ha utilizado durante todas las etapas anteriores.
• Sin Transformación: variables continuas en su forma natural y variables categóricas con transformación WoE.
• Transformación LogN: variables continuas con transformación, aplicando logaritmo natural y variables categóricas con transformación WoE.
• Transformación Log: variables continuas con transformación, aplicando logaritmo en base 10 y variables categóricas con transformación WoE.
• Transformación Sqrt: variables continuas con transformación, aplicando raı́z cuadrada y variables categóricas con transformación WoE.
Cabe destacar que al utilizar las tres últimas transformaciones mencionadas, se tomó la decisión de dejar los datos que se indeﬁnen al aplicar la transformación en su forma natural para lograr realizar los experimentos.
Dado que en la etapa 1 se utilizaron seis datasets, nueve técnicas de modelamiento y transformación WoE, se compara las otras tres transformaciones restantes y las variables sin transformación con el ﬁn de observar el impacto en el desempeño de los modelos.
Los resultados se encuentran en los cuadros 4.14 a 4.19.
Cuadro 4.14: Resultados de la transformación de variables en el Dataset 1 respecto de la Etapa 1
 |  | Etapa 1 (WoE) | Sin Transformación | Transformación LogN | Transformación Log | Transformación Sqrt
 | --- | --- | --- | --- | --- | ---
 | Logistic Regression | 0,597 | 0,594 | 0,5944 | 0,5926 | 0,601 | 0,6002 | 0,6034 | 0,6007 | 0,6008 | 0,6003
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Techniques/Transform Train KS Test KS Train KS Test KS Train KS Test KS Train KS Test KS Train KS Test KS
 | Random Forest | 0,6097 | 0,6056 | 0,8286 | 0,8119 | 0,827 | 0,8105 | 0,8286 | 0,8116 | 0,8286 | 0,8115
 | Extra Tree | 0,6097 | 0,6056 | 0,8292 | 0,8154 | 0,8276 | 0,8149 | 0,8292 | 0,8163 | 0,8292 | 0,8164
 | Bagging | 0,6097 | 0,6056 | 0,8286 | 0,8124 | 0,827 | 0,8123 | 0,8286 | 0,8126 | 0,8286 | 0,8118
 | Gradient Boosting | 0,6026 | 0,5995 | 0,62 | 0,6147 | 0,6234 | 0,6153 | 0,62 | 0,6147 | 0,62 | 0,6147
 | AdaBoost | 0,5978 | 0,5947 | 0,6086 | 0,6045 | 0,611 | 0,6062 | 0,6086 | 0,6045 | 0,6086 | 0,6045
 | Decision Tree | 0,6097 | 0,6056 | 0,8292 | 0,8097 | 0,8276 | 0,8099 | 0,8292 | 0,8104 | 0,8292 | 0,8095
 | Bernoulli Naive Bayes | 0,557 | 0,5513 | 0,59 | 0,5854 | 0,5893 | 0,5846 | 0,59 | 0,5854 | 0,59 | 0,5854
 | Gaussian Naive Bayes | 0,5872 | 0,5824 | 0,5741 | 0,5749 | 0,5911 | 0,5879 | 0,5977 | 0,5995 | 0,5832 | 0,5848

Cuadro 4.15: Resultados de la transformación de variables en el Dataset 2 respecto de la Etapa 1
 |  | Etapa 1 (WoE) | Sin Transformación | Transformación LogN | Transformación Log | Transformación Sqrt
 | --- | --- | --- | --- | --- | ---
 | Logistic Regression | 0,5189 | 0,5165 | 0,2381 | 0,2292 | 0,4927 | 0,4872 | 0,493 | 0,4908 | 0,5154 | 0,5028
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Techniques/Transform Train KS Test KS Train KS Test KS Train KS Test KS Train KS Test KS Train KS Test KS
 | Random Forest | 0,6627 | 0,6517 | 1 | 0,9945 | 1 | 0,9918 | 1 | 0,9944 | 1 | 0,9944
 | Extra Tree | 0,6632 | 0,6527 | 1 | 0,9968 | 1 | 0,997 | 1 | 0,9968 | 1 | 0,9968
 | Bagging | 0,6627 | 0,6519 | 1 | 0,9913 | 1 | 0,9914 | 1 | 0,9914 | 1 | 0,9914
 | Gradient Boosting | 0,5493 | 0,5412 | 0,7039 | 0,6907 | 0,7052 | 0,6873 | 0,7039 | 0,6907 | 0,7039 | 0,6907
 | AdaBoost | 0,5174 | 0,5134 | 0,671 | 0,6671 | 0,6675 | 0,6594 | 0,671 | 0,6671 | 0,671 | 0,6671
 | Decision Tree | 0,6632 | 0,6526 | 1 | 0,9826 | 1 | 0,9795 | 1 | 0,9834 | 1 | 0,9845
 | Bernoulli Naive Bayes | 0,4869 | 0,4759 | 0,2578 | 0,2423 | 0,2616 | 0,2539 | 0,2578 | 0,2423 | 0,2578 | 0,2423
 | Gaussian Naive Bayes | 0,4963 | 0,4813 | 0,4491 | 0,4413 | 0,4814 | 0,4644 | 0,4887 | 0,4788 | 0,4926 | 0,4955

Cuadro 4.16: Resultados de la transformación de variables en el Dataset 3 respecto de la Etapa 1
 |  | Etapa 1 (WoE) | Sin Transformación | Transformación LogN | Transformación Log | Transformación Sqrt
 | --- | --- | --- | --- | --- | ---
 | Logistic Regression | 0,7165 | 0,6979 | 0,205 | 0,1748 | 0,7012 | 0,6824 | 0,6867 | 0,6702 | 0,6754 | 0,6568
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Techniques/Transform Train KS Test KS Train KS Test KS Train KS Test KS Train KS Test KS Train KS Test KS
 | Random Forest | 0,8553 | 0,8189 | 0,9956 | 0,9656 | 0,9956 | 0,9674 | 0,9955 | 0,9681 | 0,9955 | 0,9678
 | Extra Tree | 0,8586 | 0,8195 | 0,9959 | 0,9703 | 0,9959 | 0,9672 | 0,9959 | 0,9682 | 0,9959 | 0,969
 | Bagging | 0,8546 | 0,8187 | 0,9955 | 0,9647 | 0,9956 | 0,9682 | 0,9955 | 0,9692 | 0,9955 | 0,9675
 | Gradient Boosting | 0,7295 | 0,7102 | 0,7531 | 0,7323 | 0,7567 | 0,7343 | 0,7531 | 0,7323 | 0,7531 | 0,7323
 | AdaBoost | 0,7082 | 0,6877 | 0,7434 | 0,7243 | 0,733 | 0,7155 | 0,7434 | 0,7243 | 0,7434 | 0,7243
 | Decision Tree | 0,8586 | 0,8217 | 0,9959 | 0,9539 | 0,9959 | 0,9591 | 0,9959 | 0,9597 | 0,9959 | 0,9588
 | Bernoulli Naive Bayes | 0,6627 | 0,6494 | 0,3158 | 0,319 | 0,6768 | 0,6682 | 0,3158 | 0,319 | 0,3158 | 0,319
 | Gaussian Naive Bayes | 0,6854 | 0,6721 | 0,1956 | 0,1659 | 0,6373 | 0,6155 | 0,6256 | 0,6001 | 0,6051 | 0,5695

Cuadro 4.17: Resultados de la transformación de variables en el Dataset 4 respecto de la Etapa 1
 |  | Etapa 1 (WoE) | Sin Transformación | Transformación LogN | Transformación Log | Transformación Sqrt
 | --- | --- | --- | --- | --- | ---
 | Techniques/Transform Logistic Regression | Train KS 0,3396 | Test KS 0,3568 | Train KS 0,2567 | Test KS 0,2696 | Train KS 0,2955 | Test KS 0,3074 | Train KS 0,2782 | Test KS 0,3023 | Train KS 0,3013 | Test KS 0,3134
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,6794 | 0,6476 | 0,9572 | 0,9247 | 0,9559 | 0,9208 | 0,9573 | 0,9251 | 0,9573 | 0,9247
 | Extra Tree | 0,6798 | 0,6543 | 0,9575 | 0,9251 | 0,956 | 0,9209 | 0,9575 | 0,9247 | 0,9575 | 0,9259
 | Bagging | 0,6793 | 0,6471 | 0,957 | 0,9216 | 0,9559 | 0,9202 | 0,957 | 0,9203 | 0,957 | 0,9211
 | Gradient Boosting | 0,4011 | 0,4033 | 0,4722 | 0,4577 | 0,4694 | 0,4552 | 0,4722 | 0,4577 | 0,4722 | 0,4577
 | AdaBoost | 0,3453 | 0,3628 | 0,3832 | 0,3804 | 0,3861 | 0,3822 | 0,3832 | 0,3804 | 0,3832 | 0,3804
 | Decision Tree | 0,6798 | 0,6449 | 0,9575 | 0,8906 | 0,956 | 0,8899 | 0,9575 | 0,8931 | 0,9575 | 0,8921
 | Bernoulli Naive Bayes | 0,3111 | 0,3276 | 0,3014 | 0,3146 | 0,2778 | 0,2941 | 0,3014 | 0,3146 | 0,3014 | 0,3146
 | Gaussian Naive Bayes | 0,3265 | 0,3389 | 0,2329 | 0,2206 | 0,2855 | 0,293 | 0,2859 | 0,2998 | 0,2858 | 0,3047

Cuadro 4.18: Resultados de la transformación de variables en el Dataset 5 respecto de la Etapa 1
 |  | Etapa 1 (WoE) | Sin Transformación | Transformación LogN | Transformación Log | Transformación Sqrt
 | --- | --- | --- | --- | --- | ---
 | Techniques/Transform Logistic Regression | Train KS 0,3458 | Test KS 0,3575 | Train KS 0,2805 | Test KS 0,2882 | Train KS 0,2994 | Test KS 0,3228 | Train KS 0,2984 | Test KS 0,3246 | Train KS 0,3038 | Test KS 0,3137
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,4998 | 0,5039 | 0,9402 | 0,921 | 0,9372 | 0,9175 | 0,9402 | 0,9207 | 0,9402 | 0,921
 | Extra Tree | 0,4998 | 0,503 | 0,9402 | 0,9212 | 0,9372 | 0,9183 | 0,9402 | 0,9195 | 0,9402 | 0,9214
 | Bagging | 0,4998 | 0,503 | 0,9402 | 0,9195 | 0,9372 | 0,9169 | 0,9402 | 0,9197 | 0,9402 | 0,9195
 | Gradient Boosting | 0,4291 | 0,4365 | 0,5203 | 0,5156 | 0,5086 | 0,5087 | 0,5203 | 0,5156 | 0,5203 | 0,5156
 | AdaBoost | 0,3558 | 0,3684 | 0,4204 | 0,4246 | 0,4226 | 0,4289 | 0,4204 | 0,4246 | 0,4204 | 0,4246
 | Decision Tree | 0,4998 | 0,503 | 0,9402 | 0,9194 | 0,9372 | 0,9169 | 0,9402 | 0,9181 | 0,9402 | 0,919
 | Bernoulli Naive Bayes | 0,3378 | 0,3533 | 0,3139 | 0,3301 | 0,3296 | 0,3427 | 0,3139 | 0,3298 | 0,3139 | 0,3301
 | Gaussian Naive Bayes | 0,3458 | 0,3561 | 0,1807 | 0,2138 | 0,3155 | 0,3245 | 0,3062 | 0,3176 | 0,298 | 0,303

Cuadro 4.19: Resultados de la transformación de variables en el Dataset 6 respecto de la Etapa 1
 |  | Etapa 1 (WoE) | Sin Transformación | Transformación LogN | Transformación Log | Transformación Sqrt
 | --- | --- | --- | --- | --- | ---
 | Techniques/Transform Logistic Regression | Train KS 0,5071 | Test KS 0,4973 | Train KS 0,0911 | Test KS 0,08 | Train KS 0,3889 | Test KS 0,3936 | Train KS 0,3698 | Test KS 0,3815 | Train KS 0,3593 | Test KS 0,3762
 | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
 | Random Forest | 0,6344 | 0,6172 | 1 | 0,9932 | 1 | 0,993 | 1 | 0,9933 | 1 | 0,9931
 | Extra Tree | 0,6347 | 0,6215 | 1 | 0,9972 | 1 | 0,9969 | 1 | 0,9972 | 1 | 0,9962
 | Bagging | 0,6342 | 0,6172 | 1 | 0,9932 | 1 | 0,993 | 1 | 0,9933 | 1 | 0,9932
 | Gradient Boosting | 0,5412 | 0,5279 | 0,7042 | 0,6728 | 0,7064 | 0,6833 | 0,7042 | 0,6728 | 0,7042 | 0,6728
 | AdaBoost | 0,5014 | 0,4913 | 0,6496 | 0,6239 | 0,6354 | 0,6194 | 0,6496 | 0,6239 | 0,6496 | 0,6239
 | Decision Tree | 0,6347 | 0,6217 | 1 | 0,9889 | 1 | 0,9938 | 1 | 0,9888 | 1 | 0,9888
 | Bernoulli Naive Bayes | 0,4926 | 0,4882 | 0,4258 | 0,4068 | 0,4304 | 0,4125 | 0,4258 | 0,4068 | 0,4258 | 0,4068
 | Gaussian Naive Bayes | 0,4908 | 0,4867 | 0,1593 | 0,1368 | 0,3765 | 0,3672 | 0,2266 | 0,2151 | 0,2531 | 0,2381

Como puede observarse en los cuadros 4.14 al 4.19, al no realizar transformación en las variables continuas, todas las técnicas asociadas a árboles (Random Forest, Extra Tree, Bagging, Gradient Boosting, AdaBoost y Decision Tree) se potencian y aumentan su KS en los dos set de datos (entrenamiento y prueba); esto también se ve reﬂejado al aplicar las transformaciones de LogN, Log y Sqrt.
Este mejoramiento de los indicadores se debe a que las variables continuas, al no estar categorizadas con la transformación WoE, mantienen su poder predictivo debido a que los valores no fueron agrupados, por lo tanto los árboles de decisión pueden desarrollar una mayor complejidad al momento de predecir, haciendo que su poder de discriminación aumente, aunque siendo más propensos a sobreajuste al utilizar datos fuera del conjunto de entrenamiento y prueba; es decir, si se llega a utilizar el modelo con todos los datos de la población, es altamente probable que los modelos que utilizaron árboles contengan sobreajuste o no sirva para explicar el problema.
Esto último se comprueba al ver que las métricas de KS estan muy cercanas al 100 % para las técnicas de
Random Forest, Extra Tree, Bagging y Decision Tree.
Por otro lado, se puede ver en los cuadros 4.15 al 4.19 que Logistic Regression pierde su poder predictivo considerablemente si no se utiliza la transformación WoE.
Esto es de esperarse, debido a que esta transformación busca potenciar los supuestos de la regresión para que se obtenga un buen modelo tales como variables con monotonı́a (creciente o decreciente), agrupación de outliers y no de variables correlacionadas, entre otras.
Por último, para las técnicas de Naive Bayes, el no utilizar transformación WoE también es perjudicial por motivos similares a Logistic Regression.
Figura 4.15: Desempeño del KS en el Dataset 1 al no realizar transformación en las variables continuas Figura 4.17: Desempeño del KS en el Dataset 1 al realizar transformación Log en las variables continuas Figura 4.19: Desempeño del KS en el Dataset 4 sin transformación en las variables continuas 4.16: Desempeño del KS en el Dataset 1 al realizar transformación LogN en las variables continuas 4.18: Desempeño del KS en el Dataset 1 al realizar transformación Sqrt en las variables continuas 4.20: Desempeño del KS en el Dataset 4 al realizar transformación LogN en las variables continuas
Figura 4.21: Desempeño del KS en el Dataset 4 al realizar transformación Log en las variables continuas Figura 4.23: Desempeño del KS en el Dataset 6 al sin transformación en las variables continuas Figura 4.25: Desempeño del KS en el Dataset 6 al realizar transformación Log en las variables continuas 4.22: Desempeño del KS en el Dataset 4 al realizar transformación Sqrt en las variables continuas 4.24: Desempeño del KS en el Dataset 6 al realizar transformación LogN en las variables continuas 4.26: Desempeño del KS en el Dataset 6 al realizar transformación Sqrt en las variables continuas
Respecto al beneﬁcio de no utilizar la transformación WoE, se puede observar en las ﬁguras 4.15 a 4.18 que la ganancia en poder de discriminación (KS) entre clientes”buenos” y”malos” es considerable para las variables del Dataset 1 para Random Forest, Extra Tree, Bagging y Decision Tree, dados los argumentos anteriores.
Una situación particular que se dio durante la etapa 2, fue que el Dataset 1 tiene variables más estables, provocando que la ganancia o pérdida de KS no fuera tan signiﬁcativa respecto al resto de los dataset.
Esto se vuelve a reﬂejar nuevamente para esta etapa, provocando en los otros datasets una diferencia en el resultado para Logistic Regression cuando no se utiliza la transformación WoE; lo cual es notorio al ver en las ﬁguras 4.19 a 4.26, donde el KS, tanto en los datos de entrenamiento como de prueba, baja signiﬁcativamente para los Datasets 4 y 6.
Luego de todos los experimentos realizados, se comprueba empı́ricamente que las tres etapas realizadas arrojan buenos resultados para las técnicas de ensamblaje y en las técnicas de selección de variables que involucran algoritmos de ensamblaje para el calculo de la importancia de las variables.
El equipo de analistas de desarrollo de modelos de la Institución Bancaria encuentra valor en los descubrimientos encontrados y esperan poder llevar a la práctica todo lo positivo para que su metodologı́a sea actualizada.
Se aspira que el trabajo técnico implementado (los scripts en Python) se adapten para que estén dentro de los procesos automatizados y se construyan modelos de credit scoring con mejor poder de predicción."
REDISEÑO DE LA METODOLOGÍA DE DESARROLLO DE UN MODELO DE SCORING DE UNA INSTITUCIÓN BANCARIA,5. Conclusiones,"Luego de esta investigación se tiene una mirada más clara del proceso de desarrollo de un modelo de scoring y los potenciales beneﬁcios de las nuevas técnicas de modelamiento que existen en herramientas de programación como Python.
Cabe destacar que estas conclusiones aplican para el contexto de la investigación realizada, y podrı́an variar para otros datasets, aunque se recomienda tomarlas como referencia para trabajos similares.
Los primeros experimentos usando técnicas de modelamiento para comparar los beneﬁcios versus la regresión logı́stica comprueban que las técnicas de ensamblaje, especı́ﬁcamente, Random Forest, Extra Tree, Bagging y Gradient Boosting logran generar un modelo con mejor poder de discriminación capaz de acertar con mayor precisión qué cliente sera bueno.
Además, si el contexto del problema a resolver es más complejo, es decir, presenta una cantidad de variables explicativas mayor, estas técnicas aumentan su diferencia de desempeño contra la regresión logı́stica.
Esto se debe principalmente porque estos algoritmos utilizan árboles de decisión para aprender los patrones del problema, los que se adaptan de mejor manera a clasiﬁcar en niveles altos de complejidad; pero por esta misma capacidad de adaptación, son más propensos a sobreajustarse a los datos de entrenamiento en el caso que no se controle la expansión de los árboles (parámetros de profundidad y división).
Los experimentos en la selección de variables con nuevas técnicas demuestran que una buena heurı́stica a seguir, es probar muchas técnicas de selección (como las de correlación y convergencia en KS) en paralelo y en base a los resultados, escoger la que mejor se adapte al problema.
Debido a que no existe la mejor técnica de selección para todos los problemas, conviene aplicar por lo menos dos, para realizar un análisis con las variables resultantes que devuelve cada técnica y su desempeño en el modelo.
Además, existe un trade oﬀ de a mayor cantidad de variables, mejor es el desempeño del modelo.
La selección de variables busca reducir justamente esta cantidad de variables al mı́nimo posible sin perder tanto desempeño, con el ﬁn de facilitar la salida del modelo en la explicación de por qué el cliente es”bueno” o”malo”.
Los últimos experimentos de la investigación en la transformación de las variables, demuestran que la transformación WoE es perjudicial para las técnicas de ensamblaje y los arboles de decisión, debido a que la categorización realizada con WoE acota en gran parte el poder predictivo de cada variable, por lo que el modelo va a perder mucho desempeño debido a esta pérdida de poder predictivo en sus variables.
Esto sólo es válido para las técnicas basadas en árboles de decisión utilizadas en los experimentos.
Esta investigación espera que beneﬁcie directamente al área de desarrollo de modelos de la Institución Bancaria, pero para lograr esto se debe comenzar a utilizar las herramientas open source como Python dentro de su metodologı́a por el gran potencial que entrega junto a la biblioteca de machine learning Scikit-learn.
Al realizarse estos cambios, la calidad de los modelos desarrollados aumentará signiﬁcativamente y los analistas tendrán un abanico de posibilidades para desarrollar un modelo.
Por otro lado, los tiempos de ejecución de procesos automatizados con herramientas de programación reducen signiﬁcativamente los tiempos de espera para cada etapa del desarrollo, ahorrando horas en la ejecución de los proyectos.
Una de las medidas sobre la que se deberı́a hacer hincapié, es replicar y mejorar todo el proceso de desarrollo de modelos fuera del Software IBM SPSS Modeler, con el ﬁn de aprovechar las herramientas open source que existen hoy para construir modelos predictivos, siempre y cuando estos procesos sean automatizados y no se requiera programar todo desde cero para cada proyecto.
En relación al enfoque CRISP-DM en reversa, resulta una buena forma de abordar la investigación si el enfoque es siempre experimentar con nuevas técnicas de modelamiento, con el ﬁn de comparar los resultados de las métricas en base a los modelos existentes ya entrenados con parte del set de datos y testeados con el resto del mismo.
En el caso que la investigación buscara otro enfoque, se recomienda utilizar CRISP-DM para abordar un proyecto completo desde lo que quiere el negocio, hasta tener resultados de un modelo.
Respecto a la herramienta de programación usada, Python como herramienta para la minerı́a de datos y máquinas de aprendizaje, es muy recomendada ya que existen bibliotecas bien avanzadas para el tratamiento de datos como pandas y para el uso de técnicas de modelamiento como Scikit-learn, además cuenta con la biblioteca de visualización matplotlib para ver de mejor forma los resultados.
Por último, se destaca que la comunidad de desarrolladores en Python es amplia, por lo que existe buena documentación y soporte respecto a las dudas del uso de estas bibliotecas en distintos sitios web.
En cuanto a la relación de esta memoria con lo aprendido en la carrera de Ingenierı́a Civil en Informática, se destaca el pensamiento lógico para resolver problemas, la aplicación de métodos cuantitativos desde el punto de vista estadı́stico y el uso de herramientas de inteligencia artiﬁcial para resolver problemas reales.
Por otro lado, para esta investigación, fue útil lo aprendido en las siguientes asignaturas:
• Estadı́stica Computacional: el uso de la estadı́stica es crucial para empezar a entender los datos y sus cualidades desde el lado cuantitativo.
• Computación Cientı́ﬁca: lo aprendido en este curso por el lado aplicativo fue importante para el manejo de datos con Python.
• Inteligencia Artiﬁcial: las nociones de heurı́sticas y forma de resolver problemas mediante algoritmos de optimización motivo en querer especializarse en esta área.
• Patrones de Reconocimiento en Minerı́a de Datos: lo enseñado en este curso de forma introductoria a esta área motivó a querer desarrollar este tema de memoria.
• Máquinas de Aprendizaje: este electivo de especialidad fue crucial para aplicar las técnicas de modelamiento desde las habilidades técnicas gracias a las tareas y presentaciones realizadas en el curso usando Scikit-learn.
Por otro lado, las asignaturas de Programación y Estructura de Datos fueron cruciales para implementar la solución tecnológica de forma correcta y eﬁciente, ya que no sólo basta con que la solución de automatización funcione, si no que ésta haya sido desarrollada en forma óptima para agregar valor al negocio desde el punto de vista de tener algoritmos de tiempos de ejecución cortos y que no fallan al usarse por profesionales que necesitan estas automatizaciones.
Finalmente, se recomienda como extensiones a este estudio:
• Optimización de hiper-parámetros en técnicas de modelamiento: la investigación se vio complicada varias veces por generar modelos con sobreajuste debido a no usar los mejores parámetros para restringir los árboles de decisión que utilizaban gran parte de las técnicas de modelamiento.
Por otro lado, la búsqueda de hiper-parámetros es costosa y compleja, ya que requiere distintas pruebas donde se utilice la misma técnica de modelamiento varias veces, pero modiﬁcando los parámetros de entrada hasta encontrar el modelo óptimo.
• Uso de más técnicas de modelamiento en bibliotecas distintas: la investigación se acotó al uso de técnicas dentro de Scikit-learn, pero existen mas técnicas interesantes de comparar con la regresión logı́stica como Redes Neuronales, que se encuentran en TensorFlow [11].
• Desarrollo de modelos de credit scoring con cluster computing: un gran problema que cada vez es más notorio para las instituciones ﬁnancieras, es la capacidad de procesar grandes volúmenes de datos en un tiempo prudente, para esto se puede utilizar bibliotecas de cluster computing para machine learning, como por ejemplo MLib en Apache Spark, con el ﬁn de desarrollar modelos de credit scoring en tiempo real a gran escala utilizando tecnologı́as de Big Data con Hadoop."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,1. RESUMEN,"El sostenido aumento de los costos de la energía ha golpeado fuertemente no sólo a nuestro país, sino a todo el mundo, principalmente debido a que la generación de energía eléctrica está fuertemente asociada a fuentes de energía convencional, como el petróleo diésel, el carbón y el gas natural, los cuales han incrementado sus costos de manera muy importante en estos últimos años.
En ese contexto y dado que en Chile el costo de la energía se encuentra dentro de los más altos de la región y del mundo, el fomento para el uso de las energías renovables no convencionales ha tomado una fuerza muy importante, especialmente en Chile en los últimos 2 años, siendo los sistemas eólicos y solares los que han tenido mayor auge, en particular estos últimos.
En el presente trabajo, se pretende determinar la viabilidad técnica y económica de implementar un sistema de generación fotovoltaico para el Hospital Militar de Santiago, que permita no sólo aportar a la contención de sus gastos en materia de consumo eléctrico, sino también, aportar con un proyecto de eficiencia energética a los objetivos que el Ejército de Chile ha dispuesto para sus unidades y también a la meta que el Ministerio de Energía ha definido en el largo plazo, cual es, que en el año 2025, el 20% de la energía generada en el país, provenga de fuentes ERNC (Energías Renovables No Convencionales)."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2. OBJETIVOS,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.1. OBJETIVO GENERAL,"Evaluar la viabilidad técnico-económica del uso de generación eléctrica fotovoltaica, mediante el uso de paneles solares, para proporcionar una potencia nominal de 100 kW, que será utilizada en las bombas recirculadoras del sistema de climatización del Hospital Militar de Santiago."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,2.2. OBJETIVOS ESPECÍFICOS,"Conocer el principio de funcionamiento de los paneles solares fotovoltaicos y las tendencias de uso actuales.
 Conocer y evaluar las condiciones solares de Santiago para la aplicación de esta tecnología de generación.
 Estudio y levantamiento de información de consumo energético en el Hospital Militar de Santiago.
 Determinar la fracción de energía que aportará la tecnología fotovoltaica en el consumo total.
 Evaluación económica de la implementación de generación fotovoltaica en base a paneles solares."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3. ANTECEDENTES DEL PROYECTO,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.1. INFORMACIÓN SOBRE EL HOSPITAL MILITAR DE SANTIAGO,"En el año 1914, el Ejército de Chile, consciente de la necesidad de contar con un organismo capaz de entregar una atención especializada en salud al personal de la institución, dispone la creación de un “Comité Pro Hospital” con la misión de dar forma al proyecto de construcción de un Hospital Militar para Santiago, tarea que demoró 14 años, hasta que en el año 1928 se da inicio a su construcción, proceso que tomó 4 años en materializarse, para ser inaugurado oficialmente en el año 1932, en la actual comuna de Providencia.
A partir de esa fecha, e ininterrumpidamente, las instalaciones del hospital fueron creciendo a medida que las necesidades del momento lo ameritaban, muchas veces de manera inorgánica, aprovechando los espacios físicos disponibles.
Sin embargo, las cada vez mayores exigencias en materia de salud para cumplir con su labor y la mayor escasez de espacio físico, hacen que en el año 1998, nazca la iniciativa de construir un nuevo hospital, como parte del nuevo Proyecto Integral de Modernización de Salud del Ejército, proceso que luego de 10 años de trabajo en las áreas de estudio, diseño, construcción y habilitación de las nuevas instalaciones, materializó su apertura el 27 de Abril de 2009, en un terreno de 9,9 hectáreas en la comuna de La Reina, contando en la actualidad con una superficie 85.000 m2 construidos, divididos en 3 edificios principales: Placa Técnica, Hospitalización y Académico."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,3.2. ORIGEN DEL PROYECTO,"En el año 2008, dada la estrechez energética que se vislumbraba en el futuro en Chile, debido a la falta de materialización de proyectos de generación de energía por medios convencionales (agua, gas, carbón, petróleo diésel), se promulgó en el país la Ley N° 20.257 para fomentar el uso de Energías Renovables No Convencionales (ERNC), la cual se encuentra vigente desde el año 2010.
Dicho cuerpo legal obliga a las generadoras eléctricas a que el 5% de la energía que comercializan, provenga de fuentes renovables no convencionales, tales como solar, eólica, geotérmica, mareomotriz, hidroeléctrica de hasta 20 MW o también de la biomasa.
Según esta ley, el aporte de este tipo de generación debe ser gradual hasta alcanzar un 10% de producción en el año 2024.
Sin embargo, las buenas perspectivas en el uso de este tipo de ERNC propició que el 14 de Octubre del año 2013 la autoridad gubernamental promulgara la Ley N° 20.698 que amplió la matriz energética del país mediante el uso de ERNC aumentando la meta de producción hasta un 20% en el año 2025.
Consecuente con lo anterior y dentro del contexto de falta de proyectos de inversión en generación eléctrica para sustentar el desarrollo futuro del país, en el año 2012, el Ministerio de Energía lanza la Estrategia Nacional de Energía 2012-2030 (ENE), con el objetivo de enfocarse en el desarrollo de la matriz energética eléctrica del país, estableciendo para ello lineamientos de política pública y promoviendo que para el año 2020 se alcance una disminución del 12% en la demanda de energía final.
Basada en esta estrategia de política pública, el mismo ministerio publica en el año 2012 el Plan de Acción de Eficiencia Energética (PAEE20), documento dirigido a entidades públicas y privadas con medidas técnicas para que orienten sus labores, dentro de sus respectivos ámbitos de acción, en el uso eficiente de la energía, manteniendo la meta de reducción de demanda de energía de un 12% establecida por la ENE.
El Ejército de Chile, como parte de su política de Responsabilidad Social Institucional (RSI) da especial relevancia a la protección del medio ambiente y al uso eficiente de los recursos.
En tal sentido, en su Memoria Anual del año 2012, la institución, como entidad pública del Estado de Chile, adopta el PAEE20 y sus medidas técnicas en materia energética, estableciendo los siguientes tres ejes principales de acción para cumplir con dicho programa y la meta propuesta:
a) Orientar sus acciones hacia un uso eficaz y eficiente de la energía.
b) Promover la educación ambiental para un mejor empleo de la energía y generar ahorro en el consumo.
c) Buscar el reemplazo gradual del consumo de energía convencional o tradicional hacia Energías Renovables No Convencionales (ERNC).
Lo anterior se refuerza en la Memoria Anual del año 2013, período en el que se firma un convenio de cooperación entre la Agencia Chilena de Eficiencia Energética (AChee) y el Ejército de Chile, con la finalidad de mejorar los procesos operacionales de la institución, integrar equipamiento de alta eficiencia tecnológica y promover en el personal la internalización de una cultura responsable que permita el ahorro sistemático de energía.
Además, en el año 2014 el Ejército y el Ministerio de Energía firmaron un convenio en el cual se comprometieron a generar una política de Eficiencia Energética, junto con la preparación de planes e implementación de medidas en estas materias.
Junto con ello, ambas entidades se comprometieron también a generar instancias de educación y capacitación al personal institucional en materias de ahorro y eficiencia energética.
Finalmente, el convenio suscrito con el Ministerio de Energía incorpora el desarrollo del proyecto denominado “Techos Solares en Edificios Públicos”, que dicha entidad pretende implementar en las reparticiones del Estado y que en el caso del Ejército, serían beneficiadas, en una primera etapa, las unidades militares del norte del país, todo lo cual está alineado con la política de Responsabilidad Social Institucional y que además muestre al Ejército ante la comunidad como una entidad no sólo preocupada del ahorro y eficiencia energética, sino también de buscar nuevas alternativas energéticas.
En razón de los antecedentes antes descritos y considerando que el Hospital Militar de Santiago, tiene dentro de sus objetivos estratégicos la reducción de sus costos operacionales en materia de consumo eléctrico, es que nace la necesidad de realizar el presente estudio y evaluar una alternativa de ERNC de tipo fotovoltaica para que pueda ser implementado en dicha instalación de salud."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4. ANTECEDENTES DE LA TECNOLOGÍA FOTOVOLTAICA,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.1. CONCEPTOS FOTOVOLTAICOS,"La tecnología fotovoltaica se basa en la conversión directa de la radiación solar que la Tierra recibe diariamente para ser utilizada en forma de energía eléctrica.
Dicha radiación no es otra cosa que la energía que emite el Sol en forma de ondas electromagnéticas, el cual debido a su temperatura efectiva de 5.776°K genera una cantidad de energía recibida, medida en el exterior de la atmósfera terrestre, de 1.366 W/m2 en promedio.
Parte de esta energía es absorbida por la atmósfera y otra parte es dispersada por ella, lo que hace que la energía que efectivamente llega a la superficie de la Tierra tenga un valor máximo de 1.000 W/m2 en el trópico a medio día.
En la actualidad, son varias las aplicaciones que pueden ser alimentadas con energía producida por sistemas fotovoltaicos, las cuales pueden ser tan variadas como servicios de iluminación, refrigeración, bombas hidráulicas, telecomunicaciones y televisión, entre otros.
Una instalación fotovoltaica tiene como principal elemento las celdas solares, las cuales son las que convierten la energía proveniente del sol directamente en electricidad utilizable, debido al fenómeno llamado Efecto Fotoeléctrico (también llamado Fotovoltaico), el cual fue descrito por el científico Albert Einstein en el año 1905."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.2. PRINCIPIO DE FUNCIONAMIENTO DE UNA CELDA Y PANEL FOTOVOLTAICO,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.2.1. CELDA FOTOVOLTAICA,"Las celdas fotovoltaicas están compuestas de varios materiales con propiedades semiconductoras, los cuales se transforman en conductores eléctricos cuando son expuestos a la luz solar.
La mayoría de las celdas fotovoltaicas están compuestas de silicio, elemento que está presente en cantidades suficientes y que no contamina el medioambiente.
Figura 1: Celda fotovoltaica
En la Figura 1 se muestra un diagrama esquemático de una celda fotovoltaica, la cual se encuentra compuesta de dos capas semiconductoras de diferentes propiedades eléctricas, lo cual es consecuencia de la denominada técnica de dopado.
La capa superior, denominada de tipo n, se encuentra dopada con fósforo para generar carga negativa, lo cual permite contar con una mayor cantidad de electrones libres (valencia +1).
La capa inferior, de tipo p, se encuentra dopada con boro para generar carga positiva, lo que permite contar con una menor cantidad de electrones libres (valencia -1).
Cuando estas dos capas se unen, forman la unión p-n, la cual crea un campo eléctrico que permite el flujo de electrones desde la capa p a la capa n, al incidir la luz solar sobre la celda.
Los fotones de los cuales se compone la luz solar, bombardean la celda fotovoltaica y ceden su energía a los electrones, permitiendo romper sus enlaces con el núcleo del átomo para generar el flujo de electrones (corriente eléctrica) deseado desde la capa p a la capa n. Para que finalmente dicha corriente eléctrica fluya, se requiere que dicha celda fotovoltaica cuente con electrodos metálicos en ambas caras.
En la Figura 2 se muestra el campo eléctrico generado en la celda al incidir fotones (luz solar) sobre ella y el flujo de electricidad creado.
Figura 2: Conversión de energía solar a energía eléctrica
Una celda fotovoltaica mide aproximadamente 10 x 10 cm.
y se encuentra revestida de una película protectora transparente antirreflectiva.
La diferencia de potencial (voltaje) generado a través de ella depende principalmente del diseño de la misma y del material del cual esté construida, mientras que la corriente eléctrica depende fundamentalmente de la radiación solar que incide sobre ella."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.2.2. TIPOS DE CELDAS FOTOVOLTAICAS,"En el mercado actual existen distintos tipos de tecnología para producir celdas fotovoltaicas, en cuyos procesos se utiliza principalmente el silicio como materia prima, además de otros materiales de más reciente uso.
De acuerdo a la estructura cristalina del silicio, las celdas fotovoltaicas se clasifican en dos categorías: monocristalinas y policristalinas, las cuales representan entre el 85% y 90% del mercado mundial.
Además, existe una tercera categoría de celda fotovoltaica, que se denomina de capa fina, la cual es confeccionada no solo con silicio amorfo y micromorfo, sino también con materiales como CdTe (Telururo de Cadmio), CIS (Diseleniuro de Indio de Cobre) y CIGS (Cobre – Indio – Galio – Selenio), que representa entre un 10% y un 15% del mercado mundial.
Otras tecnologías utilizadas en muy baja proporción en la producción de celdas fotovoltaicas se pueden mencionar la concentración fotovoltaica y celdas fotovoltaicas orgánicas.
Las tecnologías actualmente disponibles se resumen a continuación:
 Celdas de Silicio Monocristalino: Se confeccionan a partir de un sólido cristal de silicio puro.
Tienen mayores eficiencias, del orden de 15% a 17%, pero son de altos costos.
 Celdas de Silicio Policristalino: Se fabrican cortando las barras de silicio en varios trozos de forma cuadrangular.
Poseen menor eficiencia (13% a 15%) y menor costo que el monocristalino.
 Celdas de Silicio Amorfo: Se forman mediante el depósito de diferentes tipos de silicio tratado sobre un substrato de vidrio.
Utiliza silicio de menor calidad, tienen baja eficiencia (6% a 8%), pero a la vez un menor costo.
A pesar de su baja eficiencia funcionan correctamente con baja luminosidad, lo cual lo ha hecho ser muy utilizado en calculadoras, relojes y linternas LED, para recargar sus baterías.
 Celdas de Capa Fina: están compuestas de una sustancia absorbente solar, impresa sobre una capa de material flexible.
Este tipo de celda utiliza menos del 1% de silicio, siendo más económica y flexible que las celdas monocristalinas o policristalinas, pero es menos eficiente que estas (8% a 13%).
 Celdas Orgánicas: este tipo de tecnología permite transformar la energía luminosa en energía eléctrica a través de un proceso fotoelectroquímico.
Su eficiencia es del orden del 11% y su vida útil es muy corta (5 años aprox.).
 Concentración fotovoltaica: esta tecnología utiliza un elemento óptico para concentrar la luz solar y aumentar la eficiencia de las celdas y así reducir los costos.
Están diseñados para grandes potencias de varios megawatts y requieren ser guiados hacia el sol para estar siempre alineados.
Su mantenimiento es alto y costoso."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.2.3. PANEL FOTOVOLTAICO,"Basado en el concepto de celda solar, es posible definir un panel fotovoltaico como un arreglo de celdas solares con propiedades semiconductoras, dispuestas en serie y en paralelo capaces de convertir los fotones provenientes de la luz solar en electricidad, todo ello dentro de una estructura que mantiene las celdas aisladas del medio exterior, permitiendo sólo el paso de la luz solar.
En la Figura 3 se muestra un panel fotovoltaico y su funcionamiento.
Figura 3: Panel fotovoltaico y su funcionamiento."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.3. TIPOLOGÍAS DE INSTALACIONES FOTOVOLTAICAS,"Un sistema de generación fotovoltaico posee dos tipologías básicas, que son las instalaciones aisladas (off-grid) y aquellas conectada a la red (on-grid).
Existe también un tercer tipo de conexión que se denominan instalaciones híbridas, que son una combinación de las dos anteriores a las que se agregan equipos de respaldo."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.3.1. SISTEMAS FOTOVOLTAICOS AISLADOS,"El objetivo de esta tipología (off-grid) es satisfacer directamente la demanda energética de los consumos asociados, independiente de la red eléctrica.
Se utilizan principalmente para alimentar a satélites, señales viales, equipos de telecomunicaciones, sistemas de bombeo de agua y viviendas sin acceso a la red eléctrica.
Estos sistemas requieren de un elemento almacenador de energía, generalmente una batería o un conjunto de ellas, siendo este el elemento crítico de una instalación aislada.
Esto significa que si el generador fotovoltaico deja de funcionar (como por ejemplo en las noches), el consumo de las cargas es cubierto en cualquier momento por la(s) batería(s).
La Figura 11 muestra un diagrama de este tipo de instalaciones:
Figura 11: Sistema fotovoltaico aislado."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.3.2. SISTEMAS FOTOVOLTAICOS CONECTADOS A LA RED,"El objetivo de este tipo de sistemas (on-grid) es producir electricidad en complemento a la recibida de la red eléctrica e inyectarla directamente a la red de consumo y en caso de haber excedentes, se inyecta a la red eléctrica pública.
Por este motivo no cuenta con baterías de almacenamiento.
La conexión de estos sistemas a la red de consumo puede ser monofásica o trifásica, dependiendo de la configuración elegida y las características del punto de conexión de dicha red.
Un punto importante que se debe tener presente al implementar este tipo de sistemas, es que la calidad de la energía inyectada a la red de consumo, debe cumplir con la reglamentación técnica vigente en el país, para no perturbar el comportamiento de la red ni perjudicar a los equipos conectados a ella.
Adicionalmente a esto, el inversor utilizado (que se describe más adelante), debe ser capaz de sincronizar la corriente generada con el voltaje de la red eléctrica y detectar una eventual caída de este para desconectar la generación fotovoltaica, evitando así islas energéticas.
La Figura 12 muestra este tipo de instalación.
Figura 12: Sistema fotovoltaico conectado a la red eléctrica."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.3.3. SISTEMAS FOTOVOLTAICOS HÍBRIDOS,"Este tipo de sistemas tiene como objetivo producir electricidad no sólo a través de un generador fotovoltaico, sino también utilizando sistemas de respaldo de energía, tales como los grupos generadores que utilizan petróleo diésel como combustible, de manera de asegurar un suministro continuo de electricidad, ya sea indirectamente cargando las baterías o inyectando directamente a los consumos, lo cual es especialmente útil cuando la ubicación geográfica de dichos consumos se encuentre alejada de la red eléctrica pública.
La Figura 13 muestra un diagrama con una instalación de este tipo.
Figura 13: Sistema fotovoltaico híbrido.
Considerando que el proyecto en estudio pretende aportar en la disminución del consumo de energía y que por tanto abarcará sólo una fracción de la potencia total que consume el Hospital Militar, esto es, 100 Kw, que además, los equipos que atenderá (bombas recirculadoras de climatización) funcionan las 24 hrs.
del día todo el año, que la instalación se encuentra dentro del radio urbano de Santiago y cuenta con el suministro de energía de Enel Distribución (Ex Chilectra), se ha estimado que la configuración que más aplica a los requerimientos de esta instalación de tipo industrial es la configuración “on grid”, de inyección directa a la red, que es la alternativa que se evaluará."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.4. COMPONENTES DE UNA INSTALACIÓN FOTOVOLTAICA,"De acuerdo a la configuración “on grid” determinada, en este apartado se describen los principales dispositivos que forman parte de dicha instalación fotovoltaica:"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.4.1. PANELES FOTOVOLTAICOS,"Son los dispositivos que se encargan de convertir la energía proveniente del sol en energía eléctrica utilizable.
Pueden ser conectados en serie y/o en paralelo al igual que las celdas que los componen.
La cantidad de paneles determina el voltaje y la potencia del conjunto de paneles, también llamado generador fotovoltaico.
Este sistema, por sus características, entrega corriente continua en su salida, la cual debe ser transformada a corriente alterna para poder ser utilizada en la red de consumo."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.4.2. INVERSOR,"Es el equipo que se encarga de transformar la corriente continua en corriente alterna, tanto para inyectar energía a la red pública como para la red de consumo que utiliza energía alterna.
Existen para requerimientos monofásicos (220V) como también para trifásicos (380V).
Los principales parámetros de un inversor son la potencia nominal, el voltaje y corrientes de operación (tanto de entrada como de salida), la frecuencia de trabajo y la eficiencia.
La potencia nominal corresponde a la potencia que entrega el inversor a la salida, la cual es consumida por los equipos conectados o inyectada a la red pública.
Esta potencia es menor que la potencia pico del generador fotovoltaico, debido a que difícilmente los paneles proporcionan la potencia pico de diseño.
Además, este parámetro determina el tamaño físico del inversor.
El voltaje de entrada señala el voltaje máximo que el inversor puede recibir del generador fotovoltaico.
Por otra parte, el voltaje de salida es el voltaje que este dispositivo entrega a la red de consumo o a la red pública, que puede ser monofásica (220V) o trifásica (380V).
La corriente de operación indica el valor máximo de intensidad de corriente que el inversor puede recibir del generador fotovoltaico (corriente de entrada) y entregar a la red pública o a los consumos asociados (corriente de salida).
La frecuencia de trabajo debe ser equivalente a la frecuencia de la red eléctrica pública, que en el caso de Chile corresponde a 50 Hz.
La eficiencia es el parámetro que relaciona la potencia de salida de un inversor con la potencia entregada por el generador fotovoltaico.
Por tanto, a medida que esta última varía, la eficiencia del inversor sufre variaciones, lo cual se refleja en las curvas de eficiencia que cada fabricante entrega."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,4.4.3. MEDIDOR DE ENERGÍA,"Es un aparato que contabiliza la energía eléctrica producida por la instalación fotovoltaica, ya sea monofásica o trifásica.
Existen de tipo unidireccional y bidireccional.
En el primer caso, es el medidor que se utiliza regularmente en las instalaciones domiciliarias e industriales para contabilizar la energía consumida de la red pública.
En el segundo caso, el medidor bidireccional, es el dispositivo que se debe utilizar en las instalaciones fotovoltaicas para cumplir con la ley N° 20.571 para la Generación Distribuida, también llamada ley de Net Billing.
En este último caso, este equipo permitirá no solo medir la energía producida por el sistema fotovoltaico y restarla de la energía total consumida por el usuario, sino también contabilizar el excedente de energía que el usuario no utilice durante el día y que se inyecte a la red pública, con el fin de recibir ingresos por parte de la distribuidora de energía por concepto de generación y con ello reducir su facturación eléctrica, que es en definitiva, el espíritu de dicha ley."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,5. ESCENARIO ACTUAL DEL PAÍS,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,5.1. LEGISLACIÓN DE ENERGÍAS RENOVABLES NO CONVENCIONALES (ERNC) EN CHILE,"El 22 de marzo de 2012 se publicó en el Diario Oficial la Ley N° 20.571 de Generación Distribuida, la cual regula el pago de las tarifas eléctricas de las generadoras residenciales (Ley de Net Billing), en virtud de la cual se permite tanto a los clientes eléctricos residenciales como también a pequeñas y medianas empresas, a generar energía para consumo propio, inyectar al sistema eléctrico los excedentes de energía no consumida y recibir pagos o compensaciones vía descuentos en su factura mensual por dicha inyección por parte de las empresas distribuidoras.
Esta ley de Net Billing entró en vigencia el 22 de Octubre de 2014, una vez que la Contraloría General de la República tomó razón del reglamento que permite su aplicación.
A dicho cuerpo legal se somete todo sistema de generación de energía eléctrica en base a fuentes de energía renovable no convencional (ERNC) o a instalaciones de cogeneración eficiente, cuyas potencias instaladas, en ambos casos, sean menores o iguales a 100 KW.
La instalación eléctrica debe cumplir, en todo caso, con las exigencias y requisitos establecidos en legislación eléctrica y normativa técnica aplicable vigente en el país.
Considerando el alcance de este trabajo que pretende cubrir una proporción del consumo eléctrico total del Hospital Militar, la presente ley de Net Billing, si bien podría ser utilizada, dado que se cumple con la potencia máxima que se permite, la producción total generada por este sistema fotovoltaico será consumido en su totalidad por las bombas recirculadoras de climatización, por lo cual, este marco legal no tendría aplicación práctica en este caso particular, por lo que no se entrará en los aspectos y detalles que esta ley regula."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,5.2. INCENTIVOS PARA EL DESARROLLO DE PROYECTOS EN BASE A ERNC,"Los proyectos ERNC requieren una importante cantidad de recursos financieros, por lo que su viabilidad mejora sustancialmente al contar con incentivos externos para su financiamiento.
Para tales fines, existen en la actualidad un conjunto de organismos, tanto gubernamentales como privados, que ofrecen fondos concursables para realizar proyectos que incorporen innovación, lo cual permite a las empresas buscar soluciones para sus procesos.
Algunos fondos importantes que proporcionan recursos son los siguientes:
 | Institución que entrega los fondos | Nombre del concurso | Detalles de la cobertura de los subsidios
 | --- | --- | ---
 | CER | Concurso para estudios de preinversión y etapas avanzadas de proyectos de generación eléctrica. |  Hasta 40% financiamiento de estudio de pre inversión.  Tope máximo de 1.000 UF
 | Corfo | Capital Semilla |  Hasta 75% financiamiento del costo del proyecto.  Tope máximo de $40.000.000.-
 | FIA | Proyectos de Inversión para la Innovación |  Aporte máximo por proyecto: $100.000.000.-  Plazo ejecución máx: 18 meses.
 | Comisión Nacional de Riego | ERNC Nacional I y II. |  Monto total a repartir de $750.000.000.-
 | Conicyt | Programa de I+D en acción - Fondef |  Hasta 70% financiamiento del costo del proyecto.  Tope máximo de $120.000.000.-
 | Fundación Copec - Universidad Católica. | Concurso de I+D |  Tope máximo de 4.000 UF.-

Tabla 1: Incentivos (Fuente: CER)
Tabla 1: Incentivos (Fuente: CER)"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6. ESTUDIO TÉCNICO,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.1. DESCRIPCIÓN DEL PROYECTO,"Tal como se mencionó anteriormente, el proyecto considera inyectar electricidad generada a través de la energía proporcionada por el sol, para atender los consumos de las bombas recirculadoras de climatización, las cuales están ubicadas en un sexto piso del Hospital Militar de Santiago, denominado “piso mecánico” y cuyo detalle se muestra en la tabla siguiente:
 | Item | Bomba | Potencia motor (KW - 380V) | Corriente consumida en funcionamiento (Amp.) | Condición de funcionamiento | Pot. Diaria Consumida (KW - 380V)
 | --- | --- | --- | --- | --- | ---
 | 1 | BAEP 1 | 22,5 | 0 | DETENIDA | 
 | 2 | BAEP 1R | 22,5 | 0 | DETENIDA | 
 | 3 | BAEP 2 | 22,5 | 32 | OPERATIVA | 16,8
 | 4 | BAC 1 | 45 | 42 | OPERATIVA | 22,1
 | 5 | BAC 1R | 45 | 0 | DETENIDA | 
 | 6 | BAC 2 | 45 | 0 | DETENIDA | 
 | 7 | BAES 1 | 37,5 | 39 | OPERATIVA | 20,5
 | 8 | BAES 1R | 37,5 | 0 | DETENIDA | 
 | 9 | BAES 2 | 30 | 0 | DETENIDA | 
 | 10 | BAES 2R | 30 | 32 | OPERATIVA | 16,8
 | 11 | BAES 3 | 22,5 | 36 | OPERATIVA | 19,0
 | 12 | BAES 3R | 22,5 | 0 | DETENIDA | 
 | 13 | BAES 4 | 11,25 | 5,5 | OPERATIVA | 2,9
 | 14 | BAES 4R | 11,25 | 0 | DETENIDA | 
 | 15 | BAES 5 | 5,625 | 0 | DETENIDA | 
 | 16 | BAES 5R | 5,625 | 2,5 | OPERATIVA | 1,3
 |  | Pot. Total Instalada | 416,25 |  | Pot. Total Consumida | 99,5

Tabla 2: Bombas Recirculadoras de Climatización. (Fuente: Elaboración propia).
Los datos del cuadro anterior fueron obtenidos con mediciones “in situ” efectuadas con la pinza amperimétrica digital Fluke, modelo 376, True RMS Clamp Meter.
Tabla 2: Bombas Recirculadoras de Climatización. (Fuente: Elaboración propia).
Los datos del cuadro anterior fueron obtenidos con mediciones “in situ” efectuadas con la pinza amperimétrica digital Fluke, modelo 376, True RMS Clamp Meter.
Características:
 Rango Corriente CA: 999,9 A (Precisión 2%)
 Rango Corriente CC: 999.9 A (Precisión 2%)
 Tensión CA: 1000,0 V (Precisión 2%)
 Tensión CC: 1000 V (Precisión 2%)
 Resistencia: 60.000 Ohm (Precisión 1%)
 Frecuencia: 5 – 500 Hz
 Continuidad: <= 30 Ohm
 Respuesta CA: Verdadero valor eficaz
Fotografía 1: Multímetro
La configuración original del proyecto consideró dotar a cada circuito y/o equipo de climatización de una bomba principal y otra de respaldo para el reemplazo de la primera en caso de falla.
En tal sentido, del total de 16 bombas instaladas, sólo 7 funcionan de forma permanente, las cuales suman una potencia consumida de 99,5 KW, que actualmente es suministrada por la red eléctrica pública de la empresa Enel Distribución (Ex Chilectra).
Para efectos prácticos de dimensionamiento la potencia consumida se aproximará a 100 KW.
La elección de estos equipos para dotarlos de energía generada por paneles fotovoltaicos, fue determinada considerando no sólo que estos equipos funcionan todos los días del año, durante las 24 hrs., lo que genera una ventaja importante ya que existe un consumo constante, sino también, por consideraciones de infraestructura, debido a que estos equipos están ubicados inmediatamente debajo de la techumbre, muy cercanos al único lugar disponible para la ubicación de paneles solares, dado que dentro del emplazamiento del Hospital no hay otros lugares con espacio físico suficiente y cercano a las fuentes de consumo que puedan ser utilizados para estos fines.
Las siguientes fotografías permiten apreciar la instalación actual de las bombas recirculadoras que serán atendidas por este sistema fotovoltaico:
Fotografía 1: Multímetro
La configuración original del proyecto consideró dotar a cada circuito y/o equipo de climatización de una bomba principal y otra de respaldo para el reemplazo de la primera en caso de falla.
En tal sentido, del total de 16 bombas instaladas, sólo 7 funcionan de forma permanente, las cuales suman una potencia consumida de 99,5 KW, que actualmente es suministrada por la red eléctrica pública de la empresa Enel Distribución (Ex Chilectra).
Para efectos prácticos de dimensionamiento la potencia consumida se aproximará a 100 KW.
La elección de estos equipos para dotarlos de energía generada por paneles fotovoltaicos, fue determinada considerando no sólo que estos equipos funcionan todos los días del año, durante las 24 hrs., lo que genera una ventaja importante ya que existe un consumo constante, sino también, por consideraciones de infraestructura, debido a que estos equipos están ubicados inmediatamente debajo de la techumbre, muy cercanos al único lugar disponible para la ubicación de paneles solares, dado que dentro del emplazamiento del Hospital no hay otros lugares con espacio físico suficiente y cercano a las fuentes de consumo que puedan ser utilizados para estos fines.
Las siguientes fotografías permiten apreciar la instalación actual de las bombas recirculadoras que serán atendidas por este sistema fotovoltaico:"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.2. CONCEPTOS SOLARES,"Le energía proveniente del sol se transmite a la Tierra por medio de ondas denominada radiación, cuyo concepto se divide en varios tipos que se definen a continuación:"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.2.1. RADIACIÓN SOLAR,"Se define como el flujo de energía proveniente del sol en forma de ondas electromagnéticas de diferentes frecuencias, las cuales se generan en el proceso de fusión nuclear que ocurre en su interior.
Debido a la gran distancia que existe entre la Tierra y el Sol, la radiación solar que llega a la superficie terrestre es sólo una proporción muy menor de la radiación total que emite.
Dicho valor se denomina constante solar y su valor medio aproximado es de
1.367 W/m2.
De la radiación total que recibe la Tierra, aproximadamente la mitad de ella se encuentra en el rango entre 0.4μm y 0.7μm.
Esta radiación puede ser detectada por el ojo humano, y es lo que se conoce como luz visible.
De la otra mitad del espectro de radiación, la mayoría de esta se sitúa en el espectro infrarrojo y una pequeña parte se ubica en el espectro ultravioleta.
Cuando la radiación llega al tope de la atmósfera terrestre sufre una serie de cambios antes de llegar a la superficie, motivo por el cual se presenta en diferentes formas que se definen a continuación:
 Radiación: Se define como la potencia de radiación solar por unidad de superficie y se expresa en W/m2.
 Irradiación: Es la energía que incide por unidad de superficie en un tiempo determinado y se expresa en J/m2 ó por consideraciones prácticas se utiliza también la unidad kWh.
 Radiación espectral: Se define como la potencia de radiación solar por unidad de superficie y de longitud de onda.
Se expresa en W/(m2 ∙µm)
 Radiación directa: Es aquella radiación que proviene directamente del Sol, que no sufre cambios en su dirección y que se caracteriza por proyectar una sombra definida.
Se mide en W/m2.
 Radiación difusa: Es aquella fracción de la radiación solar que es reflejada o absorbida por las nubes y que también se ve afectada por las partículas de polvo, árboles, edificios y el suelo entre otras.
Debido a ello, su dirección es variable y a diferencia de la radiación directa, no produce sombra.
Se mide en W/m2.
 Radiación reflejada: Es la radiación solar que rebota en el suelo u otra superficie, cuya magnitud depende del coeficiente de reflexión de dicha superficie.
Se denomina también Albedo.
Se mide en W/m2.
 Radiación global: Se define como la suma total de las radiaciones directa, difusa y reflejada que llega a un determinado lugar.
Se mide en W/m2.
La radiación solar se ve afectada también, en su paso por la atmósfera, por el efecto que produce la absorción de la radiación por las moléculas que la componen como son el O3, H2O, O2, CO2, entre otros, como también por la difusión de la radiación que producen las moléculas de aire y otros componentes, incluidos los aerosoles.
En la Figura 14, se pueden apreciar los diferentes tipos de radiación solar que llegan a la Tierra: Figura 14: Tipos de radiación solar.
1.367 W/m2.
De la radiación total que recibe la Tierra, aproximadamente la mitad de ella se encuentra en el rango entre 0.4μm y 0.7μm.
Esta radiación puede ser detectada por el ojo humano, y es lo que se conoce como luz visible.
De la otra mitad del espectro de radiación, la mayoría de esta se sitúa en el espectro infrarrojo y una pequeña parte se ubica en el espectro ultravioleta.
Cuando la radiación llega al tope de la atmósfera terrestre sufre una serie de cambios antes de llegar a la superficie, motivo por el cual se presenta en diferentes formas que se definen a continuación:
 Radiación: Se define como la potencia de radiación solar por unidad de superficie y se expresa en W/m2.
 Irradiación: Es la energía que incide por unidad de superficie en un tiempo determinado y se expresa en J/m2 ó por consideraciones prácticas se utiliza también la unidad kWh.
 Radiación espectral: Se define como la potencia de radiación solar por unidad de superficie y de longitud de onda.
Se expresa en W/(m2 ∙µm)
 Radiación directa: Es aquella radiación que proviene directamente del Sol, que no sufre cambios en su dirección y que se caracteriza por proyectar una sombra definida.
Se mide en W/m2.
 Radiación difusa: Es aquella fracción de la radiación solar que es reflejada o absorbida por las nubes y que también se ve afectada por las partículas de polvo, árboles, edificios y el suelo entre otras.
Debido a ello, su dirección es variable y a diferencia de la radiación directa, no produce sombra.
Se mide en W/m2.
 Radiación reflejada: Es la radiación solar que rebota en el suelo u otra superficie, cuya magnitud depende del coeficiente de reflexión de dicha superficie.
Se denomina también Albedo.
Se mide en W/m2.
 Radiación global: Se define como la suma total de las radiaciones directa, difusa y reflejada que llega a un determinado lugar.
Se mide en W/m2.
La radiación solar se ve afectada también, en su paso por la atmósfera, por el efecto que produce la absorción de la radiación por las moléculas que la componen como son el O3, H2O, O2, CO2, entre otros, como también por la difusión de la radiación que producen las moléculas de aire y otros componentes, incluidos los aerosoles.
En la Figura 14, se pueden apreciar los diferentes tipos de radiación solar que llegan a la Tierra: Figura 14: Tipos de radiación solar."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.2.2. PRINCIPALES VARIABLES UTILIZADAS EN LA TECNOLOGÍA FOTOVOLTAICA,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.2.3. INFORMACIÓN DE RADIACIÓN SOLAR EN SANTIAGO,"A continuación se muestran los datos de radiación promedio para la ciudad de Santiago, tomando en cuenta una base de datos con información recopilada durante 10 años.
Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic Radiación promedio (KWh/m2*día) 8,92 7,96 6,58 4,65 3,07 2,35 2,65 3,37 4,72 6,33 7,98 8,89
Tabla 3: Radiación mensual.
(Fuente: fcfm, Geofísica U. Chile, elaboración propia)"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.2.4. INCLINACIÓN DE LOS PANELES FOTOVOLTAICOS,"La inclinación de los paneles es uno de los parámetros más relevantes dentro del diseño de un sistema de generación fotovoltaica, ya que de ella depende la optimización del aprovechamiento de la radiación solar y consecuentemente con ello la producción de energía.
Considerando que en las diferentes estaciones del año la posición del sol varía, la inclinación de los paneles respecto del sol tiene directa incidencia en el aprovechamiento de la radiación solar.
Esto significa que en la época de verano el ángulo de inclinación es menor, dada la ubicación más vertical que tiene el sol, situación que en invierno cambia ya que el sol está en una posición más baja respecto del horizonte y en este caso, el ángulo de inclinación es mayor.
Dado este escenario, el mayor aprovechamiento de la energía solar se logra con un ángulo de inclinación intermedio, el cual, para el caso del hemisferio sur se considera aceptable como criterio de inclinación, la latitud del lugar, lo que permite aprovechar de la manera más uniforme posible el sistema fotovoltaico durante todo el año, que en este caso el ángulo se aproximaría a 30°."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.2.5. AZIMUT,"Para el caso del hemisferio sur, el ángulo que optimiza el rendimiento del sistema fotovoltaico es de 0°, es decir, orientado al Norte."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3. DIMENSIONAMIENTO Y SELECCIÓN DE EQUIPOS,"Consecuente con la tipología de sistemas descrita en el apartado 4.3 anterior, el sistema fotovoltaico que plantea este trabajo considera inyectar la energía generada directamente al consumo de las bombas recirculadoras de climatización, es decir, una instalación On Grid.
Para cumplir con ello, el sistema en líneas generales contempla la incorporación de los paneles fotovoltaicos, encargados de generar la energía, el inversor, que transforma la corriente continua en alterna y además un regulador de voltaje, que es el dispositivo eléctrico que recibe una tensión variable a la entrada, dentro de un parámetro predeterminado y lo mantiene constante a la salida de este dispositivo."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.1. CONSUMO DE ENERGÍA,"Dadas las características de funcionamiento de las bombas de climatización, que operan las 24 hrs.
del día los 365 días del año y las potencias individuales de cada una de ellas, el consumo total de energía se estima en 859.824 KWH / año (Tabla 4)."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.2. SUPERFICIE DISPONIBLE,"Para la instalación de los paneles fotovoltaicos se cuenta con la superficie de techumbre del Edificio A, la cual se muestra en la Figura 16. En ella se cuenta con lucarnas que permiten aprovechar la luz natural en los patios interiores, motivo por el cual no pueden ser utilizadas para instalación de paneles fotovoltaicos.
Si bien existen elementos adicionales, como las descargas de ductos de climatización, ventilaciones de baños y equipos de climatización entre otros, que disminuyen la superficie disponible, existe un remanente de superficie superior a los 1.000 m2, que puede ser utilizada.
Figura 16: Planta de cubierta Edificio A.
(fuente: Google Earth)"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.3. DIMENSIONAMIENTO DE PANELES FOTOVOLTAICOS,"Tal como se señaló anteriormente, para la selección de los paneles se cuenta con la potencia nominal a satisfacer de 100 KW, equivalente a la potencia del grupo de bombas.
Las principales características para seleccionar los paneles fotovoltaicos a utilizar son la calidad, garantía de funcionamiento y su eficiencia, con el fin de garantizar la correcta operación del sistema a largo plazo.
Considerando estos atributos y la oferta existente en el mercado nacional, se escogió los paneles CSun, pues ofrece productos con las características antes mencionadas, siendo el modelo a utilizar el CSUN 270-60M (Anexo 10.5), el cual cumple no sólo con el requerimiento de la carga, sino que también cuenta con un peso y dimensiones razonables para manejarlo en la etapa de montaje.
Teniendo en cuenta la potencia a satisfacer de 100 KW, la potencia nominal del panel seleccionado (270 Wp) y utilizando la ampliamente aceptada herramienta de dimensionamiento fotovoltaico PVSYST V5.74 se obtiene como resultado una cantidad total de 460 paneles fotovoltaicos a instalar, los que en su conjunto totalizan una potencia global nominal generada de 124 KWp, que en términos de potencia efectiva entregada al sistema de bombas totaliza los 100 KW considerados.
La eficiencia del sistema que involucra a su vez la eficiencia de los paneles (16,63%), inversores (98%) y otros elementos adicionales como cableado, interruptores y conexiones entre otros (80%), genera una eficiencia del sistema del orden de un 80,6%."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.4. DIMENSIONAMIENTO DE INVERSOR,"En términos generales, un inversor es un convertidor que transforma la corriente continua producida por los paneles fotovoltaicos en corriente alterna útil.
Es el cerebro del sistema fotovoltaico y por ello, debe cumplir con la normativa SEC. Se clasifican en los de tipo aislados y los conectados a la red, que son los que se utilizarán en el presente trabajo.
Considerando la potencia máxima proyectada de 100 KW en 380V y teniendo presente la condición de Hospital de este edificio, se ha estimado necesario asegurar la producción y suministro de energía, para lo cual se contempla utilizar cinco inversores, que cubran cada uno un 20% aproximadamente de la potencia total a cubrir.
Con ello, ante la eventual falla de alguno de los inversores, los restantes continuarán produciendo electricidad.
Para este fin se escogió el modelo TRIO 20,0-TL-OUTD de la marca ABB, el cual admite una potencia individual de 20 KW (Anexo 10.6) y que dentro de las alternativas existentes actualmente en el mercado, es un equipo que tiene buenas referencias en términos de durabilidad, eficiencia, modularidad y facilidad de mantenimiento, además de cumplir con la normativa SEC. Dentro de la tecnología que posee este equipo, adicionalmente cuenta con un algoritmo para rastrear el MPPT (Maximum Power Point Tracking) o también llamado “Seguidor de Punto de Máxima Potencia”, lo que le permite seguir el comportamiento dinámico de la planta fotovoltaica en tiempo real, y al hacer esto, conseguir maximizar la producción de energía suministrada a los equipos, en este caso, a las bombas recirculadoras de climatización."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.5. CABLEADO DE LA INSTALACIÓN,"En el diseño del generador fotovoltaico se debe considerar que por cada string de paneles no circulará corriente proveniente de otros strings.
Por tal motivo se instalarán fusibles de protección que consideran un factor de seguridad de un 20% sobre la corriente máxima que puede circular por cada string y que para este caso el fusible de 10 [A] en cada polo (positivo y negativo), es adecuado para el requerimiento.
Para el dimensionamiento de los conductores, se han tomado en cuenta las relaciones de caída de tensión máxima admisible y la información disponible en las tablas de intensidad de corriente admisible para conductores aislados contenidas en la norma NCh 4/2003 y/o las entregadas por los fabricantes.
Para el cálculo de los alimentadores se consideran las demandas máximas que soportarán.
Por tanto, considerando las potencias nominales de los consumos y teniendo en cuenta el escenario más desfavorable, se considerará que la demanda máxima (Dmáx) es equivalente a la potencia instalada (Pi), esto es, con factor de demanda (FD) igual a 1. La relación entre ambas queda reflejada en la ecuación:
Dmáx = FD * Pi [W]
Conforme a lo anterior, la corriente que transportará el alimentador será:
Dmáx = FD * Pi [W]
Conforme a lo anterior, la corriente que transportará el alimentador será:
𝐼 =
Dmáx √3∗𝑉∗𝑐𝑜𝑠ф [A]
Para las canalizaciones de los alimentadores y conductores se considerará lo establecido por la norma NCh 4/2003 para este tipo de instalaciones en baja tensión."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.6. DISPOSICIÓN Y CONFIGURACIÓN DE LA INSTALACIÓN,"De acuerdo a los espacios disponibles en la cubierta del Edificio A del Hospital Militar, a continuación se muestra la disposición proyectada de los paneles fotovoltaicos del sistema: Figura 17: Disposición paneles fotovoltaicos en cubierta Edificio A.
(Fuente: Google Earth y elaboración propia) De acuerdo a los requerimientos de esta instalación y a los resultados obtenidos con la herramienta PVSYST 5.74, la configuración de los módulos fotovoltaicos queda determinada de la siguiente manera: se conectarán 23 paneles en serie por cada string y en paralelo se conectarán un máximo de 20 strings."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.3.7. PRODUCCIÓN DE ENERGÍA DEL SISTEMA,"La producción de energía de la planta fotovoltaica considerada, se aprecia en el siguiente cuadro:
 |  | Producción de energía del sistema (KWh / mes) | Consumo de energía bombas (KWh / mes) | Aporte sistema fotovoltaico
 | --- | --- | --- | ---
 | Ene | 23.730 | 71.652 | 33,1%
 | Feb | 20.720 | 71.652 | 28,9%
 | Mar | 21.380 | 71.652 | 29,8%
 | Abr | 18.050 | 71.652 | 25,2%
 | May | 14.190 | 71.652 | 19,8%
 | Jun | 12.370 | 71.652 | 17,3%
 | Jul | 13.840 | 71.652 | 19,3%
 | Ago | 16.160 | 71.652 | 22,6%
 | Sep | 18.270 | 71.652 | 25,5%
 | Oct | 20.980 | 71.652 | 29,3%
 | Nov | 22.290 | 71.652 | 31,1%
 | Dic | 23.590 | 71.652 | 32,9%
 | Total | 225.570 | 859.824 | 26,2%

Tabla 4: Producción de energía y aporte del sistema fotovoltaico (Fuente: elaboración propia)
Gráficamente se aprecia la producción mensual:
Producción de energía
25.000
20.000
15.000
10.000
5.000
0 Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic MESES
Gráfico 1: Producción mensual estimada de energía por año (Fuente: Elaboración propia) Del cuadro anterior se desprende que el total de energía producida por la planta fotovoltaica es de 225.570 KWh para el primer año de operación, lo que equivale a un aporte promedio del orden de un 26,2% del total de energía consumida por las bombas recirculadoras.
A partir del segundo año de operación y en los siguientes, la producción de la planta se reduce en aproximadamente un 0,7% anual, correspondiente al valor promedio de disminución de rendimiento anual de los paneles fotovoltaicos.
Por ello y considerando este escenario, se calculó una proyección de rendimiento el cual se resume en la siguiente tabla:
 | Año | Año | Rendimiento Producción Energía (KWh)
 | --- | --- | ---
 | 2017 | 1 | 100% 225.570
 | 2018 | 2 | 99,3% 223.991
 | 2019 | 3 | 98,6% 222.434
 | 2020 | 4 | 97,9% 220.899
 | 2021 | 5 | 97,2% 219.385
 | 2022 | 6 | 96,5% 217.892
 | 2023 | 7 | 95,9% 216.419
 | 2024 | 8 | 95,2% 214.967
 | 2025 | 9 | 94,5% 213.534
 | 2026 | 10 | 93,9% 212.121
 | 2027 | 11 | 93,2% 210.727
 | 2028 | 12 | 92,6% 209.352
 | 2029 | 13 | 91,9% 207.996
 | 2030 | 14 | 91,3% 206.657
 | 2031 | 15 | 90,6% 205.337
 | 2032 | 16 | 90,0% 204.034
 | 2033 | 17 | 89,4% 202.749
 | 2034 | 18 | 88,7% 201.480
 | 2035 | 19 | 88,1% 200.229
 | 2036 | 20 | 87,5% 198.994

Tabla 5: Rendimiento y producción de energía del sistema fotovoltaico (Fuente: elaboración propia)"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.4. ELEMENTOS COMPLEMENTARIOS PARA LA INSTALACIÓN FOTOVOLTAICA,
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.4.1. MEDIDORES DE ENERGÍA,"La producción de energía del generador fotovoltaico será controlada por un medidor de energía, el que estará ubicado en el punto de inyección a la red de consumo, en este caso, en el tablero eléctrico de las bombas recirculadoras de climatización."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,6.4.2. PROTECCIONES,"Por el lado del circuito de corriente continua, la protección contra cortocircuitos se hará con 2 fusibles de 10 [A] para cada serie de paneles; la protección contra sobrecorriente se hará por el mismo fusible, debido a que este será del tipo gL / gG.
La protección contra el contacto humano directo e indirecto se hará a través de la conexión adecuada de los paneles fotovoltaicos y sus estructuras de montaje a la tierra.
Dados los avances tecnológicos en el campo de los inversores, los equipos seleccionados para este proyecto cuentan con varias protecciones integradas, tales como, polaridad inversa, sobretensión y falla de aislamiento.
Por el lado del circuito de corriente alterna, la instalación fotovoltaica tendrá protección contra cortocircuito, sobrecorriente, fallas a tierra y/o de red, las cuales serán realizadas por los inversores seleccionados para este proyecto en conjunto con los interruptores asociados."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7. ANÁLISIS ECONÓMICO,"El estudio técnico permitió determinar la producción de energía y requerimientos de equipamiento e infraestructura que demanda la instalación fotovoltaica proyectada.
Por ello, corresponde ahora valorizar todos estos recursos para poder efectuar posteriormente el análisis económico respectivo."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.1. ANÁLISIS FODA,"En este apartado se presenta un análisis cualitativo, tanto interno como externo del proyecto revisando sus fortalezas, oportunidades, debilidades y amenazas:
 | FORTALEZAS | OPORTUNIDADES
 | Aprovechamiento de la energía solar abundante, inagotable, limpia y sustentable. | Potencial solar en Chile es alto.
 | Tecnología fotovoltaica está madura y probada, lo que da seguridad y confianza. | Alza continua de los precios de la energía.
 | Buena percepción social de la energía solar fotovoltaica. | Espacio físico disponible.
 | Bajos costos de mantenimiento
 | Mejorar la imagen externa del Hospital y del Ejército de Chile ante la comunidad.
 | DEBILIDADES | AMENAZAS
 | Alto costo de la inversión inicial. | Daños físicos a los paneles fotovoltaicos.
 | Paneles fotovoltaicos no se producen en Chile. | Alta necesidad de conocimiento eléctrico.

La radiación solar sólo está presente durante un período de tiempo y de distinta intensidad durante el día.
Si bien la radiación tiene un comportamiento característico en el tiempo, el clima es un elemento que no es posible manejar."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,"7.2. PRODUCCIÓN DE ENERGÍA, AHORROS Y COSTOS","Para analizar la viabilidad de este proyecto es imprescindible determinar la producción de energía estimada de la instalación fotovoltaica diseñada.
Consecuente con ello, y tal como se estimó en la Tabla 4 del ítem 6.3.7 anterior, la producción de energía esperada para esta instalación fotovoltaica es 225.570 KWh en el primer año de operaciones y en la Tabla 5 se presentó la proyección de producción de energía para los años siguientes.
Es importante resaltar que en este proyecto no se tienen ingresos por venta de energía, sino que sólo beneficios debido al ahorro que significa no utilizar energía de la red pública para el funcionamiento de las bombas de climatización."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.3. PRESUPUESTO DE EQUIPAMIENTO,"Para la estimación de costos de los equipos que involucra un proyecto de estas características, se ha tomado en consideración un costo promedio de US$ 1,80/Wp, valor que actualmente es utilizado por diversas empresas del rubro fotovoltaico presentes en el mercado nacional.
Otras consideraciones que se tuvieron presente para la estimación de costos se indican a continuación:
 Costo de paneles fotovoltaicos de un 81% del costo total de inversión.
 Costo de inversores de un 5,4% del costo total de inversión.
 Costo de cableado de un 2% del costo total de inversión.
 Costo de estructura de montaje de un 10% del costo total de inversión.
 Costo de tablero eléctrico de 1,6% del costo total de inversión.
 Costos de operación y mantenimiento de un 5% de los ahorros.
 Vida útil de paneles e inversores de 20 años."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.3.1. EQUIPAMIENTO,"De acuerdo a lo señalado anteriormente, los costos estimados de equipamiento se resumen en el siguiente cuadro:
 | Item 1 | Equipamiento Paneles | Unidad Un. | Cantidad 460 | Costo Total ($) 143.396.979
 | --- | --- | --- | --- | ---
 | 2 | Inversores | Un. | 5 | 9.595.738
 | 3 | Cableado | Gl. | 1 | 3.541.440
 | 4 | Estructura montaje | Gl. | 1 | 17.707.200
 | 5 | Tablero eléctrico | Gl. | 1 | 2.830.642
 |  |  |  | Costo total instalación | 177.072.000

Tabla 6: Inversión inicial (Fuente: elaboración propia)"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.3.2. AHORROS (INGRESOS) Y COSTOS OPERATIVOS,"Los ahorros anuales que se generan debido a la disminución de consumo de la red eléctrica pública y los costos operativos asociados a este sistema en base a la producción de energía expresada en la Tabla 5 del ítem 6.3.7 anterior, dan como resultado lo siguiente:
 | Año | Año | Ahorro (Ingresos) $ Costos de Explotación | $
 | --- | --- | --- | ---
 | 2017 | 1 | 20.075.730 | 1.003.787
 | 2018 | 2 | 20.732.608 | 1.036.630
 | 2019 | 3 | 21.412.035 | 1.070.602
 | 2020 | 4 | 22.114.812 | 1.105.741
 | 2021 | 5 | 18.668.751 | 933.438
 | 2022 | 6 | 19.283.358 | 964.168
 | 2023 | 7 | 19.919.155 | 995.958
 | 2024 | 8 | 20.576.894 | 1.028.845
 | 2025 | 9 | 21.257.358 | 1.062.868
 | 2026 | 10 | 21.961.356 | 1.098.068
 | 2027 | 11 | 22.689.726 | 1.134.486
 | 2028 | 12 | 23.443.339 | 1.172.167
 | 2029 | 13 | 24.223.096 | 1.211.155
 | 2030 | 14 | 25.029.932 | 1.251.497
 | 2031 | 15 | 25.864.814 | 1.293.241
 | 2032 | 16 | 26.728.747 | 1.336.437
 | 2033 | 17 | 27.622.772 | 1.381.139
 | 2034 | 18 | 28.547.967 | 1.427.398
 | 2035 | 19 | 29.505.450 | 1.475.273
 | 2036 | 20 | 30.496.381 | 1.524.819
 |  | TOTAL | 470.154.280 | 23.507.714

Tabla 7: Ahorros y costos (Fuente: elaboración propia)"
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.3.3. INCENTIVOS,"Tal como se mencionó en el ítem 5.2 anterior, Tabla 1, existen incentivos provenientes de organismos gubernamentales que ayudan a financiar proyectos en base a Energías Renovables No Convencionales (ERNC).
Para el caso de este anteproyecto en estudio, los incentivos se resumen a continuación:
 | Incentivo | Monto $
 | FIA | 50.000.000
 | CORFO | 20.000.000
 | CNR | 35.000.000
 | TOTAL | 105.000.000

Tabla 8: Incentivos (Fuente: elaboración propia) Los montos de los incentivos por institución se estimaron tomando como base los siguientes supuestos: en el caso de la FIA un 50% del monto total máximo por proyecto, para la Corfo, un 50% del tope máximo y para el caso de la CNR, que cuenta con un monto global a repartir se consideró una asignación baja, del orden del 5% de dicho monto."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.4. EVALUACIÓN ECONÓMICA,"En este apartado se presentan los resultados de la evaluación económica de los diferentes flujos de caja para los distintos casos que se tomaron en cuenta en este trabajo, esto es, proyecto puro, proyecto con incentivo incluido y sin financiamiento externo, como también los casos de proyecto financiado, sin incentivo, pero para diferentes porcentajes de financiamiento externo.
Para las evaluaciones de los flujos de caja se consideró una tasa de descuento de un 10%, que es el valor convenido que actualmente se utiliza en Chile en las evaluaciones costo beneficio del sector eléctrico, mismo valor que también es recomendado por la Agencia Internacional de Energía para la evaluación de proyectos fotovoltaicos.
El período de evaluación de este tipo de proyectos coincide con su vida útil, la cual fluctúa entre 20 y 25 años.
Para el presente trabajo, se considerará un período de 20 años.
Otros antecedentes que deben ser tomados en consideración para estas evaluaciones, tienen que ver con que el Hospital Militar de Santiago es una entidad pública sin fines de lucro y que por dicha condición está exento del pago de impuesto de primera categoría (ver Anexo 10.11), lo que afecta los flujos de caja calculados.
Los valores obtenidos para cada caso se presentan a continuación:
 |  | TASA DE DESCUENTO | VAN | TIR | IVAN | PAYBACK (Años)
 | --- | --- | --- | --- | --- | ---
 | Proyecto puro | 10,0% | $913.261 | 10,1% | $0,01 | 9
 | Proyecto con incentivo | 10,0% | $105.913.261 | 27,3% | $1,47 | 3
 | Proyecto con financiamiento 25%, sin incentivo | 10,0% | $15.956.704 | 11,6% | $0,12 | 8
 | Proyecto con financiamiento 50%, sin incentivo | 10,0% | $31.402.874 | 14,4% | $0,35 | 7
 | Proyecto con financiamiento 75%, sin incentivo | 10,0% | $47.699.822 | 22,0% | $1,08 | 4
 | Proyecto con financiamiento 90%, sin incentivo | 10,0% | $57.057.134 | 42,3% | $3,22 | 2

Tabla 9: Resultados análisis financiero (Fuente: elaboración propia) De los resultados anteriores se aprecia que en todos los casos el indicador VAN es positivo, lo que permite afirmar que en todas las opciones analizadas el proyecto es viable, lo que se refuerza con los valores obtenidos para la tasa interna de retorno (TIR), los cuales son todos mayores que la tasa de descuento.
No obstante lo anterior, es necesario señalar que en el caso del proyecto puro, los valores obtenidos para el VAN y TIR son apenas positivos, lo cual muestra que la viabilidad de este proyecto está al límite de lo que podría ser financiado con medios propios y bajo las condiciones establecidas en la evaluación.
En relación al indicador IVAN, el proyecto que mejor rentabilidad genera por cada unidad monetaria invertida es el alternativa con 90% de financiamiento.
En el lado opuesto, el de menor rentabilidad en relación a la inversión es el caso de proyecto puro.
Respecto del período de recuperación de la inversión (Payback) se observa un rango de variación que va desde los 2 años, para el proyecto financiado al 90% hasta los 9 años para el caso del proyecto puro.
Claramente la reducción de la inversión inicial propia, disminuye este indicador, debido a que el financiamiento, mejora los flujos de caja futuros."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.5. ANÁLISIS DE SENSIBILIDAD,"Para analizar los escenarios posibles en los cuales este proyecto es viable se efectúa un análisis de sensibilidad con respecto a 3 variables que se consideran relevantes: la primera de ellas tiene que ver con el costo de los paneles fotovoltaicos, dado que es el ítem más importante en términos de costo, la segunda variable es el costo de la energía eléctrica y la tercera y última es la tasa de descuento.
Dmáx √3∗𝑉∗𝑐𝑜𝑠ф [A]
Para las canalizaciones de los alimentadores y conductores se considerará lo establecido por la norma NCh 4/2003 para este tipo de instalaciones en baja tensión."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.5.1. VARIACIÓN COSTO DE PANELES FOTOVOLTAICOS,"Un factor relevante en este tipo de proyectos tiene que ver con la variación de costos que puedan experimentar los paneles fotovoltaicos.
En los últimos 5 años los costos de estos equipos ha caído en el mercado de manera muy importante, del orden de un 80% según lo señalado por el Ministerio de Energía, lo cual ha facilitado en el año 2015 y 2016, la generación de proyectos de este tipo.
No obstante este escenario, es importante analizar una coyuntura en la cual los precios de los equipos aumentan y de esta forma poder evaluar el comportamiento de los indicadores VAN y TIR.
Los resultados se aprecian el siguiente gráfico:
Variación costo paneles fotovoltaicos
VAN TIR

 |  | TIRVAN
 | --- | ---
 | $60.000.000 | 16% 14%
 | $40.000.000  $20.000.000 | 12%
 | 10% $0
 |  | 8%
 | $-20.000.000 | 6%
 | $-40.000.000 | 4%
 | $-60.000.000 $-80.000.000 | 0% 2%

-30% -20% -10% 0% 10% 20% 30% 40%
Variación costo paneles Gráfico 2: Variación costo paneles fotovoltaicos v/s VAN y TIR
Del gráfico anterior se aprecia claramente que la tendencia del indicador VAN es decreciente conforme aumentan los precios de los paneles fotovoltaicos.
En particular este proyecto está al límite de su viabilidad con el nivel de precios actual, por lo que si la disminución de precios continúa producto de la cada vez mayor masificación de esta tecnología, el proyecto se haría más rentable.
Respecto de la TIR, se observa que tiene un comportamiento exponencial a medida que los precios aumentan.
Es interesante analizar también el efecto que la variación de precios de los paneles fotovoltaicos tiene en el Payback de este proyecto.
Los resultados se muestran en el siguiente gráfico:
VAN TIR

 |  | TIRVAN
 | --- | ---
 | $60.000.000 | 16% 14%
 | $40.000.000  $20.000.000 | 12%
 | 10% $0
 |  | 8%
 | $-20.000.000 | 6%
 | $-40.000.000 | 4%
 | $-60.000.000 $-80.000.000 | 0% 2%

-30% -20% -10% 0% 10% 20% 30% 40%
Variación costo paneles Gráfico 2: Variación costo paneles fotovoltaicos v/s VAN y TIR
Del gráfico anterior se aprecia claramente que la tendencia del indicador VAN es decreciente conforme aumentan los precios de los paneles fotovoltaicos.
En particular este proyecto está al límite de su viabilidad con el nivel de precios actual, por lo que si la disminución de precios continúa producto de la cada vez mayor masificación de esta tecnología, el proyecto se haría más rentable.
Respecto de la TIR, se observa que tiene un comportamiento exponencial a medida que los precios aumentan.
Es interesante analizar también el efecto que la variación de precios de los paneles fotovoltaicos tiene en el Payback de este proyecto.
Los resultados se muestran en el siguiente gráfico:
Payback
Años
12
10
8
6
4
2
0 -30% -20% -10% 0% 10% 20% 30% 40%
Variación costo paneles Gráfico 3: Variación costo paneles fotovoltaicos v/s Payback
Se aprecia que el aumento del precio de los paneles fotovoltaicos incrementa el valor de este indicador, lo que hace menos atractivo un proyecto de estas características.
Una disminución del orden de un 30% haría que el retorno de la inversión descendiera a 6 años de los 9 años iniciales, lo que es un dato relevante al momento de tomar la decisión de inversión."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.5.2. VARIACIÓN COSTO DE LA ENERGÍA,"La sensibilización de esta variable tiene una gran relevancia por cuanto condiciona la rentabilidad y consecuente con ello, la viabilidad del proyecto en estudio.
En los últimos 5 años y en especial a partir de fines del año 2014, durante todo al año 2015 y también 2016 se ha producido un incremento importante del costo de la energía, muy superior a la inflación anual, lo que se ha visto reflejado en la facturación que el Hospital Militar de Santiago cancela a Enel Distribución por este concepto.
A partir del valor base del costo de la energía de $89 / KWh, se analizará el comportamiento de los indicadores VAN y TIR, suponiendo un aumento de 4% anual en el costo de este insumo.
Los resultados se aprecian el siguiente gráfico:
Variación del costo de la energía
 | VAN | TIR
 | Costo energía ($/KWH)
 | TIRVAN

20,0% 18,0% 16,0% 14,0% 12,0% 10,0% 8,0% 120.000.000 100.000.000 80.000.000 60.000.000 40.000.000 20.000.000 4,0% 6,0%
2,0% 0,0% 0 89,0 93,5 98,1 107,9 118,7 130,6 143,7
Gráfico 4: Variación costo energía v/s VAN y TIR Se observa que el aumento del costo de la energía impacta positivamente en el VAN y TIR haciendo este proyecto más rentable, lo que lo torna más atractivo de implementar toda vez que las alzas en el costo de la energía continuarán el mediano plazo.
Al examinar el efecto del alza de esta variable en el Payback se puede apreciar el siguiente resultado:
Payback
Años
10 9
8
7 6 5
4
3 2 1 0 89,0 93,5 98,1 107,9 118,7 130,6 143,7
Costo energía ($/KWH)
Gráfico 5: Variación costo energía v/s Payback Se advierte una clara tendencia decreciente en el tiempo de recuperación de la inversión, lo que es muy positivo para el Hospital Militar de Santiago, de 9 años iniciales se podría llegar a 5 años, disminución que es coherente con el alza de los indicadores VAN y TIR debido al alza del costo de la energía."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,7.5.3. VARIACIÓN TASA DE DESCUENTO,"Se considera pertinente analizar los efectos que tiene la variación de la tasa de descuento en la viabilidad de este proyecto, a partir del valor de 10% utilizado en la evaluación financiera.
El siguiente gráfico muestra los resultados:
Variación tasa de descuento
VAN
$60.000.000
$40.000.000
$20.000.000
$0
$-20.000.000
$-40.000.000
$-60.000.000 7% 8% 9% 10% 11% 12% 13% 14% 15%
Variación Tasa de Descuento Gráfico 6: Variación tasa de descuento v/s VAN Se aprecia que la variación de la tasa de descuento, al alza o a la baja, afecta la rentabilidad del proyecto haciéndolo viable o inviable respectivamente.
A partir de valores superiores al 10% el VAN se torna negativo, lo que hace inviable el proyecto."
ESTUDIO TÉCNICO ECONÓMICO PARA EL USO DE GENERACIÓN ELÉCTRICA FOTOVOLTAICA EN EL SISTEMA DE CLIMATIZACIÓN DEL HOSPITAL MILITAR DE SANTIAGO,8. CONCLUSIONES,"En la actualidad, el mundo está tomando cada vez más conciencia de la necesidad de cuidar el planeta.
Con esa orientación, la búsqueda de nuevas formas de generación de energía que sean amigables con el medio ambiente, renovables, que no dependan de los hidrocarburos y que permitan reducir los costos de generación para no afectar el crecimiento del país, se ha convertido en un imperativo de nuestra sociedad, especialmente en Chile, en el cual en estos últimos 5 años se ha producido un crecimiento muy importante en el desarrollo de proyectos de ERNC, especialmente con el uso de las tecnologías fotovoltaica y eólica.
El alza de los precios de la energía que se ha hecho sentir en todas las áreas económicas especialmente en los años 2015 y 2016, producto de la falta de definición de una política gubernamental clara los años anteriores, ha contribuido a la búsqueda de nuevas opciones de generación de menor costo.
Esta coyuntura, sumada a la política del Ejército de Chile de contribuir al desarrollo del país y la cohesión social, insertándose en la sociedad como una institución responsable con el medioambiente y alineada con una de sus áreas estratégicas “Ejército y Sociedad”, genera un escenario propicio para la creación e implementación de proyectos alternativos de generación eléctrica, como son los de tipo fotovoltaico, con lo cual se aporta a la meta gubernamental que establece que al año 2025, el 20% del generación de energía provenga de fuentes Renovables No Convencionales (ERNC).
En virtud de lo anterior, el presente trabajo de titulación, ha dimensionado y evaluado un sistema de generación fotovoltaico de 100 KW de potencia, determinando el tipo de instalación y el costo asociado a ello, para atender a las bombas recirculadoras que forman parte del sistema de climatización del Hospital Militar de Santiago, mediante un sistema de inyección directa (on grid), aportando un 26,2% del total de energía anual que consumen, razón por la cual no se contempla venta de energía al sistema público.
En términos de resultados, al evaluar los diferentes escenarios, los indicadores VAN y TIR son atractivos para todas las opciones analizadas, aún para el proyecto puro, cuyos valores fueron levemente positivos, con lo que se puede concluir que el proyecto planteado es viable.
El tiempo de recuperación de la inversión (Payback) para esta condición de financiamiento con medios propios se encuentra en los 9 años, que es un valor esperable de acuerdo a los actuales estándares de la industria.
Respecto de los incentivos de instituciones gubernamentales que apoyan iniciativas ERNC, se puede decir que mejoran ostensiblemente la factibilidad de este proyecto, ya que disminuyen la importante inversión inicial que requieren este tipo de iniciativas.
Similar efecto positivo ocurre cuando no se utiliza el incentivo y se obtiene un préstamo de una institución financiera para cubrir diferentes proporciones de la inversión inicial y que este sea pagado durante toda la vida útil del proyecto.
En este caso los indicadores VAN y TIR mejoran notoriamente, lo mismo que con el Payback, todo lo cual hace más atractivo este proyecto.
Los análisis de sensibilidad permitieron apreciar los efectos de 3 factores que influyen de manera relevante en este tipo de proyectos.
El precio de los equipos incide de manera importante en la rentabilidad del proyecto, mostrando que las alzas la perjudican y las disminuciones la benefician.
En relación al alza del precio de la energía, esta favorece la rentabilidad del proyecto.
Respecto de la tasa de descuento, esta tiene un efecto no menor y es que está acotada en el rango superior, ya que valores superiores al 10% hacen inviable el proyecto.
Finalmente, el desarrollo del presente trabajo permitió establecer que el rubro de las ERNC, especialmente las opciones de proyectos fotovoltaicos, tienen aún un gran potencial de crecimiento en el país, ya que el mejoramiento de las tecnologías de paneles fotovoltaicos, sumado a la reducción de sus precios debido a su masificación a nivel mundial, ha permitido la llegada al país de más actores al rubro, a través de los cuales se ha aumentado la oferta y competencia en alternativas de ERNC.
Este crecimiento se ha visto de manera palpable en el año 2015 y más aún en 2016, períodos en los cuales, las licitaciones de energía llevadas a cabo por el Ministerio de Energía, atrajeron un alto número de participantes con opciones de ERNC, los cuales fueron en definitiva los grandes ganadores en estos procesos.
Además, esta importante alza del uso de estas tecnologías, trajo como consecuencia que la autoridad gubernamental sustituyó recientemente al carbón dentro de la matriz energética del país, apostando fuerte por este tipo de tecnologías ERNC, lo que reafirma el hecho de que este tipo de opciones de generación de energía son una alternativa real y viable para el país, que permiten aprovechar el excepcional potencial solar que Chile tiene."
